{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 13:15:32,840 - INFO - [MainThread] - main - === PDF Fetcher v12-pow started at 2025-05-22 13:15:32 ===\n",
      "2025-05-22 13:15:32,867 - INFO - [MainThread] - main - Loaded 32 unique, valid PMIDs from C:\\Users\\Galaxy\\Downloads\\screening_ERAS.xlsx\n",
      "2025-05-22 13:15:32,867 - INFO - [MainThread] - fetch_metadata - EFetch PubMed metadata for 32 PMIDs in 1 batch(es)...\n",
      "2025-05-22 13:15:32,869 - INFO - [MainThread] - fetch_metadata - NCBI EFetch POST (batch 1/1) → IDs=39955421,40340819,39068053...\n",
      "2025-05-22 13:15:34,391 - INFO - [MainThread] - main - Successfully fetched metadata for 32 PMIDs.\n",
      "2025-05-22 13:15:34,392 - INFO - [MainThread] - main - PDFs will be saved to: c:\\Users\\Galaxy\\LEVI\\jupyter\\litscape\\downloaded_pdfs_v12_pow\n",
      "2025-05-22 13:15:34,392 - INFO - [MainThread] - main - Suspicious/failed validation files will be in: c:\\Users\\Galaxy\\LEVI\\jupyter\\litscape\\downloaded_pdfs_v12_pow\\suspicious_pdfs\n",
      "2025-05-22 13:15:34,393 - INFO - [MainThread] - main - --- Starting Open Access Download Phase ---\n",
      "2025-05-22 13:15:34,394 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1007/s00383-025-05977-0\n",
      "2025-05-22 13:15:34,397 - INFO - [OA_Worker_1] - oa_worker - OA ✓ 40340819: PDF already exists at downloaded_pdfs_v12_pow\\2025-40340819-Pentz-Enhanced_Recovery_After_Surgery_(ERAS)_consensus_recommendations_for_non-pharmacological_perioperative_neonatal_pain_management..pdf\n",
      "2025-05-22 13:15:34,399 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2024.06.021\n",
      "2025-05-22 13:15:34,400 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 39384309: PDF already exists at downloaded_pdfs_v12_pow\\2024-39384309-Pilkington-Enhanced_Recovery_After_Surgery_(ERAS)_consensus_recommendations_for_opioid-minimising_pharmacological_neonatal_pain_manageme.pdf\n",
      "2025-05-22 13:15:34,402 - INFO - [OA_Worker_1] - oa_worker - OA ✓ 38673038: PDF already exists at downloaded_pdfs_v12_pow\\2024-38673038-Zacha-Cryoanalgesia_as_the_Essential_Element_of_Enhanced_Recovery_after_Surgery_(ERAS)_in_Children_Undergoing_Thoracic_Surgery-Scoping_R.pdf\n",
      "2025-05-22 13:15:34,406 - INFO - [OA_Worker_1] - oa_worker - OA ✓ 30518491: PDF already exists at downloaded_pdfs_v12_pow\\2019-30518491-Haveliwala-Aortopexy_for_tracheomalacia_via_a_suprasternal_incision..pdf\n",
      "2025-05-22 13:15:34,406 - INFO - [OA_Worker_1] - oa_worker - OA ✓ 33401363: PDF already exists at downloaded_pdfs_v12_pow\\2018-33401363-Wildemeersch-Implementation_of_an_Enhanced_Recovery_Pathway_for_Minimally_Invasive_Pectus_Surgery_A_Population-Based_Cohort_Study_Evalua.pdf\n",
      "2025-05-22 13:15:34,406 - INFO - [OA_Worker_1] - oa_worker - OA ✓ 39185540: PDF already exists at downloaded_pdfs_v12_pow\\2024-39185540-Hussain-Single-Session_Endoscopic_Ultrasound-Directed_Transgastric_Endoscopic_Retrograde_Cholangiopancreatography_and_Simultaneous_Endos.pdf\n",
      "2025-05-22 13:15:34,408 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1053/j.jvca.2023.09.006\n",
      "2025-05-22 13:15:34,403 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2023.01.028\n",
      "2025-05-22 13:15:34,404 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 32145713: PDF already exists at downloaded_pdfs_v12_pow\\2020-32145713-Wei-Combined_non-intubated_anaesthesia_and_paravertebral_nerve_block_in_comparison_with_intubated_anaesthesia_in_children_undergoing_vid.pdf\n",
      "2025-05-22 13:15:34,414 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1089/lap.2022.0537\n",
      "2025-05-22 13:15:35,284 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1089/lap.2022.0537: Not OA according to Unpaywall.\n",
      "2025-05-22 13:15:35,286 - INFO - [OA_Worker_3] - oa_worker - OA: No PDF URL from Unpaywall for PMID 37062759 (DOI 10.1089/lap.2022.0537). Trying PMC.\n",
      "2025-05-22 13:15:35,286 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1007/s00383-025-05977-0: Not OA according to Unpaywall.\n",
      "2025-05-22 13:15:35,286 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ? DOI 10.1016/j.jpedsurg.2024.06.021: Article is OA, but no direct PDF URL in Unpaywall response.\n",
      "2025-05-22 13:15:35,289 - INFO - [OA_Worker_2] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39068053 (DOI 10.1016/j.jpedsurg.2024.06.021). Trying PMC.\n",
      "2025-05-22 13:15:35,288 - INFO - [OA_Worker_0] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39955421 (DOI 10.1007/s00383-025-05977-0). Trying PMC.\n",
      "2025-05-22 13:15:35,289 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1053/j.jvca.2023.09.006: Not OA according to Unpaywall.\n",
      "2025-05-22 13:15:35,286 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink → PMID 37062759: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:15:35,289 - INFO - [OA_Worker_2] - pmc_id_for_pmid - PMC ID ELink → PMID 39068053: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:15:35,290 - INFO - [OA_Worker_0] - pmc_id_for_pmid - PMC ID ELink → PMID 39955421: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:15:35,290 - INFO - [OA_Worker_1] - oa_worker - OA: No PDF URL from Unpaywall for PMID 37802689 (DOI 10.1053/j.jvca.2023.09.006). Trying PMC.\n",
      "2025-05-22 13:15:35,296 - INFO - [OA_Worker_1] - pmc_id_for_pmid - PMC ID ELink → PMID 37802689: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:15:35,299 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1016/j.jpedsurg.2023.01.028: Not OA according to Unpaywall.\n",
      "2025-05-22 13:15:35,299 - INFO - [OA_Worker_4] - oa_worker - OA: No PDF URL from Unpaywall for PMID 36788057 (DOI 10.1016/j.jpedsurg.2023.01.028). Trying PMC.\n",
      "2025-05-22 13:15:35,300 - INFO - [OA_Worker_4] - pmc_id_for_pmid - PMC ID ELink → PMID 36788057: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:15:35,711 - WARNING - [OA_Worker_3] - oa_worker - OA ✗ 37062759: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:15:35,713 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 36969299: PDF already exists at downloaded_pdfs_v12_pow\\2023-36969299-Huang-Application_of_laryngeal_mask_airway_anesthesia_with_preserved_spontaneous_breathing_in_children_undergoing_video-assisted_thoraci.pdf\n",
      "2025-05-22 13:15:35,713 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 33888360: PDF already exists at downloaded_pdfs_v12_pow\\2022-33888360-Hung-Comparison_of_perioperative_outcomes_between_intubated_and_nonintubated_thoracoscopic_surgery_in_children..pdf\n",
      "2025-05-22 13:15:35,714 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 35790215: PDF already exists at downloaded_pdfs_v12_pow\\2022-35790215-Lucente-Erector_spinae_plane_block_in_children_a_narrative_review..pdf\n",
      "2025-05-22 13:15:35,715 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 34089071: PDF already exists at downloaded_pdfs_v12_pow\\2021-34089071-Cramm-Thoracic_epidural-based_Enhanced_Recovery_After_Surgery_(ERAS)_pathway_for_Nuss_repair_of_pectus_excavatum_shortened_length_of_sta.pdf\n",
      "2025-05-22 13:15:35,717 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 33210165: PDF already exists at downloaded_pdfs_v12_pow\\2021-33210165-Rettig-Cryoablation_is_associated_with_shorter_length_of_stay_and_reduced_opioid_use_in_pectus_excavatum_repair..pdf\n",
      "2025-05-22 13:15:35,717 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 34059337: PDF already exists at downloaded_pdfs_v12_pow\\2021-34059337-Fuller-The_American_Association_for_Thoracic_Surgery_Congenital_Cardiac_Surgery_Working_Group_2021_consensus_document_on_a_comprehensive.pdf\n",
      "2025-05-22 13:15:35,719 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 32696123: PDF already exists at downloaded_pdfs_v12_pow\\2020-32696123-Mangat-The_impact_of_an_enhanced_recovery_perioperative_pathway_for_pediatric_pectus_deformity_repair..pdf\n",
      "2025-05-22 13:15:35,719 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 32385680: PDF already exists at downloaded_pdfs_v12_pow\\2020-32385680-Brindle-Consensus_Guidelines_for_Perioperative_Care_in_Neonatal_Intestinal_Surgery_Enhanced_Recovery_After_Surgery_(ERAS®)_Society_Recom.pdf\n",
      "2025-05-22 13:15:35,720 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 30686518: PDF already exists at downloaded_pdfs_v12_pow\\2019-30686518-Schlatter-Progressive_reduction_of_hospital_length_of_stay_following_minimally_invasive_repair_of_pectus_excavatum_A_retrospective_compa.pdf\n",
      "2025-05-22 13:15:35,720 - INFO - [OA_Worker_3] - oa_worker - OA ✓ 30922685: PDF already exists at downloaded_pdfs_v12_pow\\2019-30922685-Holmes-Opioid_use_and_length_of_stay_following_minimally_invasive_pectus_excavatum_repair_in_436_patients_-_Benefits_of_an_enhanced_reco.pdf\n",
      "2025-05-22 13:15:35,721 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.23736/S0375-9393.19.13880-1\n",
      "2025-05-22 13:15:36,126 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.23736/S0375-9393.19.13880-1: Not OA according to Unpaywall.\n",
      "2025-05-22 13:15:36,127 - WARNING - [OA_Worker_0] - oa_worker - OA ✗ 39955421: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:15:36,128 - INFO - [OA_Worker_0] - oa_worker - OA ✓ 27679510: PDF already exists at downloaded_pdfs_v12_pow\\2017-27679510-Pearson-What_is_the_role_of_enhanced_recovery_after_surgery_in_children_A_scoping_review..pdf\n",
      "2025-05-22 13:15:36,127 - INFO - [OA_Worker_3] - oa_worker - OA: No PDF URL from Unpaywall for PMID 31274269 (DOI 10.23736/S0375-9393.19.13880-1). Trying PMC.\n",
      "2025-05-22 13:15:36,129 - INFO - [OA_Worker_0] - oa_worker - OA ✓ 27810148: PDF already exists at downloaded_pdfs_v12_pow\\2017-27810148-Bryskin-Introduction_of_a_novel_ultrasound-guided_extrathoracic_sub-paraspinal_block_for_control_of_perioperative_pain_in_Nuss_procedure.pdf\n",
      "2025-05-22 13:15:36,129 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink → PMID 31274269: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:15:36,130 - WARNING - [OA_Worker_4] - oa_worker - OA ✗ 36788057: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:15:36,131 - INFO - [OA_Worker_0] - oa_worker - OA ✓ 27083963: PDF already exists at downloaded_pdfs_v12_pow\\2016-27083963-Shinnick-Enhancing_recovery_in_pediatric_surgery_a_review_of_the_literature..pdf\n",
      "2025-05-22 13:15:36,133 - INFO - [OA_Worker_4] - oa_worker - OA: No DOI for PMID 26888001. Skipping Unpaywall, trying PMC.\n",
      "2025-05-22 13:15:36,134 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2024.162046\n",
      "2025-05-22 13:15:36,134 - INFO - [OA_Worker_4] - pmc_id_for_pmid - PMC ID ELink → PMID 26888001: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:15:36,207 - WARNING - [OA_Worker_2] - oa_worker - OA ✗ 39068053: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:15:36,208 - INFO - [OA_Worker_2] - oa_worker - OA ✓ 39489669: PDF already exists at downloaded_pdfs_v12_pow\\2025-39489669-Meier-Enhanced_Recovery_after_Surgery_(ERAS)_in_Pediatric_Cardiac_Surgery_Status_Quo_of_Implementation_in_Europe..pdf\n",
      "2025-05-22 13:15:36,209 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1001/jamasurg.2024.2044\n",
      "2025-05-22 13:15:36,384 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1016/j.jpedsurg.2024.162046: Not OA according to Unpaywall.\n",
      "2025-05-22 13:15:36,384 - INFO - [OA_Worker_0] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39520824 (DOI 10.1016/j.jpedsurg.2024.162046). Trying PMC.\n",
      "2025-05-22 13:15:36,384 - INFO - [OA_Worker_0] - pmc_id_for_pmid - PMC ID ELink → PMID 39520824: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:15:36,403 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1001/jamasurg.2024.2044: Not OA according to Unpaywall.\n",
      "2025-05-22 13:15:36,404 - INFO - [OA_Worker_2] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39083294 (DOI 10.1001/jamasurg.2024.2044). Trying PMC.\n",
      "2025-05-22 13:15:36,406 - INFO - [OA_Worker_2] - pmc_id_for_pmid - PMC ID ELink → PMID 39083294: Querying with linkname 'pubmed_pmc'.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Combined OA-first + Sci-Hub PDF Fetcher (v12-pow, standalone)\n",
    "-----------------------------------------------------------\n",
    "1) Reads PMIDs from an Excel file.\n",
    "2) Fetches metadata (year, first author, title, DOI) via NCBI EFetch.\n",
    "3) Tries Open Access first:\n",
    "    a) Unpaywall.\n",
    "    b) PMC (handles Proof-of-Work challenge, resolves final URL).\n",
    "4) Any remaining PMIDs are tried via Sci-Hub in parallel.\n",
    "5) All PDFs are validated (size, content) and land in OUTPUT_PDF_DIR\n",
    "   named `{year}-{pmid}-{author}-{title}.pdf`.\n",
    "6) Detailed logging of every URL fetched and why it failed or succeeded.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, quote_plus\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import hashlib # For PoW\n",
    "try:\n",
    "    from pypdf import PdfReader # Preferred (PyPDF2 successor)\n",
    "    from pypdf.errors import PdfReadError\n",
    "except ImportError:\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader # Fallback\n",
    "        from PyPDF2.errors import PdfReadError\n",
    "    except ImportError:\n",
    "        print(\"Please install pypdf or PyPDF2: pip install pypdf\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "EXCEL_FILE_PATH     = r\"C:\\Users\\Galaxy\\Downloads\\screening_ERAS.xlsx\" # UPDATE THIS\n",
    "OUTPUT_PDF_DIR      = \"downloaded_pdfs_v12_pow\"\n",
    "SUSPICIOUS_PDF_SUBDIR = \"suspicious_pdfs\" # Subdirectory for failed validation PDFs\n",
    "\n",
    "# Sci-Hub\n",
    "SCI_HUB_DOMAINS     = [\n",
    "    \"https://sci-hub.se\", \"https://sci-hub.ru\", \"https://sci-hub.ren\",\n",
    "    \"https://sci-hub.wf\", \"https://sci-hub.ee\", \"https://sci-hub.st\"\n",
    "]\n",
    "DELAY_SCIHUB        = 0.5\n",
    "\n",
    "# Threads / batching\n",
    "MAX_THREADS         = 5 # Reduced default, be kind to servers\n",
    "EFETCH_BATCH_SIZE   = 100\n",
    "DELAY_NCBI          = 0.35\n",
    "\n",
    "# PDF Validation Thresholds\n",
    "MIN_PDF_SIZE_KB         = 20    # PDFs smaller than this are suspicious\n",
    "MIN_PDF_PAGES           = 1     # PDFs with fewer pages are suspicious\n",
    "MIN_TEXT_LENGTH_CHARS   = 300   # Min chars expected from first few pages of a real article\n",
    "\n",
    "# API credentials - PLEASE FILL THESE IN\n",
    "NCBI_API_KEY        = \"YOUR_API_KEY_HERE\" # IMPORTANT: Fill if you have one\n",
    "CROSSREF_EMAIL      = \"your_email@example.com\" # IMPORTANT: Fill with your email for polite API use\n",
    "UNPAYWALL_EMAIL     = \"levi4328@gmail.com\" # Your email for Unpaywall\n",
    "\n",
    "# === LOGGING SETUP ===\n",
    "logger = logging.getLogger(\"PDFFetcherV12\")\n",
    "logger.handlers = [] # Clear existing handlers if any (Jupyter)\n",
    "logger.setLevel(logging.INFO) # DEBUG for more verbosity\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - [%(threadName)s] - %(funcName)s - %(message)s'\n",
    "))\n",
    "logger.addHandler(ch)\n",
    "\n",
    "# File logging (optional, but recommended for long runs)\n",
    "# log_file_path = os.path.join(OUTPUT_PDF_DIR, \"pdf_fetcher_v12.log\")\n",
    "# os.makedirs(OUTPUT_PDF_DIR, exist_ok=True) \n",
    "# fh = logging.FileHandler(log_file_path, mode='a') \n",
    "# fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - [%(threadName)s] - %(funcName)s - %(message)s'))\n",
    "# logger.addHandler(fh)\n",
    "\n",
    "\n",
    "# === HTTP SESSIONS WITH RETRIES ===\n",
    "# Using a common, recent Chrome User-Agent string\n",
    "CHROME_UA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "\n",
    "def make_session(user_agent, is_scihub_session=False):\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=frozenset(['GET', 'POST', 'HEAD'])\n",
    "    )\n",
    "    # Adjust pool connections based on whether it's for Sci-Hub or general OA/NCBI\n",
    "    # Sci-Hub might benefit from fewer connections per domain if MAX_THREADS is high,\n",
    "    # to avoid overwhelming a single Sci-Hub mirror.\n",
    "    pool_connections = MAX_THREADS // 2 if is_scihub_session and MAX_THREADS > 2 else MAX_THREADS\n",
    "    pool_maxsize = pool_connections * 2 # Standard practice: pool_maxsize often 2x pool_connections\n",
    "\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retries,\n",
    "        pool_connections=pool_connections,\n",
    "        pool_maxsize=pool_maxsize \n",
    "    )\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.headers.update({'User-Agent': user_agent})\n",
    "    return s\n",
    "\n",
    "# Create distinct sessions for different services if User-Agent policies differ\n",
    "# For NCBI, it's good to identify the tool and provide an email if possible.\n",
    "ncbi_ua_email_part = CROSSREF_EMAIL if CROSSREF_EMAIL and CROSSREF_EMAIL != \"your_email@example.com\" else \"anonymous_user\"\n",
    "session_ncbi   = make_session(f\"PDFFetcherV12/1.0 (NCBI-EUtils-Client; mailto:{ncbi_ua_email_part})\")\n",
    "session_oa     = make_session(CHROME_UA) # General OA sites often need browser-like UAs\n",
    "session_scihub = make_session(CHROME_UA, is_scihub_session=True) # Sci-Hub also prefers browser-like UAs\n",
    "\n",
    "# Shared BROWSER_LIKE_HEADERS for GET requests that might hit web pages\n",
    "BROWSER_LIKE_HEADERS = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br', # Allow compressed responses\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none', # Default, can be overridden for specific requests\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Cache-Control': 'no-cache', # Try to get fresh content\n",
    "    'Pragma': 'no-cache'        # For older HTTP/1.0 caches\n",
    "}\n",
    "\n",
    "# === UTILITIES ===\n",
    "def sanitize_filename(s: str) -> str:\n",
    "    s = str(s) # Ensure input is a string\n",
    "    s = re.sub(r'[\\\\/*?:\"<>|]', \"\", s) # Remove illegal filename characters\n",
    "    s = re.sub(r'\\s+', \" \", s).strip() # Replace multiple spaces with single, strip ends\n",
    "    if not s: s = \"untitled_article\"   # Default if string becomes empty\n",
    "    # Replace spaces with underscores and limit length (common for filenames)\n",
    "    return s.replace(\" \", \"_\")[:150] # Limit length to avoid issues with long paths/names\n",
    "\n",
    "def _get_ncbi_params(extra=None):\n",
    "    params = {\"tool\": \"PDFFetcherV12\"} # Identify the tool to NCBI\n",
    "    if CROSSREF_EMAIL and CROSSREF_EMAIL != \"your_email@example.com\":\n",
    "        params[\"email\"] = CROSSREF_EMAIL\n",
    "    if NCBI_API_KEY and NCBI_API_KEY != \"YOUR_API_KEY_HERE\":\n",
    "        params[\"api_key\"] = NCBI_API_KEY\n",
    "    if extra:\n",
    "        params.update(extra)\n",
    "    return params\n",
    "\n",
    "# === PROOF-OF-WORK (PoW) SOLVING LOGIC for PMC ===\n",
    "def extract_pow_params_from_html(html_content: str) -> tuple[str, int, str, str, str] | None:\n",
    "    # Regex to find JavaScript const declarations for PoW parameters\n",
    "    challenge_match = re.search(r'const\\s+POW_CHALLENGE\\s*=\\s*\"(.*?)\"', html_content)\n",
    "    difficulty_match = re.search(r'const\\s+POW_DIFFICULTY\\s*=\\s*\"(.*?)\"', html_content)\n",
    "    cookie_name_match = re.search(r'const\\s+POW_COOKIE_NAME\\s*=\\s*\"(.*?)\"', html_content)\n",
    "    cookie_exp_match = re.search(r'const\\s+POW_COOKIE_EXPIRATION\\s*=\\s*\"(.*?)\"', html_content) # Currently unused in solving\n",
    "    cookie_path_match = re.search(r'const\\s+POW_COOKIE_PATH\\s*=\\s*\"(.*?)\"', html_content)\n",
    "\n",
    "    if challenge_match and difficulty_match and cookie_name_match and cookie_path_match:\n",
    "        challenge_string = challenge_match.group(1)\n",
    "        cookie_name = cookie_name_match.group(1)\n",
    "        cookie_exp_str = cookie_exp_match.group(1) if cookie_exp_match else \"0.208333\" # Default if somehow missing\n",
    "        cookie_path = cookie_path_match.group(1)\n",
    "        try:\n",
    "            difficulty = int(difficulty_match.group(1))\n",
    "            logger.info(f\"Extracted PoW params: Challenge='{challenge_string[:20]}...', Diff={difficulty}, Name='{cookie_name}'\")\n",
    "            return challenge_string, difficulty, cookie_name, cookie_exp_str, cookie_path\n",
    "        except ValueError:\n",
    "            logger.error(f\"Could not parse PoW difficulty as int: '{difficulty_match.group(1)}'\")\n",
    "    else:\n",
    "        logger.warning(\"Could not find all required PoW parameters in HTML content.\")\n",
    "    return None\n",
    "\n",
    "def solve_pmc_pow(challenge_string: str, difficulty: int) -> tuple[int, str] | None:\n",
    "    logger.info(f\"Solving PoW: challenge='{challenge_string[:20]}...', difficulty={difficulty}\")\n",
    "    target_prefix = \"0\" * difficulty\n",
    "    nonce = 0\n",
    "    # Max nonce attempts can be adjusted; these are rough estimates based on common difficulties\n",
    "    max_nonce_map = {4: 2_000_000, 5: 35_000_000, 6: 500_000_000} # Added difficulty 6\n",
    "    max_nonce = max_nonce_map.get(difficulty, 100_000_000) # Default for other difficulties\n",
    "\n",
    "    start_time = time.time()\n",
    "    while nonce <= max_nonce:\n",
    "        test_string = challenge_string + str(nonce)\n",
    "        hash_object = hashlib.sha256(test_string.encode('utf-8')) # SHA256 is common\n",
    "        hex_digest = hash_object.hexdigest()\n",
    "        if hex_digest.startswith(target_prefix):\n",
    "            duration = time.time() - start_time\n",
    "            logger.info(f\"PoW SOLVED! Nonce: {nonce}, Hash: {hex_digest[:10]}..., Time: {duration:.4f}s\")\n",
    "            return nonce, hex_digest\n",
    "        if nonce > 0 and nonce % 1_000_000 == 0: # Log progress every million nonces\n",
    "            logger.debug(f\"PoW progress: nonce {nonce}...\")\n",
    "        nonce += 1\n",
    "    duration = time.time() - start_time\n",
    "    logger.error(f\"PoW FAILED to solve (max_nonce {max_nonce} reached for difficulty {difficulty}). Time: {duration:.2f}s\")\n",
    "    return None\n",
    "\n",
    "# === PDF VALIDATION (returns failure reason string or None for success) ===\n",
    "def validate_downloaded_pdf(pdf_path: str, pmid_for_log: str) -> str | None:\n",
    "    \"\"\"Validates a downloaded PDF. Returns None if valid, else a string describing failure reason.\"\"\"\n",
    "    failure_reason = \"\"\n",
    "    try:\n",
    "        file_size_kb = os.path.getsize(pdf_path) / 1024\n",
    "        if file_size_kb < MIN_PDF_SIZE_KB:\n",
    "            failure_reason = f\"File size {file_size_kb:.2f} KB < threshold {MIN_PDF_SIZE_KB} KB\"\n",
    "            logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}'.\")\n",
    "            return failure_reason\n",
    "\n",
    "        reader = None\n",
    "        num_pages = 0\n",
    "        try:\n",
    "            reader = PdfReader(pdf_path)\n",
    "            num_pages = len(reader.pages)\n",
    "            if num_pages < MIN_PDF_PAGES:\n",
    "                failure_reason = f\"Page count {num_pages} < threshold {MIN_PDF_PAGES}\"\n",
    "                logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f} KB).\")\n",
    "                return failure_reason\n",
    "        except PdfReadError as e:\n",
    "            failure_reason = f\"pypdf PdfReadError: {e}\"\n",
    "            logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f} KB).\")\n",
    "            return failure_reason\n",
    "        except Exception as e_open:\n",
    "            failure_reason = f\"pypdf unexpected open error: {e_open}\"\n",
    "            logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f} KB).\")\n",
    "            return failure_reason\n",
    "\n",
    "        extracted_text_len = 0\n",
    "        max_pages_to_check_text = min(3, num_pages)\n",
    "        if reader:\n",
    "            for i in range(max_pages_to_check_text):\n",
    "                try:\n",
    "                    page = reader.pages[i]\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        extracted_text_len += len(page_text)\n",
    "                except Exception as e_text_extract:\n",
    "                    logger.warning(f\"PDF Validation ? {pmid_for_log}: Error extracting text from page {i+1} of '{os.path.basename(pdf_path)}': {e_text_extract}. Continuing.\")\n",
    "        \n",
    "        if extracted_text_len < MIN_TEXT_LENGTH_CHARS:\n",
    "            failure_reason = f\"Insufficient text ({extracted_text_len} chars from first {max_pages_to_check_text} page(s) < threshold {MIN_TEXT_LENGTH_CHARS})\"\n",
    "            logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f} KB).\")\n",
    "            return failure_reason\n",
    "\n",
    "        logger.info(f\"PDF Validation ✓ {pmid_for_log}: File '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f}KB, Pages: {num_pages}, TextLen: {extracted_text_len} from first {max_pages_to_check_text} page(s)) passed validation.\")\n",
    "        return None # Success\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"PDF Validation ✗ {pmid_for_log}: File not found at {pdf_path} for validation.\")\n",
    "        return \"File not found for validation.\" # Return reason\n",
    "    except Exception as e:\n",
    "        failure_reason = f\"Unexpected error during validation setup: {e}\"\n",
    "        logger.error(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for {pdf_path}\", exc_info=True)\n",
    "        return failure_reason # Return reason\n",
    "\n",
    "# === CENTRALIZED PDF DOWNLOADER (handles PoW for PMC) ===\n",
    "def download_and_save_pdf(\n",
    "    session: requests.Session, \n",
    "    pdf_url: str, \n",
    "    output_path: str, \n",
    "    pmid_for_log: str, \n",
    "    source_name: str,\n",
    "    article_metadata: dict, # MODIFICATION: Added article_metadata parameter\n",
    "    referer: str | None = None\n",
    "    ) -> bool:\n",
    "    \n",
    "    logger.info(f\"{source_name} → PMID {pmid_for_log}: Attempting download from {pdf_url}\")\n",
    "    \n",
    "    current_headers = session.headers.copy() \n",
    "    current_headers.update(BROWSER_LIKE_HEADERS) \n",
    "    current_headers['Accept'] = 'application/pdf,text/html;q=0.9,application/xhtml+xml,application/xml;q=0.8,*/*;q=0.5'\n",
    "    if referer:\n",
    "        current_headers['Referer'] = referer\n",
    "    \n",
    "    parsed_pdf_url = urlparse(pdf_url)\n",
    "    if 'pmc.ncbi.nlm.nih.gov' in parsed_pdf_url.netloc:\n",
    "        is_same_origin_pmc = referer and 'pmc.ncbi.nlm.nih.gov' in urlparse(referer).netloc\n",
    "        current_headers['Sec-Fetch-Site'] = 'same-origin' if is_same_origin_pmc else 'cross-site'\n",
    "\n",
    "    temp_pdf_path = output_path + \".tmp\" \n",
    "\n",
    "    try:\n",
    "        r = session.get(pdf_url, headers=current_headers, stream=True, timeout=(15, 60), allow_redirects=True) \n",
    "        r.raise_for_status() \n",
    "        final_url_after_redirects = r.url \n",
    "        content_type = r.headers.get('Content-Type', '').lower()\n",
    "        logger.debug(f\"{source_name} → PMID {pmid_for_log}: Initial GET to {pdf_url} (final: {final_url_after_redirects}), Content-Type: {content_type}\")\n",
    "\n",
    "        html_content_bytes = None # Define in case it's used later for saving unexpected content\n",
    "\n",
    "        if \"pmc.ncbi.nlm.nih.gov\" in urlparse(final_url_after_redirects).netloc and 'text/html' in content_type:\n",
    "            logger.info(f\"{source_name} → PMID {pmid_for_log}: Received HTML from PMC URL, attempting PoW solve.\")\n",
    "            html_content_bytes = r.content \n",
    "            html_content_str = html_content_bytes.decode('utf-8', errors='replace')\n",
    "            \n",
    "            sanitized_source_detail_for_html = sanitize_filename(source_name)\n",
    "            debug_html_path = os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR, f\"{pmid_for_log}_{sanitized_source_detail_for_html}_challenge.html\")\n",
    "            os.makedirs(os.path.dirname(debug_html_path), exist_ok=True)\n",
    "            with open(debug_html_path, \"w\", encoding=\"utf-8\") as f_debug: f_debug.write(html_content_str)\n",
    "            logger.info(f\"Saved PMC challenge HTML to {debug_html_path}\")\n",
    "\n",
    "            pow_params = extract_pow_params_from_html(html_content_str)\n",
    "            if not pow_params:\n",
    "                logger.error(f\"{source_name} → PMID {pmid_for_log}: Failed to extract PoW params from PMC HTML.\")\n",
    "                return False # Does not log to enhanced failure log here, as no file was downloaded.\n",
    "            \n",
    "            challenge_str, difficulty_val, cookie_name, _, cookie_path = pow_params\n",
    "            solution = solve_pmc_pow(challenge_str, difficulty_val)\n",
    "            if not solution:\n",
    "                logger.error(f\"{source_name} → PMID {pmid_for_log}: Failed to solve PMC PoW.\")\n",
    "                return False # No file downloaded yet.\n",
    "\n",
    "            nonce_found, _ = solution\n",
    "            pow_cookie_value = f\"{challenge_str},{nonce_found}\" \n",
    "            \n",
    "            parsed_uri = urlparse(final_url_after_redirects)\n",
    "            session.cookies.set(name=cookie_name, value=pow_cookie_value, domain=parsed_uri.hostname, path=cookie_path)\n",
    "            logger.info(f\"Set PoW cookie '{cookie_name}' in session for {parsed_uri.hostname}.\")\n",
    "\n",
    "            logger.info(f\"{source_name} → PMID {pmid_for_log}: Re-attempting GET to {final_url_after_redirects} WITH PoW cookie.\")\n",
    "            current_headers['Accept'] = 'application/pdf,application/octet-stream,*/*;q=0.8' \n",
    "            current_headers['Referer'] = final_url_after_redirects \n",
    "            current_headers['Sec-Fetch-Site'] = 'same-origin' \n",
    "\n",
    "            r = session.get(final_url_after_redirects, headers=current_headers, stream=True, timeout=(15, 60))\n",
    "            r.raise_for_status()\n",
    "            content_type = r.headers.get('Content-Type', '').lower() \n",
    "            logger.debug(f\"{source_name} → PMID {pmid_for_log}: Second GET (post-PoW), Content-Type: {content_type}\")\n",
    "\n",
    "        if 'application/pdf' in content_type or final_url_after_redirects.lower().endswith(\".pdf\"):\n",
    "            with open(temp_pdf_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=81920): \n",
    "                    f.write(chunk)\n",
    "            \n",
    "            validation_failure_reason = validate_downloaded_pdf(temp_pdf_path, pmid_for_log) # MODIFICATION\n",
    "            if not validation_failure_reason: # MODIFICATION: Check if reason is None (success)\n",
    "                os.rename(temp_pdf_path, output_path) \n",
    "                logger.info(f\"{source_name} ✓ PMID {pmid_for_log}: Successfully downloaded and validated PDF to {output_path}\")\n",
    "                return True\n",
    "            else: # MODIFICATION: PDF failed validation\n",
    "                logger.warning(f\"{source_name} ✗ PMID {pmid_for_log}: PDF from {final_url_after_redirects} failed validation: {validation_failure_reason}\")\n",
    "                suspicious_dir = os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR)\n",
    "                os.makedirs(suspicious_dir, exist_ok=True)\n",
    "                \n",
    "                base_output_filename = sanitize_filename(os.path.basename(output_path).replace(\".pdf\", \"\")) \n",
    "                sanitized_source_detail = sanitize_filename(source_name) \n",
    "                suspicious_filename = f\"{base_output_filename}.{sanitized_source_detail}.validation_failed.pdf\"\n",
    "                if len(os.path.join(suspicious_dir, suspicious_filename)) > 250: # Basic check for path length\n",
    "                    suspicious_filename = f\"{base_output_filename[:100]}.{sanitized_source_detail[:50]}.validation_failed.pdf\"\n",
    "\n",
    "                suspicious_path = os.path.join(suspicious_dir, suspicious_filename)\n",
    "                try:\n",
    "                    if os.path.exists(temp_pdf_path): \n",
    "                        os.rename(temp_pdf_path, suspicious_path)\n",
    "                        logger.info(f\"Moved suspicious PDF to {suspicious_path}\")\n",
    "                        # MODIFICATION: Log detailed info for suspicious PDF\n",
    "                        log_enhanced_failure_details(\n",
    "                            \"suspicious_articles_details.log\", \n",
    "                            pmid_for_log, \n",
    "                            article_metadata, \n",
    "                            \"Suspicious PDF (Validation Failed)\", \n",
    "                            details=validation_failure_reason, \n",
    "                            file_path=suspicious_path,\n",
    "                            logged_set=LOGGED_SUSPICIOUS_PMIDS\n",
    "                        )\n",
    "                    else: \n",
    "                        logger.warning(f\"Temporary PDF {temp_pdf_path} not found for moving to suspicious.\")\n",
    "                except OSError as e_rename: \n",
    "                    logger.error(f\"OSError moving suspicious PDF {temp_pdf_path} to {suspicious_path}: {e_rename}\")\n",
    "                    if os.path.exists(temp_pdf_path): os.remove(temp_pdf_path) \n",
    "                return False\n",
    "        else: # Non-PDF content\n",
    "            logger.warning(f\"{source_name} ✗ PMID {pmid_for_log}: Non-PDF content from {final_url_after_redirects}. Content-Type: {content_type}\")\n",
    "            \n",
    "            sanitized_source_detail_for_debug = sanitize_filename(source_name)\n",
    "            debug_content_filename = f\"{pmid_for_log}_{sanitized_source_detail_for_debug}_unexpected_content.dat\"\n",
    "            debug_content_path = os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR, debug_content_filename)\n",
    "            os.makedirs(os.path.dirname(debug_content_path), exist_ok=True)\n",
    "            try:\n",
    "                # Use html_content_bytes if available (from PoW challenge), otherwise r.content\n",
    "                content_to_save = html_content_bytes if html_content_bytes is not None else r.content\n",
    "                with open(debug_content_path, 'wb') as f_debug:\n",
    "                    f_debug.write(content_to_save)\n",
    "                logger.info(f\"Saved unexpected content ({len(content_to_save)} bytes) to {debug_content_path}\")\n",
    "                # MODIFICATION: Log detailed info for this failure type as well\n",
    "                log_enhanced_failure_details(\n",
    "                    \"suspicious_articles_details.log\", # Or a different log for \"non-pdf content\"\n",
    "                    pmid_for_log, \n",
    "                    article_metadata,\n",
    "                    \"Non-PDF Content Received\",\n",
    "                    details=f\"Content-Type: {content_type}, URL: {final_url_after_redirects}\",\n",
    "                    file_path=debug_content_path,\n",
    "                    logged_set=LOGGED_SUSPICIOUS_PMIDS # Using same set, or create a new one\n",
    "                )\n",
    "            except Exception as e_save_debug:\n",
    "                logger.error(f\"Error saving unexpected content for PMID {pmid_for_log}: {e_save_debug}\")\n",
    "            return False\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.warning(f\"{source_name} ✗ PMID {pmid_for_log}: RequestException for {pdf_url}: {e}\")\n",
    "    except Exception as e_main: \n",
    "        logger.error(f\"{source_name} ✗ PMID {pmid_for_log}: Unexpected error during download from {pdf_url}: {e_main}\", exc_info=True)\n",
    "    \n",
    "    if os.path.exists(temp_pdf_path):\n",
    "        try:\n",
    "            os.remove(temp_pdf_path)\n",
    "        except OSError as e_remove:\n",
    "            logger.warning(f\"Could not remove temp PDF {temp_pdf_path}: {e_remove}\")\n",
    "    return False\n",
    "\n",
    "# === STEP 1: FETCH METADATA FROM PUBMED ===\n",
    "def fetch_metadata(pmids):\n",
    "    meta = {}\n",
    "    batches = [pmids[i:i+EFETCH_BATCH_SIZE] for i in range(0, len(pmids), EFETCH_BATCH_SIZE)]\n",
    "    logger.info(f\"EFetch PubMed metadata for {len(pmids)} PMIDs in {len(batches)} batch(es)...\")\n",
    "    \n",
    "    for i_batch, batch in enumerate(batches):\n",
    "        efetch_payload = _get_ncbi_params({\n",
    "            \"db\": \"pubmed\",\n",
    "            \"retmode\": \"xml\",\n",
    "            \"id\": \",\".join(map(str, batch))\n",
    "        })\n",
    "        logger.info(f\"NCBI EFetch POST (batch {i_batch+1}/{len(batches)}) → IDs={','.join(map(str,batch[:3]))}...\")\n",
    "        try:\n",
    "            resp = session_ncbi.post(\n",
    "                \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\",\n",
    "                data=efetch_payload, \n",
    "                timeout=60 \n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            root = ET.fromstring(resp.content)\n",
    "            \n",
    "            for art_node in root.findall(\".//PubmedArticle\"):\n",
    "                pmid_el = art_node.find(\".//PMID\")\n",
    "                pmid = pmid_el.text.strip() if pmid_el is not None and pmid_el.text else None\n",
    "                if not pmid: continue\n",
    "\n",
    "                doi_el = art_node.find(\".//ArticleId[@IdType='doi']\") or \\\n",
    "                         art_node.find(\".//ELocationID[@EIdType='doi'][@ValidYN='Y']\")\n",
    "                doi = doi_el.text.strip() if doi_el is not None and doi_el.text else None\n",
    "                \n",
    "                year_el = art_node.find(\".//PubDate/Year\") or \\\n",
    "                          art_node.find(\".//Journal/JournalIssue/PubDate/Year\")\n",
    "                year = \"UnknownYear\"\n",
    "                if year_el is not None and year_el.text and year_el.text.strip().isdigit() and len(year_el.text.strip()) == 4:\n",
    "                    year = year_el.text.strip()\n",
    "                else:\n",
    "                    medline_date_el = art_node.find(\".//PubDate/MedlineDate\") or \\\n",
    "                                      art_node.find(\".//Article/Journal/JournalIssue/PubDate/MedlineDate\")\n",
    "                    if medline_date_el is not None and medline_date_el.text:\n",
    "                        year_match = re.match(r\"^\\d{4}\", medline_date_el.text.strip())\n",
    "                        if year_match: year = year_match.group(0)\n",
    "                \n",
    "                author_el = art_node.find(\".//AuthorList/Author[1]/LastName\")\n",
    "                author = author_el.text.strip() if author_el is not None and author_el.text else \"UnknownAuthor\"\n",
    "                \n",
    "                title_el = art_node.find(\".//ArticleTitle\")\n",
    "                title = \"\".join(title_el.itertext()).strip() if title_el is not None else f\"NoTitle_{pmid}\"\n",
    "\n",
    "                # Extract Abstract\n",
    "                abstract_parts = []\n",
    "                for abstract_text_node in art_node.findall(\".//Abstract/AbstractText\"):\n",
    "                    if abstract_text_node.text:\n",
    "                        label = abstract_text_node.get(\"Label\")\n",
    "                        if label:\n",
    "                            abstract_parts.append(f\"[{label.upper()}] {abstract_text_node.text.strip()}\")\n",
    "                        else:\n",
    "                            abstract_parts.append(abstract_text_node.text.strip())\n",
    "                abstract = \"\\n\".join(abstract_parts) if abstract_parts else \"N/A\"\n",
    "\n",
    "                # Extract MeSH Terms\n",
    "                mesh_terms = []\n",
    "                for mesh_heading_node in art_node.findall(\".//MeshHeadingList/MeshHeading\"):\n",
    "                    descriptor_name_node = mesh_heading_node.find(\"./DescriptorName\")\n",
    "                    if descriptor_name_node is not None and descriptor_name_node.text:\n",
    "                        mesh_terms.append(descriptor_name_node.text.strip())\n",
    "                mesh_terms_str = \"; \".join(mesh_terms) if mesh_terms else \"N/A\"\n",
    "                \n",
    "                meta[pmid] = {\n",
    "                    'doi': doi, 'year': year, 'author': author, 'title': title,\n",
    "                    'abstract': abstract, 'mesh_terms': mesh_terms_str\n",
    "                }\n",
    "        except requests.exceptions.RequestException as e_req:\n",
    "            logger.warning(f\"NCBI EFetch batch {i_batch+1} RequestException: {e_req}\")\n",
    "        except ET.ParseError as e_xml:\n",
    "            response_text_snippet = resp.text[:200] if 'resp' in locals() and hasattr(resp, 'text') else \"N/A\"\n",
    "            logger.warning(f\"NCBI EFetch batch {i_batch+1} XML ParseError: {e_xml}. Content: {response_text_snippet}\")\n",
    "        except Exception as e_generic:\n",
    "            logger.error(f\"NCBI EFetch batch {i_batch+1} unexpected error: {e_generic}\", exc_info=True)\n",
    "        \n",
    "        if i_batch < len(batches) - 1: \n",
    "            time.sleep(DELAY_NCBI)\n",
    "\n",
    "    missing_meta_pmids = [p for p in pmids if p not in meta]\n",
    "    if missing_meta_pmids:\n",
    "        logger.warning(f\"Metadata missing for {len(missing_meta_pmids)} PMIDs: {missing_meta_pmids[:10]}...\")\n",
    "    return meta\n",
    "\n",
    "# === STEP 2: OPEN ACCESS (Unpaywall, PMC with PoW) ===\n",
    "def unpaywall_get_pdf_url(doi: str) -> str | None:\n",
    "    if not doi: return None # Skip if no DOI\n",
    "    if not UNPAYWALL_EMAIL or UNPAYWALL_EMAIL == \"your_email@example.com\":\n",
    "        logger.debug(f\"Unpaywall API skipped for DOI {doi}: UNPAYWALL_EMAIL not configured.\")\n",
    "        return None\n",
    "\n",
    "    # Use quote_plus for proper URL encoding of the DOI\n",
    "    api_url = f\"https://api.unpaywall.org/v2/{quote_plus(doi)}?email={UNPAYWALL_EMAIL}\"\n",
    "    logger.info(f\"Unpaywall API GET → DOI {doi}\") # Removed full URL from log for brevity\n",
    "    try:\n",
    "        r = session_oa.get(api_url, timeout=20) # Use the general OA session\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        \n",
    "        if data.get(\"is_oa\"):\n",
    "            pdf_url = None\n",
    "            # Prioritize best_oa_location\n",
    "            best_loc = data.get(\"best_oa_location\")\n",
    "            if best_loc and best_loc.get(\"url_for_pdf\"):\n",
    "                pdf_url = best_loc.get(\"url_for_pdf\")\n",
    "            \n",
    "            # Fallback to checking all oa_locations if best_oa_location has no PDF URL\n",
    "            if not pdf_url:\n",
    "                for loc in data.get(\"oa_locations\", []):\n",
    "                    if loc.get(\"url_for_pdf\"):\n",
    "                        pdf_url = loc.get(\"url_for_pdf\")\n",
    "                        logger.debug(f\"Unpaywall API: Found PDF URL in other oa_locations: {pdf_url}\")\n",
    "                        break # Take the first one found\n",
    "            \n",
    "            if pdf_url:\n",
    "                logger.info(f\"Unpaywall API ✓ DOI {doi}: Found PDF URL: {pdf_url.split('?')[0]}...\") # Log base URL\n",
    "                return pdf_url\n",
    "            else:\n",
    "                logger.info(f\"Unpaywall API ? DOI {doi}: Article is OA, but no direct PDF URL in Unpaywall response.\")\n",
    "        else:\n",
    "            logger.info(f\"Unpaywall API ~ DOI {doi}: Not OA according to Unpaywall.\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e_req:\n",
    "        logger.warning(f\"Unpaywall API ✗ DOI {doi}: RequestException: {e_req}\")\n",
    "    except ValueError as e_json: # Handles JSONDecodeError\n",
    "        logger.warning(f\"Unpaywall API ✗ DOI {doi}: JSON Decode Error: {e_json}. Response: {r.text[:200] if 'r' in locals() else 'N/A'}\")\n",
    "    except Exception as e_generic:\n",
    "        logger.error(f\"Unpaywall API ✗ DOI {doi}: Unexpected error: {e_generic}\", exc_info=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "def pmc_id_for_pmid(pmid: str, article_metadata: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    Finds the PMCID for a given PMID using Entrez ELink.\n",
    "    Prioritizes direct 'pubmed_pmc' link.\n",
    "    article_metadata is passed for potential future verification steps.\n",
    "    \"\"\"\n",
    "    linkname_to_try = \"pubmed_pmc\" # Query only for direct PMC links\n",
    "\n",
    "    params = _get_ncbi_params({\n",
    "        \"dbfrom\": \"pubmed\",\n",
    "        \"db\": \"pmc\",\n",
    "        \"id\": pmid,\n",
    "        \"cmd\": \"neighbor_score\", # Recommended by NCBI for robust linking\n",
    "        \"linkname\": linkname_to_try\n",
    "    })\n",
    "\n",
    "    logger.info(f\"PMC ID ELink → PMID {pmid}: Querying with linkname '{linkname_to_try}'.\")\n",
    "    try:\n",
    "        r = session_ncbi.post(\n",
    "            \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\",\n",
    "            data=params,\n",
    "            timeout=20 \n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        root = ET.fromstring(r.content)\n",
    "\n",
    "        link_set_db_el = root.find(f\".//LinkSetDb[LinkName='{linkname_to_try}']\")\n",
    "\n",
    "        if link_set_db_el is None:\n",
    "            info_el_general = root.find(\".//Info\") \n",
    "            if info_el_general is not None and info_el_general.text:\n",
    "                 logger.info(f\"PMC ID ELink ~ PMID {pmid}: NCBI Info: {info_el_general.text.strip()}\")\n",
    "            else:\n",
    "                 logger.debug(f\"PMC ID ELink ~ PMID {pmid}: No <LinkSetDb> for '{linkname_to_try}'. XML: {r.text[:250]}\")\n",
    "            return None\n",
    "\n",
    "        ids = [el.text.strip() for el in link_set_db_el.findall(\"./Link/Id\") if el.text]\n",
    "\n",
    "        if ids:\n",
    "            pmc_candidate_id_num = ids[0] # Typically one direct link\n",
    "            returned_pmcid = \"\"\n",
    "            if pmc_candidate_id_num.upper().startswith(\"PMC\"):\n",
    "                returned_pmcid = pmc_candidate_id_num\n",
    "            elif pmc_candidate_id_num.isdigit():\n",
    "                returned_pmcid = \"PMC\" + pmc_candidate_id_num\n",
    "            else:\n",
    "                logger.warning(f\"PMC ID ELink ? PMID {pmid}: Non-standard ID '{pmc_candidate_id_num}' from '{linkname_to_try}'.\")\n",
    "                return None \n",
    "            \n",
    "            # Optional: verify_pmcid_against_metadata(returned_pmcid, pmid, article_metadata.get('doi'), article_metadata.get('title'))\n",
    "            logger.info(f\"PMC ID ELink ✓ PMID {pmid}: Found PMCID {returned_pmcid} via '{linkname_to_try}'.\")\n",
    "            return returned_pmcid\n",
    "        else:\n",
    "            info_el = link_set_db_el.find(\"./Info\") # Check for <Info> within this specific LinkSetDb\n",
    "            if info_el is not None and info_el.text:\n",
    "                logger.info(f\"PMC ID ELink ~ PMID {pmid}: NCBI Info for '{linkname_to_try}': {info_el.text.strip()}\")\n",
    "            else:\n",
    "                logger.info(f\"PMC ID ELink ~ PMID {pmid}: No PMCID <Id> elements for '{linkname_to_try}'.\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e_req:\n",
    "        logger.warning(f\"PMC ID ELink ✗ PMID {pmid} ('{linkname_to_try}'): RequestException: {e_req}\")\n",
    "    except ET.ParseError as e_xml:\n",
    "        response_text_snippet = r.text[:250] if 'r' in locals() and hasattr(r, 'text') else \"N/A\"\n",
    "        logger.warning(f\"PMC ID ELink ✗ PMID {pmid} ('{linkname_to_try}'): XML ParseError: {e_xml}. Content: {response_text_snippet}\")\n",
    "    except Exception as e_generic:\n",
    "        logger.error(f\"PMC ID ELink ✗ PMID {pmid} ('{linkname_to_try}'): Unexpected error: {e_generic}\", exc_info=True)\n",
    "    \n",
    "    logger.warning(f\"PMC ID ELink ✗ PMID {pmid}: No PMCID from '{linkname_to_try}' after full attempt.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def pmc_attempt_download(pmcid: str, pmid: str, md: dict) -> bool:\n",
    "    \"\"\"Attempts to download a PDF from PMC, handling PoW.\"\"\"\n",
    "    # Ensure md has the necessary keys for filename generation, or provide defaults\n",
    "    year_val = md.get('year', 'UnknownYear')\n",
    "    author_val = md.get('author', 'UnknownAuthor')\n",
    "    title_val = md.get('title', f'NoTitle_{pmid}')\n",
    "    fname_base = sanitize_filename(f\"{year_val}-{pmid}-{author_val}-{title_val}\")\n",
    "    output_pdf_path  = os.path.join(OUTPUT_PDF_DIR, fname_base + \".pdf\")\n",
    "\n",
    "    if os.path.exists(output_pdf_path): # Should not be strictly necessary if oa_worker checks, but good defense\n",
    "        logger.info(f\"PMC ✓ {pmid} ({pmcid}): PDF already exists at {output_pdf_path} (checked in pmc_attempt_download).\")\n",
    "        return True\n",
    "\n",
    "    # This is the \"landing page\" for the PDF, which should redirect to the actual PDF file URL\n",
    "    pmc_article_pdf_landing_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/\"\n",
    "    final_pdf_url_from_pmc = None # Will hold the URL after HEAD request and redirects\n",
    "    \n",
    "    logger.info(f\"PMC HEAD → PMID {pmid} ({pmcid}): Probing {pmc_article_pdf_landing_url} for final PDF URL.\")\n",
    "    head_req_headers = session_oa.headers.copy()\n",
    "    head_req_headers.update(BROWSER_LIKE_HEADERS) # Use general browser headers\n",
    "    head_req_headers['Accept'] = 'application/pdf, text/html;q=0.9, */*;q=0.8' # Accept PDF or HTML for HEAD\n",
    "    head_req_headers['Sec-Fetch-Site'] = 'cross-site' # Navigating to NCBI\n",
    "\n",
    "    try:\n",
    "        head_resp = session_oa.head(\n",
    "            pmc_article_pdf_landing_url,\n",
    "            headers=head_req_headers,\n",
    "            timeout=(10, 25), # connect, read timeouts\n",
    "            allow_redirects=True # Follow redirects to find the final URL\n",
    "        )\n",
    "        head_resp.raise_for_status()\n",
    "        final_pdf_url_from_pmc = head_resp.url # This is the URL after all redirects\n",
    "        \n",
    "        # Check if the resolved URL looks like a PDF link\n",
    "        if not (final_pdf_url_from_pmc.lower().endswith(\".pdf\") or \"format=pdf\" in final_pdf_url_from_pmc.lower() or \"/pdf/\" in final_pdf_url_from_pmc.lower()):\n",
    "            logger.warning(f\"PMC HEAD ? PMID {pmid} ({pmcid}): Resolved URL {final_pdf_url_from_pmc} doesn't strongly indicate PDF. Proceeding cautiously.\")\n",
    "        else:\n",
    "            logger.info(f\"PMC HEAD ✓ PMID {pmid} ({pmcid}): Resolved potential PDF URL: {final_pdf_url_from_pmc}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.warning(f\"PMC HEAD ✗ PMID {pmid} ({pmcid}) for {pmc_article_pdf_landing_url}: {e}. Will attempt GET on landing URL.\")\n",
    "        # Fallback: if HEAD fails, try to use the original landing page URL for the GET request.\n",
    "        # This might trigger the PoW challenge directly on the landing page.\n",
    "        final_pdf_url_from_pmc = pmc_article_pdf_landing_url \n",
    "    except Exception as e_head_generic:\n",
    "        logger.error(f\"PMC HEAD ✗ PMID {pmid} ({pmcid}): Unexpected error during HEAD request: {e_head_generic}\", exc_info=True)\n",
    "        return False # Cannot proceed if HEAD has critical error\n",
    "\n",
    "    if final_pdf_url_from_pmc:\n",
    "        return download_and_save_pdf(\n",
    "            session_oa, # Use the general OA session for PMC downloads\n",
    "            final_pdf_url_from_pmc, \n",
    "            output_pdf_path, \n",
    "            pmid, \n",
    "            source_name=f\"PMC({pmcid})\",\n",
    "            referer=pmc_article_pdf_landing_url # Referer for the GET can be the initial landing URL\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"PMC ✗ PMID {pmid} ({pmcid}): No URL determined for download attempt after HEAD request.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def oa_worker(pmid: str, md: dict) -> tuple[str, bool]:\n",
    "    \"\"\"Worker for Open Access PDF fetching (Unpaywall then PMC).\"\"\"\n",
    "    try: # MODIFICATION: Added top-level try-except\n",
    "        doi = md.get('doi')\n",
    "        # Ensure md has the necessary keys for filename generation, or provide defaults\n",
    "        year_val = md.get('year', 'UnknownYear')\n",
    "        author_val = md.get('author', 'UnknownAuthor')\n",
    "        title_val = md.get('title', f'NoTitle_{pmid}') # Use pmid in default title if original is missing\n",
    "\n",
    "        fname_base = sanitize_filename(f\"{year_val}-{pmid}-{author_val}-{title_val}\")\n",
    "        output_pdf_path  = os.path.join(OUTPUT_PDF_DIR, fname_base + \".pdf\")\n",
    "\n",
    "        if os.path.exists(output_pdf_path): # Check once at the beginning of worker\n",
    "            logger.info(f\"OA ✓ {pmid}: PDF already exists at {output_pdf_path}\")\n",
    "            return pmid, True\n",
    "\n",
    "        # 1) Unpaywall\n",
    "        if doi:\n",
    "            unpaywall_url = unpaywall_get_pdf_url(doi) # Expects DOI string\n",
    "            if unpaywall_url:\n",
    "                # Use doi.org as referer for Unpaywall links\n",
    "                referer_unpaywall = f\"https://doi.org/{quote_plus(doi)}\"\n",
    "                # MODIFIED LINE: Added article_metadata=md\n",
    "                if download_and_save_pdf(session_oa, unpaywall_url, output_pdf_path, pmid, source_name=f\"Unpaywall(DOI:{doi})\", article_metadata=md, referer=referer_unpaywall):\n",
    "                    return pmid, True\n",
    "                else:\n",
    "                    logger.info(f\"OA: Unpaywall attempt for PMID {pmid} (DOI {doi}) failed download/validation. Trying PMC.\")\n",
    "            else:\n",
    "                logger.info(f\"OA: No PDF URL from Unpaywall for PMID {pmid} (DOI {doi}). Trying PMC.\")\n",
    "        else:\n",
    "            logger.info(f\"OA: No DOI for PMID {pmid}. Skipping Unpaywall, trying PMC.\")\n",
    "        \n",
    "        # 2) PMC fallback\n",
    "        pmcid = pmc_id_for_pmid(pmid, md) # md is passed here\n",
    "        if pmcid:\n",
    "            if pmc_attempt_download(pmcid, pmid, md):\n",
    "                return pmid, True\n",
    "        \n",
    "        logger.warning(f\"OA ✗ {pmid}: No PDF found via Unpaywall or PMC.\")\n",
    "        return pmid, False\n",
    "    \n",
    "    except Exception as e_oa_worker: # Catch any unexpected error within oa_worker\n",
    "        logger.error(f\"OA Worker UNHANDLED EXCEPTION for PMID {pmid}: {e_oa_worker}\", exc_info=True)\n",
    "        return pmid, False # Ensure a tuple is always returned, flagging as failure\n",
    "\n",
    "# === STEP 3: SCI-HUB ===\n",
    "def test_scihub_domain(domain: str) -> bool:\n",
    "    \"\"\"Tests if a Sci-Hub domain is responsive.\"\"\"\n",
    "    test_doi = \"10.1000/182\" # A generic, usually available test DOI\n",
    "    url = f\"{domain.rstrip('/')}/{test_doi}\"\n",
    "    logger.debug(f\"Sci-Hub TEST GET → {url}\")\n",
    "    try:\n",
    "        # Use a shorter timeout for domain testing\n",
    "        r = session_scihub.get(url, timeout=10, headers=BROWSER_LIKE_HEADERS) \n",
    "        # Sci-Hub can return 200 with HTML, or 404 if DOI not found, both indicate responsiveness\n",
    "        # Also check for common Sci-Hub page elements if status is 200 but not obvious HTML\n",
    "        if r.status_code == 200 and ('html' in r.headers.get('Content-Type','').lower() or \\\n",
    "                                     any(kw in r.text.lower() for kw in ['sci-hub', 'save', 'download', '<button id=\"download\">'])):\n",
    "            logger.info(f\"Sci-Hub TEST ✓ {domain} is responsive (status {r.status_code}).\")\n",
    "            return True\n",
    "        elif r.status_code == 404: # 404 for a test DOI is also a sign the domain itself is working\n",
    "             logger.info(f\"Sci-Hub TEST ✓ {domain} is responsive (status 404, expected for non-existent test DOI).\")\n",
    "             return True\n",
    "        else:\n",
    "            logger.warning(f\"Sci-Hub TEST ? {domain} responded status {r.status_code}, CT: {r.headers.get('Content-Type','')}. Text snippet: {r.text[:100]}\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.warning(f\"Sci-Hub TEST ✗ {domain} timed out.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.warning(f\"Sci-Hub TEST ✗ {domain} error: {e}\")\n",
    "    except Exception as e_generic_test: # Catch any other error during test\n",
    "        logger.error(f\"Sci-Hub TEST ✗ {domain} unexpected error: {e_generic_test}\", exc_info=True)\n",
    "    return False\n",
    "\n",
    "def init_scihub_domains() -> list[str]:\n",
    "    \"\"\"Probes Sci-Hub domains for availability and returns a list of working ones.\"\"\"\n",
    "    logger.info(\"Probing Sci-Hub mirrors for availability...\")\n",
    "    working_domains = []\n",
    "    # Use a ThreadPoolExecutor to test domains in parallel for speed\n",
    "    # Limit max_workers for domain testing to avoid too many simultaneous requests\n",
    "    with ThreadPoolExecutor(max_workers=min(len(SCI_HUB_DOMAINS), 3), thread_name_prefix=\"SciHub_Domain_Test\") as executor:\n",
    "        future_to_domain = {executor.submit(test_scihub_domain, d): d for d in SCI_HUB_DOMAINS}\n",
    "        for future in as_completed(future_to_domain):\n",
    "            domain = future_to_domain[future]\n",
    "            try:\n",
    "                if future.result(): # result() is True if domain is working\n",
    "                    working_domains.append(domain)\n",
    "            except Exception as exc: # Catch any exception during future.result() itself\n",
    "                logger.error(f\"Sci-Hub domain test for {domain} generated an exception during result retrieval: {exc}\")\n",
    "    \n",
    "    if not working_domains:\n",
    "        logger.error(\"CRITICAL: No working Sci-Hub domains found after testing!\")\n",
    "    else:\n",
    "        # Prioritize .se if available as it's often reliable\n",
    "        # (This is a simple heuristic, actual reliability can vary)\n",
    "        if \"https://sci-hub.se\" in working_domains:\n",
    "            working_domains.insert(0, working_domains.pop(working_domains.index(\"https://sci-hub.se\")))\n",
    "        logger.info(f\"Using Sci-Hub domains: {working_domains}\")\n",
    "    return working_domains\n",
    "\n",
    "\n",
    "def find_scihub_pdf_in_html(html_content: bytes, base_page_url: str) -> str | None:\n",
    "    \"\"\"Parses Sci-Hub HTML content to find the direct PDF link.\"\"\"\n",
    "    # Ensure html_content is bytes for BeautifulSoup, then decode for regex if needed\n",
    "    # For BS4, it's often better to let it handle encoding detection from bytes.\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    parsed_base_url = urlparse(base_page_url)\n",
    "    # Ensure absolute_base correctly reconstructs scheme and netloc\n",
    "    absolute_base = f\"{parsed_base_url.scheme}://{parsed_base_url.netloc}\"\n",
    "\n",
    "    # Selectors for common PDF embedding methods on Sci-Hub\n",
    "    # Order can matter; more specific selectors first.\n",
    "    selectors_and_attrs = [\n",
    "        ('iframe#pdf', 'src'),             # Specific iframe ID often used\n",
    "        ('iframe#article', 'src'),         # Another common iframe ID\n",
    "        ('embed[type=\"application/pdf\"]', 'src'), # Embed tag for PDFs\n",
    "        ('iframe[src*=\".pdf\"]', 'src'),    # Any iframe whose src contains .pdf\n",
    "        ('a#download', 'href'),            # Common download button ID\n",
    "        ('div.buttons > a[href*=\".pdf\"]', 'href'), # Anchor in buttons div linking to PDF\n",
    "        ('div#buttons > a[href*=\".pdf\"]', 'href'), # Variation\n",
    "        ('a[href*=\".pdf\"]', 'href')        # General anchor tags linking to PDFs (less specific)\n",
    "    ]\n",
    "\n",
    "    for selector, attr in selectors_and_attrs:\n",
    "        element = soup.select_one(selector)\n",
    "        if element and element.get(attr):\n",
    "            src_val = element.get(attr)\n",
    "            # Handle protocol-relative URLs (e.g., //example.com/file.pdf)\n",
    "            if src_val.startswith(\"//\"): \n",
    "                src_val = f\"{parsed_base_url.scheme}:{src_val}\"\n",
    "            \n",
    "            # Avoid JavaScript or data URIs\n",
    "            if not src_val.lower().startswith(('data:', 'javascript:')):\n",
    "                # Resolve relative URLs to absolute ones\n",
    "                resolved_url = urljoin(absolute_base, src_val) \n",
    "                # Basic check: does it look like a PDF link or is it on a known Sci-Hub domain?\n",
    "                # This helps filter out non-PDF links that might match selectors.\n",
    "                if \".pdf\" in resolved_url.lower() or any(sh_domain_base in resolved_url for sh_domain_base in [urlparse(d).netloc for d in SCI_HUB_DOMAINS]):\n",
    "                    logger.debug(f\"Sci-Hub HTML Parse: Found PDF link '{resolved_url}' using selector '{selector}'\")\n",
    "                    return resolved_url\n",
    "\n",
    "    # Fallback for 'location.href' in onclick attributes (less reliable, more prone to false positives)\n",
    "    # Try to be more specific with regex to avoid matching unrelated JS.\n",
    "    onclick_buttons = soup.select('button[onclick*=\"location.href\"], a[onclick*=\"location.href\"]')\n",
    "    for button in onclick_buttons:\n",
    "        onclick_val = button.get('onclick', '')\n",
    "        # Regex to find .pdf URLs within location.href assignments\n",
    "        match = re.search(r\"location\\.href\\s*=\\s*['\\\"]([^'\\\"]+\\.pdf[^'\\\"]*)['\\\"]\", onclick_val, re.IGNORECASE)\n",
    "        if match:\n",
    "            href = match.group(1).strip()\n",
    "            if href.startswith(\"//\"): href = f\"{parsed_base_url.scheme}:{href}\"\n",
    "            resolved_url = urljoin(absolute_base, href)\n",
    "            logger.debug(f\"Sci-Hub HTML Parse: Found PDF link '{resolved_url}' from onclick attribute.\")\n",
    "            return resolved_url\n",
    "            \n",
    "    logger.debug(f\"Sci-Hub HTML Parse: No obvious PDF link found in HTML from {base_page_url}\")\n",
    "    return None\n",
    "\n",
    "def scihub_worker(identifier: str, pmid: str, md: dict, active_domains: list) -> tuple[str, bool]:\n",
    "    \"\"\"Worker for Sci-Hub PDF fetching.\"\"\"\n",
    "    # Ensure md has the necessary keys for filename generation\n",
    "    year_val = md.get('year', 'UnknownYear')\n",
    "    author_val = md.get('author', 'UnknownAuthor')\n",
    "    title_val = md.get('title', f'NoTitle_{pmid}')\n",
    "    fname_base = sanitize_filename(f\"{year_val}-{pmid}-{author_val}-{title_val}\")\n",
    "    output_pdf_path = os.path.join(OUTPUT_PDF_DIR, fname_base + \".pdf\")\n",
    "\n",
    "    if os.path.exists(output_pdf_path): # Check if already downloaded\n",
    "        logger.info(f\"Sci-Hub ✓ {pmid}: PDF already exists at {output_pdf_path} (checked in scihub_worker).\")\n",
    "        return pmid, True\n",
    "    \n",
    "    if not active_domains: # Should be caught by main logic, but defensive check\n",
    "        logger.error(f\"Sci-Hub ✗ {pmid}: No active Sci-Hub domains to try for identifier '{identifier}'.\")\n",
    "        return pmid, False\n",
    "\n",
    "    for i, domain_url in enumerate(active_domains): # Iterate through available domains\n",
    "        # Construct Sci-Hub page URL using the identifier (DOI or PMID)\n",
    "        scihub_page_url = f\"{domain_url.rstrip('/')}/{quote_plus(identifier)}\"\n",
    "        logger.info(f\"Sci-Hub HTML GET → PMID {pmid} from {scihub_page_url} (Attempt {i+1}/{len(active_domains)})\")\n",
    "        \n",
    "        sh_headers = session_scihub.headers.copy() # Use Sci-Hub specific session\n",
    "        sh_headers.update(BROWSER_LIKE_HEADERS)    # Add general browser headers\n",
    "        sh_headers['Sec-Fetch-Site'] = 'none'      # Initial request to SH is 'none' or 'cross-site'\n",
    "\n",
    "        try:\n",
    "            r_page = session_scihub.get(scihub_page_url, headers=sh_headers, timeout=30) # Increased timeout for Sci-Hub page load\n",
    "            r_page.raise_for_status() # Check for HTTP errors\n",
    "            page_content_type = r_page.headers.get('Content-Type','').lower()\n",
    "\n",
    "            # Case 1: Sci-Hub sometimes directly serves PDF if it's cached that way or is the final link\n",
    "            if 'application/pdf' in page_content_type:\n",
    "                logger.info(f\"Sci-Hub ? PMID {pmid}: URL {scihub_page_url} served PDF directly. Attempting download...\")\n",
    "                # Use r_page.url as it might have redirected to the actual PDF URL\n",
    "                if download_and_save_pdf(session_scihub, r_page.url, output_pdf_path, pmid, source_name=f\"SciHub_Direct({domain_url})\", referer=domain_url):\n",
    "                    return pmid, True\n",
    "                else:\n",
    "                    logger.warning(f\"Sci-Hub ✗ PMID {pmid}: Direct PDF from {scihub_page_url} failed validation or download.\")\n",
    "                    continue # Try next domain if direct download fails\n",
    "\n",
    "            # Case 2: Most common - HTML viewer page\n",
    "            # Check if content starts like HTML, as Content-Type can be misleading\n",
    "            elif 'html' in page_content_type or r_page.content[:100].strip().lower().startswith((b'<!doctype html', b'<html')):\n",
    "                # Pass r_page.content (bytes) to find_scihub_pdf_in_html\n",
    "                pdf_url_from_html = find_scihub_pdf_in_html(r_page.content, r_page.url) \n",
    "                \n",
    "                if pdf_url_from_html:\n",
    "                    logger.info(f\"Sci-Hub HTML ✓ PMID {pmid}: Found potential PDF link: {pdf_url_from_html.split('?')[0]}...\")\n",
    "                    # Use session_scihub for downloading the extracted PDF link\n",
    "                    if download_and_save_pdf(session_scihub, pdf_url_from_html, output_pdf_path, pmid, source_name=f\"SciHub_Extracted({domain_url})\", referer=scihub_page_url):\n",
    "                        return pmid, True\n",
    "                    # else: if download_and_save_pdf fails, loop continues to next domain\n",
    "                else:\n",
    "                    logger.warning(f\"Sci-Hub HTML ✗ {pmid} via {domain_url}: No PDF link found within HTML from {scihub_page_url}\")\n",
    "            else: \n",
    "                logger.warning(f\"Sci-Hub ✗ {pmid} via {domain_url}: Unexpected Content-Type '{page_content_type}' from {scihub_page_url}. Snippet: {r_page.text[:100]}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e_req:\n",
    "            logger.warning(f\"Sci-Hub ✗ {pmid} via {domain_url} ({scihub_page_url}): RequestException: {e_req}\")\n",
    "        except Exception as e_sh_domain: # Catch broader exceptions for one domain attempt\n",
    "            logger.error(f\"Sci-Hub ✗ {pmid} via {domain_url} ({scihub_page_url}): General error {e_sh_domain.__class__.__name__}: {e_sh_domain}\", exc_info=True)\n",
    "        \n",
    "        if len(active_domains) > 1 and i < len(active_domains) - 1: # If not the last domain\n",
    "            time.sleep(DELAY_SCIHUB) # Delay before trying the next Sci-Hub domain\n",
    "\n",
    "    logger.error(f\"Sci-Hub ✗ {pmid}: Failed for identifier '{identifier}' after trying all active domains.\")\n",
    "    return pmid, False\n",
    "\n",
    "def log_enhanced_failure_details(log_filename: str, pmid: str, metadata: dict, \n",
    "                                 failure_type: str, details: str | None = None, \n",
    "                                 file_path: str | None = None, logged_set: set | None = None):\n",
    "    \"\"\"Logs detailed information about a failed or suspicious PMID to a specified log file.\"\"\"\n",
    "    if logged_set is not None: # Allow disabling the duplicate check if needed for some reason\n",
    "        if pmid in logged_set:\n",
    "            logger.debug(f\"PMID {pmid} already logged in {log_filename}. Skipping duplicate entry.\")\n",
    "            return \n",
    "    \n",
    "    # Ensure metadata is not None and provide defaults if keys are missing\n",
    "    if metadata is None: \n",
    "        metadata = {} \n",
    "        logger.warning(f\"No metadata provided for PMID {pmid} during detailed logging to {log_filename}.\")\n",
    "\n",
    "    entry_lines = [\n",
    "        \"----------------------------------------\",\n",
    "        f\"PMID: {pmid}\",\n",
    "        f\"Failure Type: {failure_type}\"\n",
    "    ]\n",
    "    if details: \n",
    "        entry_lines.append(f\"Details: {details}\")\n",
    "    if file_path: \n",
    "        entry_lines.append(f\"File Path: {os.path.abspath(file_path)}\") # Log absolute path\n",
    "    \n",
    "    entry_lines.extend([\n",
    "        f\"DOI: {metadata.get('doi', 'N/A')}\",\n",
    "        f\"Year: {metadata.get('year', 'N/A')}\",\n",
    "        f\"Author: {metadata.get('author', 'N/A')}\",\n",
    "        f\"Title: {metadata.get('title', 'N/A')}\",\n",
    "        # Abstract can be long, ensure it's handled well (already string with newlines)\n",
    "        f\"Abstract:\\n{metadata.get('abstract', 'N/A')}\",\n",
    "        f\"MeSH Terms: {metadata.get('mesh_terms', 'N/A')}\",\n",
    "        \"----------------------------------------\\n\" # Extra newline for separation\n",
    "    ])\n",
    "    \n",
    "    log_file_full_path = os.path.join(OUTPUT_PDF_DIR, log_filename)\n",
    "    try:\n",
    "        with open(log_file_full_path, \"a\", encoding=\"utf-8\") as f_log:\n",
    "            f_log.write(\"\\n\".join(entry_lines))\n",
    "        if logged_set is not None:\n",
    "            logged_set.add(pmid)\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Could not write to enhanced log file {log_file_full_path}: {e}\")\n",
    "        \n",
    "# === MAIN SCRIPT EXECUTION ===\n",
    "def main():\n",
    "    t_start = time.time()\n",
    "    logger.info(f\"=== PDF Fetcher v12-pow started at {time.strftime('%Y-%m-%d %H:%M:%S')} ===\")\n",
    "\n",
    "    # --- File Logging Setup (Optional) ---\n",
    "    # log_file_handler_path = os.path.join(OUTPUT_PDF_DIR, \"pdf_fetcher_v12_run.log\")\n",
    "    # try:\n",
    "    #     os.makedirs(OUTPUT_PDF_DIR, exist_ok=True) \n",
    "    #     fh = logging.FileHandler(log_file_handler_path, mode='a') # Append mode\n",
    "    #     fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - [%(threadName)s] - %(funcName)s - %(message)s'))\n",
    "    #     logger.addHandler(fh)\n",
    "    #     logger.info(f\"Detailed logging to file: {os.path.abspath(log_file_handler_path)}\")\n",
    "    # except Exception as e_log_file:\n",
    "    #     logger.error(f\"Could not set up file logging to {log_file_handler_path}: {e_log_file}\")\n",
    "    # --- End File Logging Setup ---\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(EXCEL_FILE_PATH)\n",
    "        if 'PMID' not in df.columns:\n",
    "            logger.error(f\"Excel file {EXCEL_FILE_PATH} must contain a 'PMID' column.\")\n",
    "            return\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Excel file not found: {EXCEL_FILE_PATH}\")\n",
    "        return\n",
    "    except Exception as e_excel: # Catch other potential pandas/excel reading errors\n",
    "        logger.error(f\"Cannot read Excel file {EXCEL_FILE_PATH}: {e_excel}\", exc_info=True)\n",
    "        return\n",
    "    \n",
    "    # Clean and validate PMIDs from the Excel sheet\n",
    "    pmids_raw = df['PMID'].dropna().unique()\n",
    "    pmids = []\n",
    "    for p_raw in pmids_raw:\n",
    "        try:\n",
    "            # Attempt to convert to float then int to handle numbers like \"12345.0\"\n",
    "            pmids.append(str(int(float(str(p_raw))))) \n",
    "        except ValueError:\n",
    "            logger.warning(f\"Skipping invalid PMID format in Excel: '{p_raw}'\")\n",
    "    \n",
    "    if not pmids:\n",
    "        logger.error(\"No valid PMIDs found in the Excel file.\")\n",
    "        return\n",
    "    logger.info(f\"Loaded {len(pmids)} unique, valid PMIDs from {EXCEL_FILE_PATH}\")\n",
    "\n",
    "    metadata_dict = fetch_metadata(pmids)\n",
    "    # Filter PMIDs to only those for which metadata was successfully fetched\n",
    "    valid_pmids_with_meta = [p for p in pmids if p in metadata_dict and metadata_dict[p]]\n",
    "    \n",
    "    logger.info(f\"Successfully fetched metadata for {len(valid_pmids_with_meta)} PMIDs.\")\n",
    "    if not valid_pmids_with_meta:\n",
    "        logger.error(\"No metadata could be fetched for any valid PMIDs. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    try:\n",
    "        os.makedirs(OUTPUT_PDF_DIR, exist_ok=True)\n",
    "        os.makedirs(os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR), exist_ok=True)\n",
    "        logger.info(f\"PDFs will be saved to: {os.path.abspath(OUTPUT_PDF_DIR)}\")\n",
    "        logger.info(f\"Suspicious/failed validation files will be in: {os.path.abspath(os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR))}\")\n",
    "    except OSError as e_mkdir:\n",
    "        logger.error(f\"Could not create output directories: {e_mkdir}. Exiting.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    logger.info(\"--- Starting Open Access Download Phase ---\")\n",
    "    oa_succeeded_pmids, oa_failed_pmids = [], []\n",
    "    # Using ThreadPoolExecutor for concurrent OA downloads\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREADS, thread_name_prefix=\"OA_Worker\") as executor:\n",
    "        # Submit all OA tasks\n",
    "        future_to_pmid_oa = {\n",
    "            executor.submit(oa_worker, pmid, metadata_dict[pmid]): pmid\n",
    "            for pmid in valid_pmids_with_meta # Only process PMIDs for which we have metadata\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in as_completed(future_to_pmid_oa):\n",
    "            pmid_processed = future_to_pmid_oa[future]\n",
    "            try:\n",
    "                # future.result() will return the (pmid, success_status) tuple from oa_worker\n",
    "                # or re-raise an exception if one occurred in the worker and wasn't caught by its top-level try-except\n",
    "                worker_result = future.result() \n",
    "                \n",
    "                if worker_result is None: # Should ideally not happen with the worker's top-level try-except\n",
    "                    logger.error(f\"OA Thread ✗ For PMID {pmid_processed}, future.result() was None. Treating as failure.\")\n",
    "                    oa_failed_pmids.append(pmid_processed)\n",
    "                elif isinstance(worker_result, tuple) and len(worker_result) == 2:\n",
    "                    returned_pmid, success_status = worker_result\n",
    "                    # Sanity check: returned_pmid should match pmid_processed\n",
    "                    if returned_pmid != pmid_processed:\n",
    "                         logger.warning(f\"OA Thread ? Mismatch in returned PMID: expected {pmid_processed}, got {returned_pmid}. Processing with {pmid_processed}.\")\n",
    "                    \n",
    "                    if success_status:\n",
    "                        oa_succeeded_pmids.append(pmid_processed)\n",
    "                    else:\n",
    "                        oa_failed_pmids.append(pmid_processed)\n",
    "                else: # Worker returned something unexpected\n",
    "                    logger.error(f\"OA Thread ✗ For PMID {pmid_processed}, worker returned unexpected result: {worker_result}. Treating as failure.\")\n",
    "                    oa_failed_pmids.append(pmid_processed)\n",
    "\n",
    "            except Exception as e_thread: # Catch exceptions raised by future.result()\n",
    "                logger.error(f\"OA Thread ✗ Exception processing PMID {pmid_processed}: {e_thread}\", exc_info=True)\n",
    "                oa_failed_pmids.append(pmid_processed) # Add to failed list if an exception occurs\n",
    "    \n",
    "    logger.info(f\"Open Access Phase Summary: {len(oa_succeeded_pmids)} PDFs successfully downloaded and validated.\")\n",
    "    if oa_failed_pmids:\n",
    "        logger.info(f\"{len(oa_failed_pmids)} PMIDs not fetched via OA or failed validation: {sorted(oa_failed_pmids)[:10]}...\")\n",
    "\n",
    "\n",
    "    # --- Sci-Hub Phase for PMIDs that failed OA ---\n",
    "    sci_hub_succeeded_pmids = []\n",
    "    final_still_failed_pmids = list(oa_failed_pmids) # Initialize with those that failed OA\n",
    "\n",
    "    if oa_failed_pmids: # Only run Sci-Hub phase if there are PMIDs that failed OA\n",
    "        logger.info(\"--- Starting Sci-Hub Download Phase for Remaining PMIDs ---\")\n",
    "        active_scihub_domains = init_scihub_domains()\n",
    "        \n",
    "        if not active_scihub_domains:\n",
    "            logger.error(\"Sci-Hub phase skipped: No active Sci-Hub domains found.\")\n",
    "            # final_still_failed_pmids remains as oa_failed_pmids\n",
    "        else:\n",
    "            # PMIDs that succeeded in OA are removed from the list to try with Sci-Hub\n",
    "            pmids_for_scihub = list(oa_failed_pmids) # Create a copy to modify\n",
    "            final_still_failed_pmids = [] # Reset for this phase; will be populated by Sci-Hub failures\n",
    "\n",
    "            # Reduce threads for Sci-Hub to be kinder, max(1, ...) ensures at least 1 worker\n",
    "            scihub_max_workers = max(1, MAX_THREADS // 2 if MAX_THREADS > 1 else 1)\n",
    "            with ThreadPoolExecutor(max_workers=scihub_max_workers, thread_name_prefix=\"SciHub_Worker\") as executor_sh:\n",
    "                future_to_pmid_scihub = {}\n",
    "                for pmid_to_try_scihub in pmids_for_scihub:\n",
    "                    if pmid_to_try_scihub not in metadata_dict: # Should not happen if using valid_pmids_with_meta\n",
    "                        logger.warning(f\"Sci-Hub: Metadata missing for PMID {pmid_to_try_scihub}, skipping.\")\n",
    "                        final_still_failed_pmids.append(pmid_to_try_scihub)\n",
    "                        continue\n",
    "                        \n",
    "                    meta_for_pmid = metadata_dict[pmid_to_try_scihub]\n",
    "                    # Sci-Hub prefers DOI, falls back to PMID if DOI is not available or invalid\n",
    "                    identifier_for_scihub = meta_for_pmid.get('doi') if meta_for_pmid.get('doi') else pmid_to_try_scihub\n",
    "                    \n",
    "                    future_to_pmid_scihub[executor_sh.submit(\n",
    "                        scihub_worker, \n",
    "                        identifier_for_scihub, \n",
    "                        pmid_to_try_scihub, \n",
    "                        meta_for_pmid, \n",
    "                        active_scihub_domains\n",
    "                    )] = pmid_to_try_scihub\n",
    "                \n",
    "                for future_sh in as_completed(future_to_pmid_scihub):\n",
    "                    pmid_processed_scihub = future_to_pmid_scihub[future_sh]\n",
    "                    try:\n",
    "                        sh_worker_result = future_sh.result()\n",
    "                        if sh_worker_result is None:\n",
    "                            logger.error(f\"Sci-Hub Thread ✗ For PMID {pmid_processed_scihub}, future.result() was None. Treating as failure.\")\n",
    "                            final_still_failed_pmids.append(pmid_processed_scihub)\n",
    "                        elif isinstance(sh_worker_result, tuple) and len(sh_worker_result) == 2:\n",
    "                            _, success_status_scihub = sh_worker_result\n",
    "                            if success_status_scihub:\n",
    "                                sci_hub_succeeded_pmids.append(pmid_processed_scihub)\n",
    "                            else:\n",
    "                                final_still_failed_pmids.append(pmid_processed_scihub)\n",
    "                        else:\n",
    "                            logger.error(f\"Sci-Hub Thread ✗ For PMID {pmid_processed_scihub}, worker returned unexpected result: {sh_worker_result}. Treating as failure.\")\n",
    "                            final_still_failed_pmids.append(pmid_processed_scihub)\n",
    "                    except Exception as e_sh_thread:\n",
    "                        logger.error(f\"Sci-Hub Thread ✗ Exception processing PMID {pmid_processed_scihub}: {e_sh_thread}\", exc_info=True)\n",
    "                        final_still_failed_pmids.append(pmid_processed_scihub)\n",
    "\n",
    "            logger.info(f\"Sci-Hub Phase Summary: {len(sci_hub_succeeded_pmids)} PDFs successfully downloaded and validated.\")\n",
    "            if final_still_failed_pmids:\n",
    "                logger.info(f\"{len(final_still_failed_pmids)} PMIDs still missing after Sci-Hub attempts or validation: {sorted(final_still_failed_pmids)[:10]}...\")\n",
    "    else: \n",
    "        logger.info(\"--- Sci-Hub Download Phase Skipped: No PMIDs failed the Open Access phase. ---\")\n",
    "        # final_still_failed_pmids is already an empty list if oa_failed_pmids was empty\n",
    "\n",
    "\n",
    "    total_succeeded = len(oa_succeeded_pmids) + len(sci_hub_succeeded_pmids)\n",
    "    total_time_taken = time.time() - t_start\n",
    "    logger.info(\"--- Overall Summary ---\")\n",
    "    logger.info(f\"Processed {len(pmids)} unique input PMIDs.\")\n",
    "    logger.info(f\"Attempted downloads for {len(valid_pmids_with_meta)} PMIDs (those with metadata).\")\n",
    "    logger.info(f\"Total PDFs successfully downloaded & validated: {total_succeeded} / {len(valid_pmids_with_meta)}.\")\n",
    "    logger.info(f\"  - Via Open Access (Unpaywall/PMC): {len(oa_succeeded_pmids)}\")\n",
    "    logger.info(f\"  - Via Sci-Hub: {len(sci_hub_succeeded_pmids)}\")\n",
    "    \n",
    "    # Ensure final_still_failed_pmids is a unique list of PMIDs that were attempted but not successfully retrieved\n",
    "    # This list should contain PMIDs that failed both OA and Sci-Hub (if Sci-Hub was attempted for them)\n",
    "    # Or PMIDs that failed OA and Sci-Hub was not run/successful for them.\n",
    "    \n",
    "    # Recalculate final_still_failed_pmids based on what was actually attempted and not in succeeded lists\n",
    "    all_attempted_pmids = set(valid_pmids_with_meta)\n",
    "    all_succeeded_pmids = set(oa_succeeded_pmids) | set(sci_hub_succeeded_pmids)\n",
    "    final_truly_failed_pmids = sorted(list(all_attempted_pmids - all_succeeded_pmids))\n",
    "\n",
    "    if final_truly_failed_pmids:\n",
    "        logger.info(f\"Total PMIDs ultimately NOT downloaded or failed validation: {len(final_truly_failed_pmids)}\")\n",
    "        # Log all failed PMIDs for easier review\n",
    "        failed_pmids_log_path = os.path.join(OUTPUT_PDF_DIR, \"failed_pmids_v12.log\")\n",
    "        try:\n",
    "            with open(failed_pmids_log_path, \"w\") as f_failed:\n",
    "                for p_fail in final_truly_failed_pmids:\n",
    "                    f_failed.write(f\"{p_fail}\\n\")\n",
    "            logger.info(f\"List of all failed/missing PMIDs saved to: {failed_pmids_log_path}\")\n",
    "        except IOError as e_io_failed:\n",
    "            logger.error(f\"Could not write failed PMIDs log to {failed_pmids_log_path}: {e_io_failed}\")\n",
    "    else:\n",
    "        if total_succeeded == len(valid_pmids_with_meta) and len(valid_pmids_with_meta) > 0:\n",
    "             logger.info(\"All requested PDFs (with metadata) successfully downloaded and validated!\")\n",
    "        elif len(valid_pmids_with_meta) == 0 : # Should have been caught earlier\n",
    "             logger.info(\"No PMIDs with metadata were available to attempt download.\")\n",
    "        else: # total_succeeded might be less than valid_pmids_with_meta but list is empty (should not happen)\n",
    "            logger.info(\"No PMIDs failed, but counts suggest some were not processed. Review logs.\")\n",
    "\n",
    "\n",
    "    logger.info(f\"Total execution time: {total_time_taken:.2f} seconds.\")\n",
    "    logger.info(f\"=== PDF Fetcher v12-pow completed at {time.strftime('%Y-%m-%d %H:%M:%S')} ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac68a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 13:41:52,044 - INFO - [MainThread] - main - === PDF Fetcher v12-pow started at 2025-05-22 13:41:52 ===\n",
      "2025-05-22 13:41:52,257 - INFO - [MainThread] - main - Loaded 32 unique, valid PMIDs from C:\\Users\\Galaxy\\Downloads\\screening_ERAS.xlsx\n",
      "2025-05-22 13:41:52,257 - INFO - [MainThread] - fetch_metadata - EFetch PubMed metadata for 32 PMIDs in 1 batch(es)...\n",
      "2025-05-22 13:41:52,258 - INFO - [MainThread] - fetch_metadata - NCBI EFetch POST (batch 1/1) → IDs=39955421,40340819,39068053...\n",
      "2025-05-22 13:41:53,152 - INFO - [MainThread] - main - Successfully fetched metadata for 32 PMIDs.\n",
      "2025-05-22 13:41:53,154 - INFO - [MainThread] - main - PDFs will be saved to: c:\\Users\\Galaxy\\LEVI\\jupyter\\litscape\\downloaded_pdfs_v12_pow\n",
      "2025-05-22 13:41:53,154 - INFO - [MainThread] - main - Suspicious/failed validation files will be in: c:\\Users\\Galaxy\\LEVI\\jupyter\\litscape\\downloaded_pdfs_v12_pow\\suspicious_pdfs\n",
      "2025-05-22 13:41:53,155 - INFO - [MainThread] - main - --- Starting Open Access Download Phase ---\n",
      "2025-05-22 13:41:53,158 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1007/s00383-025-05977-0\n",
      "2025-05-22 13:41:53,161 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1136/bmjpo-2024-003280\n",
      "2025-05-22 13:41:53,165 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2024.06.021\n",
      "2025-05-22 13:41:53,171 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1136/bmjpo-2024-002824\n",
      "2025-05-22 13:41:53,174 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.3390/jpm14040411\n",
      "2025-05-22 13:41:53,810 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API ✓ DOI 10.1136/bmjpo-2024-003280: Found PDF URL: https://bmjpaedsopen.bmj.com/content/9/1/e003280.full.pdf...\n",
      "2025-05-22 13:41:53,811 - INFO - [OA_Worker_1] - download_and_save_pdf - Unpaywall(DOI:10.1136/bmjpo-2024-003280) → PMID 40340819: Attempting download from https://bmjpaedsopen.bmj.com/content/9/1/e003280.full.pdf\n",
      "2025-05-22 13:41:53,819 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1007/s00383-025-05977-0: Not OA according to Unpaywall.\n",
      "2025-05-22 13:41:53,822 - INFO - [OA_Worker_0] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39955421 (DOI 10.1007/s00383-025-05977-0). Trying PMC.\n",
      "2025-05-22 13:41:53,823 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API ? DOI 10.1136/bmjpo-2024-002824: Article is OA, but no direct PDF URL in Unpaywall response.\n",
      "2025-05-22 13:41:53,828 - INFO - [OA_Worker_3] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39384309 (DOI 10.1136/bmjpo-2024-002824). Trying PMC.\n",
      "2025-05-22 13:41:53,826 - INFO - [OA_Worker_0] - pmc_id_for_pmid - PMC ID ELink → PMID 39955421: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:53,827 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ? DOI 10.1016/j.jpedsurg.2024.06.021: Article is OA, but no direct PDF URL in Unpaywall response.\n",
      "2025-05-22 13:41:53,826 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API ✓ DOI 10.3390/jpm14040411: Found PDF URL: https://www.mdpi.com/2075-4426/14/4/411/pdf...\n",
      "2025-05-22 13:41:53,828 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink → PMID 39384309: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:53,832 - INFO - [OA_Worker_2] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39068053 (DOI 10.1016/j.jpedsurg.2024.06.021). Trying PMC.\n",
      "2025-05-22 13:41:53,833 - INFO - [OA_Worker_4] - download_and_save_pdf - Unpaywall(DOI:10.3390/jpm14040411) → PMID 38673038: Attempting download from https://www.mdpi.com/2075-4426/14/4/411/pdf?version=1712927058\n",
      "2025-05-22 13:41:53,836 - INFO - [OA_Worker_2] - pmc_id_for_pmid - PMC ID ELink → PMID 39068053: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:54,008 - WARNING - [OA_Worker_4] - download_and_save_pdf - Unpaywall(DOI:10.3390/jpm14040411) ✗ PMID 38673038: RequestException for https://www.mdpi.com/2075-4426/14/4/411/pdf?version=1712927058: 403 Client Error: Forbidden for url: https://www.mdpi.com/2075-4426/14/4/411/pdf?version=1712927058\n",
      "2025-05-22 13:41:54,009 - INFO - [OA_Worker_4] - oa_worker - OA: Unpaywall attempt for PMID 38673038 (DOI 10.3390/jpm14040411) failed download/validation. Trying PMC.\n",
      "2025-05-22 13:41:54,009 - INFO - [OA_Worker_4] - pmc_id_for_pmid - PMC ID ELink → PMID 38673038: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:54,068 - WARNING - [OA_Worker_0] - oa_worker - OA ✗ 39955421: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:41:54,069 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2023.01.028\n",
      "2025-05-22 13:41:54,266 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1016/j.jpedsurg.2023.01.028: Not OA according to Unpaywall.\n",
      "2025-05-22 13:41:54,267 - INFO - [OA_Worker_0] - oa_worker - OA: No PDF URL from Unpaywall for PMID 36788057 (DOI 10.1016/j.jpedsurg.2023.01.028). Trying PMC.\n",
      "2025-05-22 13:41:54,268 - INFO - [OA_Worker_0] - pmc_id_for_pmid - PMC ID ELink → PMID 36788057: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:54,309 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink ✓ PMID 39384309: Found PMCID PMC11474870 via 'pubmed_pmc'.\n",
      "2025-05-22 13:41:54,310 - INFO - [OA_Worker_3] - pmc_attempt_download - PMC HEAD → PMID 39384309 (PMC11474870): Probing https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11474870/pdf/ for final PDF URL.\n",
      "2025-05-22 13:41:54,369 - WARNING - [OA_Worker_2] - oa_worker - OA ✗ 39068053: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:41:54,370 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1111/aas.13572\n",
      "2025-05-22 13:41:54,564 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1111/aas.13572: Not OA according to Unpaywall.\n",
      "2025-05-22 13:41:54,567 - INFO - [OA_Worker_2] - oa_worker - OA: No PDF URL from Unpaywall for PMID 32145713 (DOI 10.1111/aas.13572). Trying PMC.\n",
      "2025-05-22 13:41:54,570 - INFO - [OA_Worker_2] - pmc_id_for_pmid - PMC ID ELink → PMID 32145713: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:55,127 - INFO - [OA_Worker_3] - pmc_attempt_download - PMC HEAD ✓ PMID 39384309 (PMC11474870): Resolved potential PDF URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC11474870/pdf/bmjpo-8-1.pdf\n",
      "2025-05-22 13:41:55,128 - INFO - [OA_Worker_3] - download_and_save_pdf - PMC(PMC11474870) → PMID 39384309: Attempting download from https://pmc.ncbi.nlm.nih.gov/articles/PMC11474870/pdf/bmjpo-8-1.pdf\n",
      "2025-05-22 13:41:55,330 - INFO - [OA_Worker_3] - download_and_save_pdf - PMC(PMC11474870) → PMID 39384309: Received HTML from PMC URL, attempting PoW solve.\n",
      "2025-05-22 13:41:55,333 - INFO - [OA_Worker_3] - download_and_save_pdf - Saved PMC challenge HTML to downloaded_pdfs_v12_pow\\suspicious_pdfs\\39384309_PMC(PMC11474870)_challenge.html\n",
      "2025-05-22 13:41:55,334 - INFO - [OA_Worker_3] - extract_pow_params_from_html - Extracted PoW params: Challenge='VwR3AQp5ZmVkZGHhZmL5...', Diff=4, Name='cloudpmc-viewer-pow'\n",
      "2025-05-22 13:41:55,337 - INFO - [OA_Worker_3] - solve_pmc_pow - Solving PoW: challenge='VwR3AQp5ZmVkZGHhZmL5...', difficulty=4\n",
      "2025-05-22 13:41:55,431 - INFO - [OA_Worker_3] - solve_pmc_pow - PoW SOLVED! Nonce: 41791, Hash: 00004d132d..., Time: 0.0929s\n",
      "2025-05-22 13:41:55,432 - INFO - [OA_Worker_3] - download_and_save_pdf - Set PoW cookie 'cloudpmc-viewer-pow' in session for pmc.ncbi.nlm.nih.gov.\n",
      "2025-05-22 13:41:55,433 - INFO - [OA_Worker_3] - download_and_save_pdf - PMC(PMC11474870) → PMID 39384309: Re-attempting GET to https://pmc.ncbi.nlm.nih.gov/articles/PMC11474870/pdf/bmjpo-8-1.pdf WITH PoW cookie.\n",
      "2025-05-22 13:41:56,582 - INFO - [OA_Worker_4] - pmc_id_for_pmid - PMC ID ELink ✓ PMID 38673038: Found PMCID PMC11051180 via 'pubmed_pmc'.\n",
      "2025-05-22 13:41:56,583 - INFO - [OA_Worker_4] - pmc_attempt_download - PMC HEAD → PMID 38673038 (PMC11051180): Probing https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11051180/pdf/ for final PDF URL.\n",
      "2025-05-22 13:41:56,601 - WARNING - [OA_Worker_0] - oa_worker - OA ✗ 36788057: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:41:56,601 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2018.10.073\n",
      "2025-05-22 13:41:56,800 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1016/j.jpedsurg.2018.10.073: Not OA according to Unpaywall.\n",
      "2025-05-22 13:41:56,801 - INFO - [OA_Worker_0] - oa_worker - OA: No PDF URL from Unpaywall for PMID 30518491 (DOI 10.1016/j.jpedsurg.2018.10.073). Trying PMC.\n",
      "2025-05-22 13:41:56,803 - INFO - [OA_Worker_0] - pmc_id_for_pmid - PMC ID ELink → PMID 30518491: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:56,893 - WARNING - [OA_Worker_2] - oa_worker - OA ✗ 32145713: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:41:56,894 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.2196/10996\n",
      "2025-05-22 13:41:57,180 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ✓ DOI 10.2196/10996: Found PDF URL: https://jmir.org/api/download...\n",
      "2025-05-22 13:41:57,181 - INFO - [OA_Worker_2] - download_and_save_pdf - Unpaywall(DOI:10.2196/10996) → PMID 33401363: Attempting download from https://jmir.org/api/download?alt_name=periop_v1i2e10996_app1.pdf\n",
      "2025-05-22 13:41:57,198 - INFO - [OA_Worker_3] - validate_downloaded_pdf - PDF Validation ✓ 39384309: File '2024-39384309-Pilkington-Enhanced_Recovery_After_Surgery_(ERAS)_consensus_recommendations_for_opioid-minimising_pharmacological_neonatal_pain_manageme.pdf.tmp' (Size: 874.32KB, Pages: 6, TextLen: 14919 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:41:57,199 - INFO - [OA_Worker_3] - download_and_save_pdf - PMC(PMC11474870) ✓ PMID 39384309: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2024-39384309-Pilkington-Enhanced_Recovery_After_Surgery_(ERAS)_consensus_recommendations_for_opioid-minimising_pharmacological_neonatal_pain_manageme.pdf\n",
      "2025-05-22 13:41:57,200 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.14309/crj.0000000000001469\n",
      "2025-05-22 13:41:57,393 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API ? DOI 10.14309/crj.0000000000001469: Article is OA, but no direct PDF URL in Unpaywall response.\n",
      "2025-05-22 13:41:57,397 - INFO - [OA_Worker_3] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39185540 (DOI 10.14309/crj.0000000000001469). Trying PMC.\n",
      "2025-05-22 13:41:57,398 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink → PMID 39185540: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:57,478 - INFO - [OA_Worker_1] - validate_downloaded_pdf - PDF Validation ✓ 40340819: File '2025-40340819-Pentz-Enhanced_Recovery_After_Surgery_(ERAS)_consensus_recommendations_for_non-pharmacological_perioperative_neonatal_pain_management..pdf.tmp' (Size: 214.50KB, Pages: 3, TextLen: 15073 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:41:57,480 - INFO - [OA_Worker_4] - pmc_attempt_download - PMC HEAD ✓ PMID 38673038 (PMC11051180): Resolved potential PDF URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC11051180/pdf/jpm-14-00411.pdf\n",
      "2025-05-22 13:41:57,481 - INFO - [OA_Worker_4] - download_and_save_pdf - PMC(PMC11051180) → PMID 38673038: Attempting download from https://pmc.ncbi.nlm.nih.gov/articles/PMC11051180/pdf/jpm-14-00411.pdf\n",
      "2025-05-22 13:41:57,481 - INFO - [OA_Worker_1] - download_and_save_pdf - Unpaywall(DOI:10.1136/bmjpo-2024-003280) ✓ PMID 40340819: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2025-40340819-Pentz-Enhanced_Recovery_After_Surgery_(ERAS)_consensus_recommendations_for_non-pharmacological_perioperative_neonatal_pain_management..pdf\n",
      "2025-05-22 13:41:57,484 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1053/j.jvca.2023.09.006\n",
      "2025-05-22 13:41:57,679 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1053/j.jvca.2023.09.006: Not OA according to Unpaywall.\n",
      "2025-05-22 13:41:57,680 - INFO - [OA_Worker_1] - oa_worker - OA: No PDF URL from Unpaywall for PMID 37802689 (DOI 10.1053/j.jvca.2023.09.006). Trying PMC.\n",
      "2025-05-22 13:41:57,681 - INFO - [OA_Worker_1] - pmc_id_for_pmid - PMC ID ELink → PMID 37802689: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:57,718 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink ✓ PMID 39185540: Found PMCID PMC11343539 via 'pubmed_pmc'.\n",
      "2025-05-22 13:41:57,719 - INFO - [OA_Worker_3] - pmc_attempt_download - PMC HEAD → PMID 39185540 (PMC11343539): Probing https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11343539/pdf/ for final PDF URL.\n",
      "2025-05-22 13:41:57,922 - WARNING - [OA_Worker_1] - oa_worker - OA ✗ 37802689: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:41:57,923 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1089/lap.2022.0537\n",
      "2025-05-22 13:41:57,955 - WARNING - [OA_Worker_2] - validate_downloaded_pdf - PDF Validation ✗ 33401363: File size 19.28 KB < threshold 20 KB for '2018-33401363-Wildemeersch-Implementation_of_an_Enhanced_Recovery_Pathway_for_Minimally_Invasive_Pectus_Surgery_A_Population-Based_Cohort_Study_Evalua.pdf.tmp'.\n",
      "2025-05-22 13:41:57,957 - WARNING - [OA_Worker_2] - download_and_save_pdf - Unpaywall(DOI:10.2196/10996) ✗ PMID 33401363: PDF from https://jmir.org/api/download?alt_name=periop_v1i2e10996_app1.pdf failed validation: File size 19.28 KB < threshold 20 KB\n",
      "2025-05-22 13:41:57,959 - INFO - [OA_Worker_2] - download_and_save_pdf - Moved suspicious PDF to downloaded_pdfs_v12_pow\\suspicious_pdfs\\2018-33401363-Wildemeersch-Implementation_of_an_Enhanced_Recovery_Pathway_for_Minimally_Invasive_Pectus_Surgery_A_Population-Based_Cohort_Study_Evalua.Unpaywall(DOI10.219610996).validation_failed.pdf\n",
      "2025-05-22 13:41:57,960 - INFO - [OA_Worker_2] - oa_worker - OA: Unpaywall attempt for PMID 33401363 (DOI 10.2196/10996) failed download/validation. Trying PMC.\n",
      "2025-05-22 13:41:57,960 - INFO - [OA_Worker_2] - pmc_id_for_pmid - PMC ID ELink → PMID 33401363: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:58,119 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1089/lap.2022.0537: Not OA according to Unpaywall.\n",
      "2025-05-22 13:41:58,120 - INFO - [OA_Worker_1] - oa_worker - OA: No PDF URL from Unpaywall for PMID 37062759 (DOI 10.1089/lap.2022.0537). Trying PMC.\n",
      "2025-05-22 13:41:58,120 - INFO - [OA_Worker_1] - pmc_id_for_pmid - PMC ID ELink → PMID 37062759: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:58,209 - INFO - [OA_Worker_3] - pmc_attempt_download - PMC HEAD ✓ PMID 39185540 (PMC11343539): Resolved potential PDF URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC11343539/pdf/ac9-11-e01469.pdf\n",
      "2025-05-22 13:41:58,225 - INFO - [OA_Worker_3] - download_and_save_pdf - PMC(PMC11343539) → PMID 39185540: Attempting download from https://pmc.ncbi.nlm.nih.gov/articles/PMC11343539/pdf/ac9-11-e01469.pdf\n",
      "2025-05-22 13:41:58,244 - INFO - [OA_Worker_4] - validate_downloaded_pdf - PDF Validation ✓ 38673038: File '2024-38673038-Zacha-Cryoanalgesia_as_the_Essential_Element_of_Enhanced_Recovery_after_Surgery_(ERAS)_in_Children_Undergoing_Thoracic_Surgery-Scoping_R.pdf.tmp' (Size: 398.82KB, Pages: 13, TextLen: 12427 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:41:58,246 - INFO - [OA_Worker_2] - pmc_id_for_pmid - PMC ID ELink ✓ PMID 33401363: Found PMCID PMC7709887 via 'pubmed_pmc'.\n",
      "2025-05-22 13:41:58,248 - INFO - [OA_Worker_4] - download_and_save_pdf - PMC(PMC11051180) ✓ PMID 38673038: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2024-38673038-Zacha-Cryoanalgesia_as_the_Essential_Element_of_Enhanced_Recovery_after_Surgery_(ERAS)_in_Children_Undergoing_Thoracic_Surgery-Scoping_R.pdf\n",
      "2025-05-22 13:41:58,250 - INFO - [OA_Worker_2] - pmc_attempt_download - PMC HEAD → PMID 33401363 (PMC7709887): Probing https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7709887/pdf/ for final PDF URL.\n",
      "2025-05-22 13:41:58,252 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.3389/fped.2023.933158\n",
      "2025-05-22 13:41:58,440 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API ✓ DOI 10.3389/fped.2023.933158: Found PDF URL: https://www.frontiersin.org/articles/10.3389/fped.2023.933158/pdf...\n",
      "2025-05-22 13:41:58,440 - INFO - [OA_Worker_4] - download_and_save_pdf - Unpaywall(DOI:10.3389/fped.2023.933158) → PMID 36969299: Attempting download from https://www.frontiersin.org/articles/10.3389/fped.2023.933158/pdf\n",
      "2025-05-22 13:41:58,741 - INFO - [OA_Worker_2] - pmc_attempt_download - PMC HEAD ✓ PMID 33401363 (PMC7709887): Resolved potential PDF URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC7709887/pdf/periop_v1i2e10996.pdf\n",
      "2025-05-22 13:41:58,742 - INFO - [OA_Worker_2] - download_and_save_pdf - PMC(PMC7709887) → PMID 33401363: Attempting download from https://pmc.ncbi.nlm.nih.gov/articles/PMC7709887/pdf/periop_v1i2e10996.pdf\n",
      "2025-05-22 13:41:58,931 - INFO - [OA_Worker_3] - validate_downloaded_pdf - PDF Validation ✓ 39185540: File '2024-39185540-Hussain-Single-Session_Endoscopic_Ultrasound-Directed_Transgastric_Endoscopic_Retrograde_Cholangiopancreatography_and_Simultaneous_Endos.pdf.tmp' (Size: 321.15KB, Pages: 3, TextLen: 12849 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:41:58,933 - INFO - [OA_Worker_3] - download_and_save_pdf - PMC(PMC11343539) ✓ PMID 39185540: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2024-39185540-Hussain-Single-Session_Endoscopic_Ultrasound-Directed_Transgastric_Endoscopic_Retrograde_Cholangiopancreatography_and_Simultaneous_Endos.pdf\n",
      "2025-05-22 13:41:58,933 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jfma.2021.03.029\n",
      "2025-05-22 13:41:59,146 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API ? DOI 10.1016/j.jfma.2021.03.029: Article is OA, but no direct PDF URL in Unpaywall response.\n",
      "2025-05-22 13:41:59,147 - INFO - [OA_Worker_3] - oa_worker - OA: No PDF URL from Unpaywall for PMID 33888360 (DOI 10.1016/j.jfma.2021.03.029). Trying PMC.\n",
      "2025-05-22 13:41:59,147 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink → PMID 33888360: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:59,151 - WARNING - [OA_Worker_0] - oa_worker - OA ✗ 30518491: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:41:59,152 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.4097/kja.22279\n",
      "2025-05-22 13:41:59,271 - INFO - [OA_Worker_2] - validate_downloaded_pdf - PDF Validation ✓ 33401363: File '2018-33401363-Wildemeersch-Implementation_of_an_Enhanced_Recovery_Pathway_for_Minimally_Invasive_Pectus_Surgery_A_Population-Based_Cohort_Study_Evalua.pdf.tmp' (Size: 1098.30KB, Pages: 18, TextLen: 12862 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:41:59,272 - INFO - [OA_Worker_2] - download_and_save_pdf - PMC(PMC7709887) ✓ PMID 33401363: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2018-33401363-Wildemeersch-Implementation_of_an_Enhanced_Recovery_Pathway_for_Minimally_Invasive_Pectus_Surgery_A_Population-Based_Cohort_Study_Evalua.pdf\n",
      "2025-05-22 13:41:59,273 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1007/s00383-021-04934-x\n",
      "2025-05-22 13:41:59,340 - INFO - [OA_Worker_0] - unpaywall_get_pdf_url - Unpaywall API ✓ DOI 10.4097/kja.22279: Found PDF URL: https://ekja.org/upload/pdf/kja-22279.pdf...\n",
      "2025-05-22 13:41:59,342 - INFO - [OA_Worker_0] - download_and_save_pdf - Unpaywall(DOI:10.4097/kja.22279) → PMID 35790215: Attempting download from https://ekja.org/upload/pdf/kja-22279.pdf\n",
      "2025-05-22 13:41:59,370 - WARNING - [OA_Worker_3] - oa_worker - OA ✗ 33888360: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:41:59,371 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1007/s00383-020-04778-x\n",
      "2025-05-22 13:41:59,467 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1007/s00383-021-04934-x: Not OA according to Unpaywall.\n",
      "2025-05-22 13:41:59,468 - INFO - [OA_Worker_2] - oa_worker - OA: No PDF URL from Unpaywall for PMID 34089071 (DOI 10.1007/s00383-021-04934-x). Trying PMC.\n",
      "2025-05-22 13:41:59,470 - INFO - [OA_Worker_2] - pmc_id_for_pmid - PMC ID ELink → PMID 34089071: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:59,575 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1007/s00383-020-04778-x: Not OA according to Unpaywall.\n",
      "2025-05-22 13:41:59,575 - INFO - [OA_Worker_3] - oa_worker - OA: No PDF URL from Unpaywall for PMID 33210165 (DOI 10.1007/s00383-020-04778-x). Trying PMC.\n",
      "2025-05-22 13:41:59,576 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink → PMID 33210165: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:41:59,694 - WARNING - [OA_Worker_2] - oa_worker - OA ✗ 34089071: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:41:59,696 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jtcvs.2021.04.072\n",
      "2025-05-22 13:41:59,889 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ? DOI 10.1016/j.jtcvs.2021.04.072: Article is OA, but no direct PDF URL in Unpaywall response.\n",
      "2025-05-22 13:41:59,891 - INFO - [OA_Worker_2] - oa_worker - OA: No PDF URL from Unpaywall for PMID 34059337 (DOI 10.1016/j.jtcvs.2021.04.072). Trying PMC.\n",
      "2025-05-22 13:41:59,891 - INFO - [OA_Worker_2] - pmc_id_for_pmid - PMC ID ELink → PMID 34059337: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:00,514 - WARNING - [OA_Worker_1] - oa_worker - OA ✗ 37062759: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:00,515 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1007/s00383-020-04695-z\n",
      "2025-05-22 13:42:00,710 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1007/s00383-020-04695-z: Not OA according to Unpaywall.\n",
      "2025-05-22 13:42:00,712 - INFO - [OA_Worker_1] - oa_worker - OA: No PDF URL from Unpaywall for PMID 32696123 (DOI 10.1007/s00383-020-04695-z). Trying PMC.\n",
      "2025-05-22 13:42:00,713 - INFO - [OA_Worker_1] - pmc_id_for_pmid - PMC ID ELink → PMID 32696123: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:01,014 - WARNING - [OA_Worker_1] - oa_worker - OA ✗ 32696123: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:01,017 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1007/s00268-020-05530-1\n",
      "2025-05-22 13:42:01,260 - INFO - [OA_Worker_4] - validate_downloaded_pdf - PDF Validation ✓ 36969299: File '2023-36969299-Huang-Application_of_laryngeal_mask_airway_anesthesia_with_preserved_spontaneous_breathing_in_children_undergoing_video-assisted_thoraci.pdf.tmp' (Size: 351.41KB, Pages: 7, TextLen: 13827 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:01,260 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API ✓ DOI 10.1007/s00268-020-05530-1: Found PDF URL: https://link.springer.com/content/pdf/10.1007/s00268-020-05530-1.pdf...\n",
      "2025-05-22 13:42:01,262 - INFO - [OA_Worker_1] - download_and_save_pdf - Unpaywall(DOI:10.1007/s00268-020-05530-1) → PMID 32385680: Attempting download from https://link.springer.com/content/pdf/10.1007/s00268-020-05530-1.pdf\n",
      "2025-05-22 13:42:01,264 - INFO - [OA_Worker_4] - download_and_save_pdf - Unpaywall(DOI:10.3389/fped.2023.933158) ✓ PMID 36969299: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2023-36969299-Huang-Application_of_laryngeal_mask_airway_anesthesia_with_preserved_spontaneous_breathing_in_children_undergoing_video-assisted_thoraci.pdf\n",
      "2025-05-22 13:42:01,265 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2018.12.003\n",
      "2025-05-22 13:42:01,474 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1016/j.jpedsurg.2018.12.003: Not OA according to Unpaywall.\n",
      "2025-05-22 13:42:01,476 - INFO - [OA_Worker_4] - oa_worker - OA: No PDF URL from Unpaywall for PMID 30686518 (DOI 10.1016/j.jpedsurg.2018.12.003). Trying PMC.\n",
      "2025-05-22 13:42:01,477 - INFO - [OA_Worker_4] - pmc_id_for_pmid - PMC ID ELink → PMID 30686518: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:01,694 - WARNING - [OA_Worker_4] - oa_worker - OA ✗ 30686518: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:01,696 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2019.02.007\n",
      "2025-05-22 13:42:01,889 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1016/j.jpedsurg.2019.02.007: Not OA according to Unpaywall.\n",
      "2025-05-22 13:42:01,890 - INFO - [OA_Worker_4] - oa_worker - OA: No PDF URL from Unpaywall for PMID 30922685 (DOI 10.1016/j.jpedsurg.2019.02.007). Trying PMC.\n",
      "2025-05-22 13:42:01,891 - INFO - [OA_Worker_4] - pmc_id_for_pmid - PMC ID ELink → PMID 30922685: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:01,938 - WARNING - [OA_Worker_3] - oa_worker - OA ✗ 33210165: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:01,939 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.23736/S0375-9393.19.13880-1\n",
      "2025-05-22 13:42:02,093 - WARNING - [OA_Worker_4] - oa_worker - OA ✗ 30922685: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:02,094 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1007/s00383-016-3986-y\n",
      "2025-05-22 13:42:02,151 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.23736/S0375-9393.19.13880-1: Not OA according to Unpaywall.\n",
      "2025-05-22 13:42:02,153 - INFO - [OA_Worker_3] - oa_worker - OA: No PDF URL from Unpaywall for PMID 31274269 (DOI 10.23736/S0375-9393.19.13880-1). Trying PMC.\n",
      "2025-05-22 13:42:02,154 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink → PMID 31274269: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:02,289 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1007/s00383-016-3986-y: Not OA according to Unpaywall.\n",
      "2025-05-22 13:42:02,290 - INFO - [OA_Worker_4] - oa_worker - OA: No PDF URL from Unpaywall for PMID 27679510 (DOI 10.1007/s00383-016-3986-y). Trying PMC.\n",
      "2025-05-22 13:42:02,291 - INFO - [OA_Worker_4] - pmc_id_for_pmid - PMC ID ELink → PMID 27679510: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:04,233 - INFO - [OA_Worker_1] - validate_downloaded_pdf - PDF Validation ✓ 32385680: File '2020-32385680-Brindle-Consensus_Guidelines_for_Perioperative_Care_in_Neonatal_Intestinal_Surgery_Enhanced_Recovery_After_Surgery_(ERAS®)_Society_Recom.pdf.tmp' (Size: 2440.91KB, Pages: 11, TextLen: 11639 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:04,234 - INFO - [OA_Worker_1] - download_and_save_pdf - Unpaywall(DOI:10.1007/s00268-020-05530-1) ✓ PMID 32385680: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2020-32385680-Brindle-Consensus_Guidelines_for_Perioperative_Care_in_Neonatal_Intestinal_Surgery_Enhanced_Recovery_After_Surgery_(ERAS®)_Society_Recom.pdf\n",
      "2025-05-22 13:42:04,235 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2016.09.065\n",
      "2025-05-22 13:42:04,394 - WARNING - [OA_Worker_2] - oa_worker - OA ✗ 34059337: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:04,395 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jss.2015.12.051\n",
      "2025-05-22 13:42:04,434 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1016/j.jpedsurg.2016.09.065: Not OA according to Unpaywall.\n",
      "2025-05-22 13:42:04,435 - INFO - [OA_Worker_1] - oa_worker - OA: No PDF URL from Unpaywall for PMID 27810148 (DOI 10.1016/j.jpedsurg.2016.09.065). Trying PMC.\n",
      "2025-05-22 13:42:04,435 - INFO - [OA_Worker_1] - pmc_id_for_pmid - PMC ID ELink → PMID 27810148: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:04,510 - WARNING - [OA_Worker_3] - oa_worker - OA ✗ 31274269: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:04,511 - INFO - [OA_Worker_3] - oa_worker - OA: No DOI for PMID 26888001. Skipping Unpaywall, trying PMC.\n",
      "2025-05-22 13:42:04,511 - INFO - [OA_Worker_3] - pmc_id_for_pmid - PMC ID ELink → PMID 26888001: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:04,597 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1016/j.jss.2015.12.051: Not OA according to Unpaywall.\n",
      "2025-05-22 13:42:04,598 - INFO - [OA_Worker_2] - oa_worker - OA: No PDF URL from Unpaywall for PMID 27083963 (DOI 10.1016/j.jss.2015.12.051). Trying PMC.\n",
      "2025-05-22 13:42:04,598 - INFO - [OA_Worker_2] - pmc_id_for_pmid - PMC ID ELink → PMID 27083963: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:04,644 - WARNING - [OA_Worker_4] - oa_worker - OA ✗ 27679510: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:04,645 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jpedsurg.2024.162046\n",
      "2025-05-22 13:42:04,840 - INFO - [OA_Worker_4] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1016/j.jpedsurg.2024.162046: Not OA according to Unpaywall.\n",
      "2025-05-22 13:42:04,842 - INFO - [OA_Worker_4] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39520824 (DOI 10.1016/j.jpedsurg.2024.162046). Trying PMC.\n",
      "2025-05-22 13:42:04,843 - INFO - [OA_Worker_4] - pmc_id_for_pmid - PMC ID ELink → PMID 39520824: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:06,875 - WARNING - [OA_Worker_3] - oa_worker - OA ✗ 26888001: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:06,875 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1053/j.jvca.2024.10.005\n",
      "2025-05-22 13:42:07,038 - WARNING - [OA_Worker_1] - oa_worker - OA ✗ 27810148: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:07,039 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1001/jamasurg.2024.2044\n",
      "2025-05-22 13:42:07,090 - INFO - [OA_Worker_3] - unpaywall_get_pdf_url - Unpaywall API ✓ DOI 10.1053/j.jvca.2024.10.005: Found PDF URL: https://pure.rug.nl/ws/files/1220552997/1-s2.0-S1053077024007870-main.pdf...\n",
      "2025-05-22 13:42:07,090 - INFO - [OA_Worker_3] - download_and_save_pdf - Unpaywall(DOI:10.1053/j.jvca.2024.10.005) → PMID 39489669: Attempting download from https://pure.rug.nl/ws/files/1220552997/1-s2.0-S1053077024007870-main.pdf\n",
      "2025-05-22 13:42:07,143 - WARNING - [OA_Worker_2] - oa_worker - OA ✗ 27083963: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:07,146 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1186/s12887-024-05083-5\n",
      "2025-05-22 13:42:07,232 - INFO - [OA_Worker_1] - unpaywall_get_pdf_url - Unpaywall API ~ DOI 10.1001/jamasurg.2024.2044: Not OA according to Unpaywall.\n",
      "2025-05-22 13:42:07,234 - INFO - [OA_Worker_1] - oa_worker - OA: No PDF URL from Unpaywall for PMID 39083294 (DOI 10.1001/jamasurg.2024.2044). Trying PMC.\n",
      "2025-05-22 13:42:07,234 - INFO - [OA_Worker_1] - pmc_id_for_pmid - PMC ID ELink → PMID 39083294: Querying with linkname 'pubmed_pmc'.\n",
      "2025-05-22 13:42:07,341 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ✓ DOI 10.1186/s12887-024-05083-5: Found PDF URL: https://link.springer.com/content/pdf/10.1186/s12887-024-05083-5.pdf...\n",
      "2025-05-22 13:42:07,342 - INFO - [OA_Worker_2] - download_and_save_pdf - Unpaywall(DOI:10.1186/s12887-024-05083-5) → PMID 39342249: Attempting download from https://link.springer.com/content/pdf/10.1186/s12887-024-05083-5.pdf\n",
      "2025-05-22 13:42:08,819 - INFO - [OA_Worker_2] - validate_downloaded_pdf - PDF Validation ✓ 39342249: File '2024-39342249-Guo-Simultaneous_unilateral_thoracoscopic_resection_of_bilateral_pulmonary_sequestration..pdf.tmp' (Size: 1398.87KB, Pages: 6, TextLen: 9557 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:08,820 - INFO - [OA_Worker_2] - download_and_save_pdf - Unpaywall(DOI:10.1186/s12887-024-05083-5) ✓ PMID 39342249: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2024-39342249-Guo-Simultaneous_unilateral_thoracoscopic_resection_of_bilateral_pulmonary_sequestration..pdf\n",
      "2025-05-22 13:42:08,820 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API GET → DOI 10.1016/j.jtcvs.2019.10.049\n",
      "2025-05-22 13:42:09,039 - INFO - [OA_Worker_2] - unpaywall_get_pdf_url - Unpaywall API ✓ DOI 10.1016/j.jtcvs.2019.10.049: Found PDF URL: http://www.jtcvs.org/article/S0022522319322792/pdf...\n",
      "2025-05-22 13:42:09,041 - INFO - [OA_Worker_2] - download_and_save_pdf - Unpaywall(DOI:10.1016/j.jtcvs.2019.10.049) → PMID 31859070: Attempting download from http://www.jtcvs.org/article/S0022522319322792/pdf\n",
      "2025-05-22 13:42:09,413 - WARNING - [OA_Worker_4] - oa_worker - OA ✗ 39520824: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:09,440 - INFO - [OA_Worker_3] - validate_downloaded_pdf - PDF Validation ✓ 39489669: File '2025-39489669-Meier-Enhanced_Recovery_after_Surgery_(ERAS)_in_Pediatric_Cardiac_Surgery_Status_Quo_of_Implementation_in_Europe..pdf.tmp' (Size: 1299.16KB, Pages: 11, TextLen: 11303 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:09,442 - INFO - [OA_Worker_3] - download_and_save_pdf - Unpaywall(DOI:10.1053/j.jvca.2024.10.005) ✓ PMID 39489669: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2025-39489669-Meier-Enhanced_Recovery_after_Surgery_(ERAS)_in_Pediatric_Cardiac_Surgery_Status_Quo_of_Implementation_in_Europe..pdf\n",
      "2025-05-22 13:42:09,681 - WARNING - [OA_Worker_1] - oa_worker - OA ✗ 39083294: No PDF found via Unpaywall or PMC.\n",
      "2025-05-22 13:42:14,634 - INFO - [OA_Worker_2] - validate_downloaded_pdf - PDF Validation ✓ 31859070: File '2020-31859070-Roy-Initial_experience_introducing_an_enhanced_recovery_program_in_congenital_cardiac_surgery..pdf.tmp' (Size: 879.74KB, Pages: 14, TextLen: 16195 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:14,635 - INFO - [OA_Worker_2] - download_and_save_pdf - Unpaywall(DOI:10.1016/j.jtcvs.2019.10.049) ✓ PMID 31859070: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2020-31859070-Roy-Initial_experience_introducing_an_enhanced_recovery_program_in_congenital_cardiac_surgery..pdf\n",
      "2025-05-22 13:42:21,764 - INFO - [OA_Worker_0] - validate_downloaded_pdf - PDF Validation ✓ 35790215: File '2022-35790215-Lucente-Erector_spinae_plane_block_in_children_a_narrative_review..pdf.tmp' (Size: 920.20KB, Pages: 14, TextLen: 9343 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:21,766 - INFO - [OA_Worker_0] - download_and_save_pdf - Unpaywall(DOI:10.4097/kja.22279) ✓ PMID 35790215: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2022-35790215-Lucente-Erector_spinae_plane_block_in_children_a_narrative_review..pdf\n",
      "2025-05-22 13:42:21,768 - INFO - [MainThread] - main - Open Access Phase Summary: 11 PDFs successfully downloaded and validated.\n",
      "2025-05-22 13:42:21,769 - INFO - [MainThread] - main - 21 PMIDs not fetched via OA or failed validation: ['26888001', '27083963', '27679510', '27810148', '30518491', '30686518', '30922685', '31274269', '32145713', '32696123']...\n",
      "2025-05-22 13:42:21,770 - INFO - [MainThread] - main - --- Starting Sci-Hub Download Phase for Remaining PMIDs ---\n",
      "2025-05-22 13:42:21,770 - INFO - [MainThread] - init_scihub_domains - Probing Sci-Hub mirrors for availability...\n",
      "2025-05-22 13:42:22,327 - INFO - [SciHub_Domain_Test_1] - test_scihub_domain - Sci-Hub TEST ✓ https://sci-hub.ru is responsive (status 200).\n",
      "2025-05-22 13:42:22,392 - INFO - [SciHub_Domain_Test_0] - test_scihub_domain - Sci-Hub TEST ✓ https://sci-hub.se is responsive (status 200).\n",
      "2025-05-22 13:42:23,686 - INFO - [SciHub_Domain_Test_1] - test_scihub_domain - Sci-Hub TEST ✓ https://sci-hub.wf is responsive (status 200).\n",
      "2025-05-22 13:42:24,214 - WARNING - [SciHub_Domain_Test_1] - test_scihub_domain - Sci-Hub TEST ? https://sci-hub.st responded status 403, CT: text/html; charset=UTF-8. Text snippet: <!doctype html><html><head><title>DDoS-Guard</title><meta charset=\"utf-8\"/><meta name=\"viewport\" con\n",
      "2025-05-22 13:42:24,453 - INFO - [SciHub_Domain_Test_2] - test_scihub_domain - Sci-Hub TEST ✓ https://sci-hub.ren is responsive (status 200).\n",
      "2025-05-22 13:42:24,855 - INFO - [SciHub_Domain_Test_0] - test_scihub_domain - Sci-Hub TEST ✓ https://sci-hub.ee is responsive (status 200).\n",
      "2025-05-22 13:42:24,858 - INFO - [MainThread] - init_scihub_domains - Using Sci-Hub domains: ['https://sci-hub.se', 'https://sci-hub.ru', 'https://sci-hub.wf', 'https://sci-hub.ren', 'https://sci-hub.ee']\n",
      "2025-05-22 13:42:24,862 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39955421 from https://sci-hub.se/10.1007%2Fs00383-025-05977-0 (Attempt 1/5)\n",
      "2025-05-22 13:42:24,864 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39068053 from https://sci-hub.se/10.1016%2Fj.jpedsurg.2024.06.021 (Attempt 1/5)\n",
      "2025-05-22 13:42:25,448 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 39955421 via https://sci-hub.se: No PDF link found within HTML from https://sci-hub.se/10.1007%2Fs00383-025-05977-0\n",
      "2025-05-22 13:42:25,543 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 39068053 via https://sci-hub.se: No PDF link found within HTML from https://sci-hub.se/10.1016%2Fj.jpedsurg.2024.06.021\n",
      "2025-05-22 13:42:25,949 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39955421 from https://sci-hub.ru/10.1007%2Fs00383-025-05977-0 (Attempt 2/5)\n",
      "2025-05-22 13:42:26,045 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39068053 from https://sci-hub.ru/10.1016%2Fj.jpedsurg.2024.06.021 (Attempt 2/5)\n",
      "2025-05-22 13:42:26,431 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 39955421 via https://sci-hub.ru: No PDF link found within HTML from https://sci-hub.ru/10.1007%2Fs00383-025-05977-0\n",
      "2025-05-22 13:42:26,525 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 39068053 via https://sci-hub.ru: No PDF link found within HTML from https://sci-hub.ru/10.1016%2Fj.jpedsurg.2024.06.021\n",
      "2025-05-22 13:42:26,932 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39955421 from https://sci-hub.wf/10.1007%2Fs00383-025-05977-0 (Attempt 3/5)\n",
      "2025-05-22 13:42:27,027 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39068053 from https://sci-hub.wf/10.1016%2Fj.jpedsurg.2024.06.021 (Attempt 3/5)\n",
      "2025-05-22 13:42:28,188 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 39068053 via https://sci-hub.wf: No PDF link found within HTML from https://sci-hub.wf/10.1016%2Fj.jpedsurg.2024.06.021\n",
      "2025-05-22 13:42:28,229 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 39955421 via https://sci-hub.wf: No PDF link found within HTML from https://sci-hub.wf/10.1007%2Fs00383-025-05977-0\n",
      "2025-05-22 13:42:28,691 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39068053 from https://sci-hub.ren/10.1016%2Fj.jpedsurg.2024.06.021 (Attempt 4/5)\n",
      "2025-05-22 13:42:28,732 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39955421 from https://sci-hub.ren/10.1007%2Fs00383-025-05977-0 (Attempt 4/5)\n",
      "2025-05-22 13:42:29,893 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 39068053 via https://sci-hub.ren (https://sci-hub.ren/10.1016%2Fj.jpedsurg.2024.06.021): RequestException: 404 Client Error: Not Found for url: https://sci-hub.ren/10.1016%2Fj.jpedsurg.2024.06.021\n",
      "2025-05-22 13:42:29,947 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub ✗ 39955421 via https://sci-hub.ren (https://sci-hub.ren/10.1007%2Fs00383-025-05977-0): RequestException: 404 Client Error: Not Found for url: https://sci-hub.ren/10.1007%2Fs00383-025-05977-0\n",
      "2025-05-22 13:42:30,396 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39068053 from https://sci-hub.ee/10.1016%2Fj.jpedsurg.2024.06.021 (Attempt 5/5)\n",
      "2025-05-22 13:42:30,448 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39955421 from https://sci-hub.ee/10.1007%2Fs00383-025-05977-0 (Attempt 5/5)\n",
      "2025-05-22 13:42:31,734 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 39068053 via https://sci-hub.ee: No PDF link found within HTML from https://sci-hub.ee/10.1016%2Fj.jpedsurg.2024.06.021\n",
      "2025-05-22 13:42:31,738 - ERROR - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 39068053: Failed for identifier '10.1016/j.jpedsurg.2024.06.021' after trying all active domains.\n",
      "2025-05-22 13:42:31,739 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 36788057 from https://sci-hub.se/10.1016%2Fj.jpedsurg.2023.01.028 (Attempt 1/5)\n",
      "2025-05-22 13:42:31,771 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 39955421 via https://sci-hub.ee: No PDF link found within HTML from https://sci-hub.ee/10.1007%2Fs00383-025-05977-0\n",
      "2025-05-22 13:42:31,772 - ERROR - [SciHub_Worker_0] - scihub_worker - Sci-Hub ✗ 39955421: Failed for identifier '10.1007/s00383-025-05977-0' after trying all active domains.\n",
      "2025-05-22 13:42:31,773 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 32145713 from https://sci-hub.se/10.1111%2Faas.13572 (Attempt 1/5)\n",
      "2025-05-22 13:42:32,313 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 36788057 via https://sci-hub.se: No PDF link found within HTML from https://sci-hub.se/10.1016%2Fj.jpedsurg.2023.01.028\n",
      "2025-05-22 13:42:32,423 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✓ PMID 32145713: Found potential PDF link: https://sci-hub.se/downloads/2020-07-12/fc/wei2020.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:32,424 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 32145713: Attempting download from https://sci-hub.se/downloads/2020-07-12/fc/wei2020.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:32,815 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 36788057 from https://sci-hub.ru/10.1016%2Fj.jpedsurg.2023.01.028 (Attempt 2/5)\n",
      "2025-05-22 13:42:33,314 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 36788057 via https://sci-hub.ru: No PDF link found within HTML from https://sci-hub.ru/10.1016%2Fj.jpedsurg.2023.01.028\n",
      "2025-05-22 13:42:33,691 - INFO - [SciHub_Worker_0] - validate_downloaded_pdf - PDF Validation ✓ 32145713: File '2020-32145713-Wei-Combined_non-intubated_anaesthesia_and_paravertebral_nerve_block_in_comparison_with_intubated_anaesthesia_in_children_undergoing_vid.pdf.tmp' (Size: 469.38KB, Pages: 9, TextLen: 15895 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:33,693 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 32145713: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2020-32145713-Wei-Combined_non-intubated_anaesthesia_and_paravertebral_nerve_block_in_comparison_with_intubated_anaesthesia_in_children_undergoing_vid.pdf\n",
      "2025-05-22 13:42:33,694 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 37802689 from https://sci-hub.se/10.1053%2Fj.jvca.2023.09.006 (Attempt 1/5)\n",
      "2025-05-22 13:42:33,816 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 36788057 from https://sci-hub.wf/10.1016%2Fj.jpedsurg.2023.01.028 (Attempt 3/5)\n",
      "2025-05-22 13:42:33,928 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 37802689 via https://sci-hub.se: No PDF link found within HTML from https://sci-hub.se/10.1053%2Fj.jvca.2023.09.006\n",
      "2025-05-22 13:42:34,431 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 37802689 from https://sci-hub.ru/10.1053%2Fj.jvca.2023.09.006 (Attempt 2/5)\n",
      "2025-05-22 13:42:34,917 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 37802689 via https://sci-hub.ru: No PDF link found within HTML from https://sci-hub.ru/10.1053%2Fj.jvca.2023.09.006\n",
      "2025-05-22 13:42:35,183 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 36788057 via https://sci-hub.wf: No PDF link found within HTML from https://sci-hub.wf/10.1016%2Fj.jpedsurg.2023.01.028\n",
      "2025-05-22 13:42:35,419 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 37802689 from https://sci-hub.wf/10.1053%2Fj.jvca.2023.09.006 (Attempt 3/5)\n",
      "2025-05-22 13:42:35,685 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 36788057 from https://sci-hub.ren/10.1016%2Fj.jpedsurg.2023.01.028 (Attempt 4/5)\n",
      "2025-05-22 13:42:36,326 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 37802689 via https://sci-hub.wf: No PDF link found within HTML from https://sci-hub.wf/10.1053%2Fj.jvca.2023.09.006\n",
      "2025-05-22 13:42:36,828 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 37802689 from https://sci-hub.ren/10.1053%2Fj.jvca.2023.09.006 (Attempt 4/5)\n",
      "2025-05-22 13:42:36,928 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 36788057 via https://sci-hub.ren (https://sci-hub.ren/10.1016%2Fj.jpedsurg.2023.01.028): RequestException: 404 Client Error: Not Found for url: https://sci-hub.ren/10.1016%2Fj.jpedsurg.2023.01.028\n",
      "2025-05-22 13:42:37,367 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub ✗ 37802689 via https://sci-hub.ren (https://sci-hub.ren/10.1053%2Fj.jvca.2023.09.006): RequestException: 404 Client Error: Not Found for url: https://sci-hub.ren/10.1053%2Fj.jvca.2023.09.006\n",
      "2025-05-22 13:42:37,430 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 36788057 from https://sci-hub.ee/10.1016%2Fj.jpedsurg.2023.01.028 (Attempt 5/5)\n",
      "2025-05-22 13:42:37,870 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 37802689 from https://sci-hub.ee/10.1053%2Fj.jvca.2023.09.006 (Attempt 5/5)\n",
      "2025-05-22 13:42:38,441 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 37802689 via https://sci-hub.ee: No PDF link found within HTML from https://sci-hub.ee/10.1053%2Fj.jvca.2023.09.006\n",
      "2025-05-22 13:42:38,442 - ERROR - [SciHub_Worker_0] - scihub_worker - Sci-Hub ✗ 37802689: Failed for identifier '10.1053/j.jvca.2023.09.006' after trying all active domains.\n",
      "2025-05-22 13:42:38,443 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 30518491 from https://sci-hub.se/10.1016%2Fj.jpedsurg.2018.10.073 (Attempt 1/5)\n",
      "2025-05-22 13:42:38,750 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 36788057 via https://sci-hub.ee: No PDF link found within HTML from https://sci-hub.ee/10.1016%2Fj.jpedsurg.2023.01.028\n",
      "2025-05-22 13:42:38,752 - ERROR - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 36788057: Failed for identifier '10.1016/j.jpedsurg.2023.01.028' after trying all active domains.\n",
      "2025-05-22 13:42:38,753 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 33888360 from https://sci-hub.se/10.1016%2Fj.jfma.2021.03.029 (Attempt 1/5)\n",
      "2025-05-22 13:42:39,021 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✓ PMID 30518491: Found potential PDF link: https://sci-hub.se/tree/cb/91/cb91a9003ea9a98bae1e369ec4c00798.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:39,022 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 30518491: Attempting download from https://sci-hub.se/tree/cb/91/cb91a9003ea9a98bae1e369ec4c00798.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:39,344 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✓ PMID 33888360: Found potential PDF link: https://sci-hub.se/downloads/2021-05-13//b1/hung2021.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:39,345 - INFO - [SciHub_Worker_1] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 33888360: Attempting download from https://sci-hub.se/downloads/2021-05-13//b1/hung2021.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:39,947 - INFO - [SciHub_Worker_0] - validate_downloaded_pdf - PDF Validation ✓ 30518491: File '2019-30518491-Haveliwala-Aortopexy_for_tracheomalacia_via_a_suprasternal_incision..pdf.tmp' (Size: 211.33KB, Pages: 4, TextLen: 19856 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:39,948 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 30518491: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2019-30518491-Haveliwala-Aortopexy_for_tracheomalacia_via_a_suprasternal_incision..pdf\n",
      "2025-05-22 13:42:39,949 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 34089071 from https://sci-hub.se/10.1007%2Fs00383-021-04934-x (Attempt 1/5)\n",
      "2025-05-22 13:42:40,085 - INFO - [SciHub_Worker_1] - validate_downloaded_pdf - PDF Validation ✓ 33888360: File '2022-33888360-Hung-Comparison_of_perioperative_outcomes_between_intubated_and_nonintubated_thoracoscopic_surgery_in_children..pdf.tmp' (Size: 223.78KB, Pages: 7, TextLen: 14995 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:40,087 - INFO - [SciHub_Worker_1] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 33888360: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2022-33888360-Hung-Comparison_of_perioperative_outcomes_between_intubated_and_nonintubated_thoracoscopic_surgery_in_children..pdf\n",
      "2025-05-22 13:42:40,088 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 37062759 from https://sci-hub.se/10.1089%2Flap.2022.0537 (Attempt 1/5)\n",
      "2025-05-22 13:42:40,181 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✓ PMID 34089071: Found potential PDF link: https://sci-hub.se/downloads/2021-06-19/86/cramm2021.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:40,183 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 34089071: Attempting download from https://sci-hub.se/downloads/2021-06-19/86/cramm2021.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:40,343 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 37062759 via https://sci-hub.se: No PDF link found within HTML from https://sci-hub.se/10.1089%2Flap.2022.0537\n",
      "2025-05-22 13:42:40,845 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 37062759 from https://sci-hub.ru/10.1089%2Flap.2022.0537 (Attempt 2/5)\n",
      "2025-05-22 13:42:41,545 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 37062759 via https://sci-hub.ru: No PDF link found within HTML from https://sci-hub.ru/10.1089%2Flap.2022.0537\n",
      "2025-05-22 13:42:42,098 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 37062759 from https://sci-hub.wf/10.1089%2Flap.2022.0537 (Attempt 3/5)\n",
      "2025-05-22 13:42:42,306 - INFO - [SciHub_Worker_0] - validate_downloaded_pdf - PDF Validation ✓ 34089071: File '2021-34089071-Cramm-Thoracic_epidural-based_Enhanced_Recovery_After_Surgery_(ERAS)_pathway_for_Nuss_repair_of_pectus_excavatum_shortened_length_of_sta.pdf.tmp' (Size: 1426.34KB, Pages: 9, TextLen: 9585 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:42,308 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 34089071: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2021-34089071-Cramm-Thoracic_epidural-based_Enhanced_Recovery_After_Surgery_(ERAS)_pathway_for_Nuss_repair_of_pectus_excavatum_shortened_length_of_sta.pdf\n",
      "2025-05-22 13:42:42,309 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 32696123 from https://sci-hub.se/10.1007%2Fs00383-020-04695-z (Attempt 1/5)\n",
      "2025-05-22 13:42:42,908 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✓ PMID 32696123: Found potential PDF link: https://sci-hub.se/downloads/2020-07-28/c3/mangat2020.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:42,909 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 32696123: Attempting download from https://sci-hub.se/downloads/2020-07-28/c3/mangat2020.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:43,483 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 37062759 via https://sci-hub.wf: No PDF link found within HTML from https://sci-hub.wf/10.1089%2Flap.2022.0537\n",
      "2025-05-22 13:42:43,988 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 37062759 from https://sci-hub.ren/10.1089%2Flap.2022.0537 (Attempt 4/5)\n",
      "2025-05-22 13:42:44,059 - INFO - [SciHub_Worker_0] - validate_downloaded_pdf - PDF Validation ✓ 32696123: File '2020-32696123-Mangat-The_impact_of_an_enhanced_recovery_perioperative_pathway_for_pediatric_pectus_deformity_repair..pdf.tmp' (Size: 627.54KB, Pages: 11, TextLen: 12563 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:44,060 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 32696123: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2020-32696123-Mangat-The_impact_of_an_enhanced_recovery_perioperative_pathway_for_pediatric_pectus_deformity_repair..pdf\n",
      "2025-05-22 13:42:44,061 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 30686518 from https://sci-hub.se/10.1016%2Fj.jpedsurg.2018.12.003 (Attempt 1/5)\n",
      "2025-05-22 13:42:44,660 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✓ PMID 30686518: Found potential PDF link: https://sci-hub.se/downloads/2019-11-11/ce/schlatter2018.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:44,660 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 30686518: Attempting download from https://sci-hub.se/downloads/2019-11-11/ce/schlatter2018.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:45,255 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 37062759 via https://sci-hub.ren (https://sci-hub.ren/10.1089%2Flap.2022.0537): RequestException: 404 Client Error: Not Found for url: https://sci-hub.ren/10.1089%2Flap.2022.0537\n",
      "2025-05-22 13:42:45,718 - INFO - [SciHub_Worker_0] - validate_downloaded_pdf - PDF Validation ✓ 30686518: File '2019-30686518-Schlatter-Progressive_reduction_of_hospital_length_of_stay_following_minimally_invasive_repair_of_pectus_excavatum_A_retrospective_compa.pdf.tmp' (Size: 327.60KB, Pages: 7, TextLen: 19189 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:45,719 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 30686518: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2019-30686518-Schlatter-Progressive_reduction_of_hospital_length_of_stay_following_minimally_invasive_repair_of_pectus_excavatum_A_retrospective_compa.pdf\n",
      "2025-05-22 13:42:45,721 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 33210165 from https://sci-hub.se/10.1007%2Fs00383-020-04778-x (Attempt 1/5)\n",
      "2025-05-22 13:42:45,757 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 37062759 from https://sci-hub.ee/10.1089%2Flap.2022.0537 (Attempt 5/5)\n",
      "2025-05-22 13:42:45,954 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✓ PMID 33210165: Found potential PDF link: https://sci-hub.se/downloads/2020-11-22/da/rettig2020.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:45,955 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 33210165: Attempting download from https://sci-hub.se/downloads/2020-11-22/da/rettig2020.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:46,563 - INFO - [SciHub_Worker_0] - validate_downloaded_pdf - PDF Validation ✓ 33210165: File '2021-33210165-Rettig-Cryoablation_is_associated_with_shorter_length_of_stay_and_reduced_opioid_use_in_pectus_excavatum_repair..pdf.tmp' (Size: 625.52KB, Pages: 9, TextLen: 14691 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:46,564 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 33210165: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2021-33210165-Rettig-Cryoablation_is_associated_with_shorter_length_of_stay_and_reduced_opioid_use_in_pectus_excavatum_repair..pdf\n",
      "2025-05-22 13:42:46,566 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 30922685 from https://sci-hub.se/10.1016%2Fj.jpedsurg.2019.02.007 (Attempt 1/5)\n",
      "2025-05-22 13:42:46,803 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✓ PMID 30922685: Found potential PDF link: https://sci-hub.se/downloads/2019-11-24/2a/holmes2019.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:46,804 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 30922685: Attempting download from https://sci-hub.se/downloads/2019-11-24/2a/holmes2019.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:47,115 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 37062759 via https://sci-hub.ee: No PDF link found within HTML from https://sci-hub.ee/10.1089%2Flap.2022.0537\n",
      "2025-05-22 13:42:47,116 - ERROR - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 37062759: Failed for identifier '10.1089/lap.2022.0537' after trying all active domains.\n",
      "2025-05-22 13:42:47,118 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 34059337 from https://sci-hub.se/10.1016%2Fj.jtcvs.2021.04.072 (Attempt 1/5)\n",
      "2025-05-22 13:42:47,262 - INFO - [SciHub_Worker_0] - validate_downloaded_pdf - PDF Validation ✓ 30922685: File '2019-30922685-Holmes-Opioid_use_and_length_of_stay_following_minimally_invasive_pectus_excavatum_repair_in_436_patients_-_Benefits_of_an_enhanced_reco.pdf.tmp' (Size: 282.88KB, Pages: 8, TextLen: 17904 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:47,264 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 30922685: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2019-30922685-Holmes-Opioid_use_and_length_of_stay_following_minimally_invasive_pectus_excavatum_repair_in_436_patients_-_Benefits_of_an_enhanced_reco.pdf\n",
      "2025-05-22 13:42:47,265 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 31274269 from https://sci-hub.se/10.23736%2FS0375-9393.19.13880-1 (Attempt 1/5)\n",
      "2025-05-22 13:42:47,432 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✓ PMID 34059337: Found potential PDF link: https://sci-hub.se/uptodate/S0022522321007571.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:47,433 - INFO - [SciHub_Worker_1] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 34059337: Attempting download from https://sci-hub.se/uptodate/S0022522321007571.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:47,928 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 31274269 via https://sci-hub.se: No PDF link found within HTML from https://sci-hub.se/10.23736%2FS0375-9393.19.13880-1\n",
      "2025-05-22 13:42:48,001 - INFO - [SciHub_Worker_1] - validate_downloaded_pdf - PDF Validation ✓ 34059337: File '2021-34059337-Fuller-The_American_Association_for_Thoracic_Surgery_Congenital_Cardiac_Surgery_Working_Group_2021_consensus_document_on_a_comprehensive.pdf.tmp' (Size: 1198.48KB, Pages: 24, TextLen: 13795 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:48,003 - INFO - [SciHub_Worker_1] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 34059337: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2021-34059337-Fuller-The_American_Association_for_Thoracic_Surgery_Congenital_Cardiac_Surgery_Working_Group_2021_consensus_document_on_a_comprehensive.pdf\n",
      "2025-05-22 13:42:48,003 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 27679510 from https://sci-hub.se/10.1007%2Fs00383-016-3986-y (Attempt 1/5)\n",
      "2025-05-22 13:42:48,266 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✓ PMID 27679510: Found potential PDF link: https://dacemirror.sci-hub.se/journal-article/2fc87e862692a7a85621a6b8ef3df25f/pearson2016.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:48,268 - INFO - [SciHub_Worker_1] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 27679510: Attempting download from https://dacemirror.sci-hub.se/journal-article/2fc87e862692a7a85621a6b8ef3df25f/pearson2016.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:48,430 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 31274269 from https://sci-hub.ru/10.23736%2FS0375-9393.19.13880-1 (Attempt 2/5)\n",
      "2025-05-22 13:42:49,201 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 31274269 via https://sci-hub.ru: No PDF link found within HTML from https://sci-hub.ru/10.23736%2FS0375-9393.19.13880-1\n",
      "2025-05-22 13:42:49,720 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 31274269 from https://sci-hub.wf/10.23736%2FS0375-9393.19.13880-1 (Attempt 3/5)\n",
      "2025-05-22 13:42:49,786 - INFO - [SciHub_Worker_1] - validate_downloaded_pdf - PDF Validation ✓ 27679510: File '2017-27679510-Pearson-What_is_the_role_of_enhanced_recovery_after_surgery_in_children_A_scoping_review..pdf.tmp' (Size: 479.25KB, Pages: 9, TextLen: 10686 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:49,787 - INFO - [SciHub_Worker_1] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 27679510: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2017-27679510-Pearson-What_is_the_role_of_enhanced_recovery_after_surgery_in_children_A_scoping_review..pdf\n",
      "2025-05-22 13:42:49,788 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 26888001 from https://sci-hub.se/26888001 (Attempt 1/5)\n",
      "2025-05-22 13:42:50,658 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 26888001 via https://sci-hub.se: No PDF link found within HTML from https://sci-hub.se/26888001\n",
      "2025-05-22 13:42:51,160 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 26888001 from https://sci-hub.ru/26888001 (Attempt 2/5)\n",
      "2025-05-22 13:42:51,190 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 31274269 via https://sci-hub.wf: No PDF link found within HTML from https://sci-hub.wf/10.23736%2FS0375-9393.19.13880-1\n",
      "2025-05-22 13:42:51,692 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 31274269 from https://sci-hub.ren/10.23736%2FS0375-9393.19.13880-1 (Attempt 4/5)\n",
      "2025-05-22 13:42:51,926 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 26888001 via https://sci-hub.ru: No PDF link found within HTML from https://sci-hub.ru/26888001\n",
      "2025-05-22 13:42:52,427 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 26888001 from https://sci-hub.wf/26888001 (Attempt 3/5)\n",
      "2025-05-22 13:42:52,918 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub ✗ 31274269 via https://sci-hub.ren (https://sci-hub.ren/10.23736%2FS0375-9393.19.13880-1): RequestException: 404 Client Error: Not Found for url: https://sci-hub.ren/10.23736%2FS0375-9393.19.13880-1\n",
      "2025-05-22 13:42:53,420 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 31274269 from https://sci-hub.ee/10.23736%2FS0375-9393.19.13880-1 (Attempt 5/5)\n",
      "2025-05-22 13:42:53,739 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 26888001 via https://sci-hub.wf: No PDF link found within HTML from https://sci-hub.wf/26888001\n",
      "2025-05-22 13:42:54,241 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 26888001 from https://sci-hub.ren/26888001 (Attempt 4/5)\n",
      "2025-05-22 13:42:54,717 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 31274269 via https://sci-hub.ee: No PDF link found within HTML from https://sci-hub.ee/10.23736%2FS0375-9393.19.13880-1\n",
      "2025-05-22 13:42:54,718 - ERROR - [SciHub_Worker_0] - scihub_worker - Sci-Hub ✗ 31274269: Failed for identifier '10.23736/S0375-9393.19.13880-1' after trying all active domains.\n",
      "2025-05-22 13:42:54,719 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 27810148 from https://sci-hub.se/10.1016%2Fj.jpedsurg.2016.09.065 (Attempt 1/5)\n",
      "2025-05-22 13:42:55,311 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✓ PMID 27810148: Found potential PDF link: https://dacemirror.sci-hub.se/journal-article/09e2782084cf32ac4f6bc95e42f3927c/bryskin2016.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:55,312 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 27810148: Attempting download from https://dacemirror.sci-hub.se/journal-article/09e2782084cf32ac4f6bc95e42f3927c/bryskin2016.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:55,432 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 26888001 via https://sci-hub.ren (https://sci-hub.ren/26888001): RequestException: 404 Client Error: Not Found for url: https://sci-hub.ren/26888001\n",
      "2025-05-22 13:42:55,933 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 26888001 from https://sci-hub.ee/26888001 (Attempt 5/5)\n",
      "2025-05-22 13:42:57,349 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 26888001 via https://sci-hub.ee: No PDF link found within HTML from https://sci-hub.ee/26888001\n",
      "2025-05-22 13:42:57,355 - ERROR - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 26888001: Failed for identifier '26888001' after trying all active domains.\n",
      "2025-05-22 13:42:57,415 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 27083963 from https://sci-hub.se/10.1016%2Fj.jss.2015.12.051 (Attempt 1/5)\n",
      "2025-05-22 13:42:57,572 - INFO - [SciHub_Worker_0] - validate_downloaded_pdf - PDF Validation ✓ 27810148: File '2017-27810148-Bryskin-Introduction_of_a_novel_ultrasound-guided_extrathoracic_sub-paraspinal_block_for_control_of_perioperative_pain_in_Nuss_procedure.pdf.tmp' (Size: 1698.93KB, Pages: 8, TextLen: 18803 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:42:57,573 - INFO - [SciHub_Worker_0] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 27810148: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2017-27810148-Bryskin-Introduction_of_a_novel_ultrasound-guided_extrathoracic_sub-paraspinal_block_for_control_of_perioperative_pain_in_Nuss_procedure.pdf\n",
      "2025-05-22 13:42:57,576 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39520824 from https://sci-hub.se/10.1016%2Fj.jpedsurg.2024.162046 (Attempt 1/5)\n",
      "2025-05-22 13:42:58,030 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✓ PMID 27083963: Found potential PDF link: https://2024.sci-hub.se/4872/f123ca830f48aad26a1f6320b77d04ef/shinnick2016.pdf#navpanes=0&view=FitH...\n",
      "2025-05-22 13:42:58,030 - INFO - [SciHub_Worker_1] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) → PMID 27083963: Attempting download from https://2024.sci-hub.se/4872/f123ca830f48aad26a1f6320b77d04ef/shinnick2016.pdf#navpanes=0&view=FitH\n",
      "2025-05-22 13:42:58,139 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 39520824 via https://sci-hub.se: No PDF link found within HTML from https://sci-hub.se/10.1016%2Fj.jpedsurg.2024.162046\n",
      "2025-05-22 13:42:58,641 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39520824 from https://sci-hub.ru/10.1016%2Fj.jpedsurg.2024.162046 (Attempt 2/5)\n",
      "2025-05-22 13:42:59,106 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 39520824 via https://sci-hub.ru: No PDF link found within HTML from https://sci-hub.ru/10.1016%2Fj.jpedsurg.2024.162046\n",
      "2025-05-22 13:42:59,608 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39520824 from https://sci-hub.wf/10.1016%2Fj.jpedsurg.2024.162046 (Attempt 3/5)\n",
      "2025-05-22 13:43:00,014 - INFO - [SciHub_Worker_1] - validate_downloaded_pdf - PDF Validation ✓ 27083963: File '2016-27083963-Shinnick-Enhancing_recovery_in_pediatric_surgery_a_review_of_the_literature..pdf.tmp' (Size: 894.64KB, Pages: 12, TextLen: 16958 from first 3 page(s)) passed validation.\n",
      "2025-05-22 13:43:00,015 - INFO - [SciHub_Worker_1] - download_and_save_pdf - SciHub_Extracted(https://sci-hub.se) ✓ PMID 27083963: Successfully downloaded and validated PDF to downloaded_pdfs_v12_pow\\2016-27083963-Shinnick-Enhancing_recovery_in_pediatric_surgery_a_review_of_the_literature..pdf\n",
      "2025-05-22 13:43:00,017 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39083294 from https://sci-hub.se/10.1001%2Fjamasurg.2024.2044 (Attempt 1/5)\n",
      "2025-05-22 13:43:00,620 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 39083294 via https://sci-hub.se: No PDF link found within HTML from https://sci-hub.se/10.1001%2Fjamasurg.2024.2044\n",
      "2025-05-22 13:43:00,928 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 39520824 via https://sci-hub.wf: No PDF link found within HTML from https://sci-hub.wf/10.1016%2Fj.jpedsurg.2024.162046\n",
      "2025-05-22 13:43:01,121 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39083294 from https://sci-hub.ru/10.1001%2Fjamasurg.2024.2044 (Attempt 2/5)\n",
      "2025-05-22 13:43:01,429 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39520824 from https://sci-hub.ren/10.1016%2Fj.jpedsurg.2024.162046 (Attempt 4/5)\n",
      "2025-05-22 13:43:01,620 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 39083294 via https://sci-hub.ru: No PDF link found within HTML from https://sci-hub.ru/10.1001%2Fjamasurg.2024.2044\n",
      "2025-05-22 13:43:01,989 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub ✗ 39520824 via https://sci-hub.ren (https://sci-hub.ren/10.1016%2Fj.jpedsurg.2024.162046): RequestException: 404 Client Error: Not Found for url: https://sci-hub.ren/10.1016%2Fj.jpedsurg.2024.162046\n",
      "2025-05-22 13:43:02,122 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39083294 from https://sci-hub.wf/10.1001%2Fjamasurg.2024.2044 (Attempt 3/5)\n",
      "2025-05-22 13:43:02,492 - INFO - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML GET → PMID 39520824 from https://sci-hub.ee/10.1016%2Fj.jpedsurg.2024.162046 (Attempt 5/5)\n",
      "2025-05-22 13:43:03,488 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 39083294 via https://sci-hub.wf: No PDF link found within HTML from https://sci-hub.wf/10.1001%2Fjamasurg.2024.2044\n",
      "2025-05-22 13:43:03,824 - WARNING - [SciHub_Worker_0] - scihub_worker - Sci-Hub HTML ✗ 39520824 via https://sci-hub.ee: No PDF link found within HTML from https://sci-hub.ee/10.1016%2Fj.jpedsurg.2024.162046\n",
      "2025-05-22 13:43:03,824 - ERROR - [SciHub_Worker_0] - scihub_worker - Sci-Hub ✗ 39520824: Failed for identifier '10.1016/j.jpedsurg.2024.162046' after trying all active domains.\n",
      "2025-05-22 13:43:03,990 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39083294 from https://sci-hub.ren/10.1001%2Fjamasurg.2024.2044 (Attempt 4/5)\n",
      "2025-05-22 13:43:05,190 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 39083294 via https://sci-hub.ren (https://sci-hub.ren/10.1001%2Fjamasurg.2024.2044): RequestException: 404 Client Error: Not Found for url: https://sci-hub.ren/10.1001%2Fjamasurg.2024.2044\n",
      "2025-05-22 13:43:05,691 - INFO - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML GET → PMID 39083294 from https://sci-hub.ee/10.1001%2Fjamasurg.2024.2044 (Attempt 5/5)\n",
      "2025-05-22 13:43:06,991 - WARNING - [SciHub_Worker_1] - scihub_worker - Sci-Hub HTML ✗ 39083294 via https://sci-hub.ee: No PDF link found within HTML from https://sci-hub.ee/10.1001%2Fjamasurg.2024.2044\n",
      "2025-05-22 13:43:06,991 - ERROR - [SciHub_Worker_1] - scihub_worker - Sci-Hub ✗ 39083294: Failed for identifier '10.1001/jamasurg.2024.2044' after trying all active domains.\n",
      "2025-05-22 13:43:06,992 - INFO - [MainThread] - main - Sci-Hub Phase Summary: 12 PDFs successfully downloaded and validated.\n",
      "2025-05-22 13:43:06,994 - INFO - [MainThread] - main - 9 PMIDs still missing after Sci-Hub attempts or validation: ['26888001', '31274269', '36788057', '37062759', '37802689', '39068053', '39083294', '39520824', '39955421']...\n",
      "2025-05-22 13:43:06,996 - INFO - [MainThread] - main - --- Overall Summary ---\n",
      "2025-05-22 13:43:06,997 - INFO - [MainThread] - main - Processed 32 unique input PMIDs.\n",
      "2025-05-22 13:43:06,997 - INFO - [MainThread] - main - Attempted downloads for 32 PMIDs (those with metadata).\n",
      "2025-05-22 13:43:06,998 - INFO - [MainThread] - main - Total PDFs successfully downloaded & validated: 23 / 32.\n",
      "2025-05-22 13:43:06,999 - INFO - [MainThread] - main -   - Via Open Access (Unpaywall/PMC): 11\n",
      "2025-05-22 13:43:07,001 - INFO - [MainThread] - main -   - Via Sci-Hub: 12\n",
      "2025-05-22 13:43:07,002 - INFO - [MainThread] - main - Total PMIDs ultimately NOT downloaded or failed validation: 9\n",
      "2025-05-22 13:43:07,003 - INFO - [MainThread] - main - Logging details of 9 failed PMIDs to downloaded_pdfs_v12_pow\\failed_articles_details.log\n",
      "2025-05-22 13:43:07,008 - INFO - [MainThread] - main - Total execution time: 74.95 seconds.\n",
      "2025-05-22 13:43:07,009 - INFO - [MainThread] - main - === PDF Fetcher v12-pow completed at 2025-05-22 13:43:07 ===\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Combined OA-first + Sci-Hub PDF Fetcher (v12-pow, standalone)\n",
    "-----------------------------------------------------------\n",
    "1) Reads PMIDs from an Excel file.\n",
    "2) Fetches metadata (year, first author, title, DOI, Abstract, MeSH) via NCBI EFetch.\n",
    "3) Tries Open Access first:\n",
    "    a) Unpaywall.\n",
    "    b) PMC (handles Proof-of-Work challenge, resolves final URL).\n",
    "4) Any remaining PMIDs are tried via Sci-Hub in parallel.\n",
    "5) All PDFs are validated (size, content) and land in OUTPUT_PDF_DIR\n",
    "   named `{year}-{pmid}-{author}-{title}.pdf`.\n",
    "6) Detailed logging of every URL fetched and why it failed or succeeded.\n",
    "7) Enhanced, structured logging for PMIDs that fail retrieval or have suspicious PDFs.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, quote_plus\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import hashlib # For PoW\n",
    "try:\n",
    "    from pypdf import PdfReader # Preferred (PyPDF2 successor)\n",
    "    from pypdf.errors import PdfReadError\n",
    "except ImportError:\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader # Fallback\n",
    "        from PyPDF2.errors import PdfReadError\n",
    "    except ImportError:\n",
    "        print(\"Please install pypdf or PyPDF2: pip install pypdf\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "EXCEL_FILE_PATH     = r\"C:\\Users\\Galaxy\\Downloads\\screening_ERAS.xlsx\" # UPDATE THIS\n",
    "OUTPUT_PDF_DIR      = \"downloaded_pdfs_v12_pow\"\n",
    "SUSPICIOUS_PDF_SUBDIR = \"suspicious_pdfs\" # Subdirectory for failed validation PDFs\n",
    "\n",
    "# Sci-Hub\n",
    "SCI_HUB_DOMAINS     = [\n",
    "    \"https://sci-hub.se\", \"https://sci-hub.ru\", \"https://sci-hub.ren\",\n",
    "    \"https://sci-hub.wf\", \"https://sci-hub.ee\", \"https://sci-hub.st\"\n",
    "]\n",
    "DELAY_SCIHUB        = 0.5\n",
    "\n",
    "# Threads / batching\n",
    "MAX_THREADS         = 5 \n",
    "EFETCH_BATCH_SIZE   = 100\n",
    "DELAY_NCBI          = 0.35 # Seconds between NCBI EUtils calls\n",
    "\n",
    "# PDF Validation Thresholds\n",
    "MIN_PDF_SIZE_KB         = 20    \n",
    "MIN_PDF_PAGES           = 1     \n",
    "MIN_TEXT_LENGTH_CHARS   = 300   \n",
    "\n",
    "# API credentials - PLEASE FILL THESE IN\n",
    "NCBI_API_KEY        = \"YOUR_API_KEY_HERE\" \n",
    "CROSSREF_EMAIL      = \"your_email@example.com\" \n",
    "UNPAYWALL_EMAIL     = \"levi4328@gmail.com\" \n",
    "\n",
    "# === Global sets for tracking logged PMIDs to avoid duplicates in detailed logs ===\n",
    "LOGGED_SUSPICIOUS_PMIDS = set()\n",
    "LOGGED_FAILED_PMIDS = set()\n",
    "\n",
    "# === LOGGING SETUP ===\n",
    "logger = logging.getLogger(\"PDFFetcherV12\")\n",
    "logger.handlers = [] \n",
    "logger.setLevel(logging.INFO) \n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - [%(threadName)s] - %(funcName)s - %(message)s'\n",
    "))\n",
    "logger.addHandler(ch)\n",
    "\n",
    "# === HTTP SESSIONS WITH RETRIES ===\n",
    "CHROME_UA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "\n",
    "def make_session(user_agent, is_scihub_session=False):\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=frozenset(['GET', 'POST', 'HEAD'])\n",
    "    )\n",
    "    pool_connections = MAX_THREADS // 2 if is_scihub_session and MAX_THREADS > 2 else MAX_THREADS\n",
    "    pool_maxsize = pool_connections * 2 \n",
    "\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retries,\n",
    "        pool_connections=pool_connections,\n",
    "        pool_maxsize=pool_maxsize \n",
    "    )\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.headers.update({'User-Agent': user_agent})\n",
    "    return s\n",
    "\n",
    "ncbi_ua_email_part = CROSSREF_EMAIL if CROSSREF_EMAIL and CROSSREF_EMAIL != \"your_email@example.com\" else \"anonymous_user\"\n",
    "session_ncbi   = make_session(f\"PDFFetcherV12/1.0 (NCBI-EUtils-Client; mailto:{ncbi_ua_email_part})\")\n",
    "session_oa     = make_session(CHROME_UA) \n",
    "session_scihub = make_session(CHROME_UA, is_scihub_session=True) \n",
    "\n",
    "BROWSER_LIKE_HEADERS = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br', \n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none', \n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Cache-Control': 'no-cache', \n",
    "    'Pragma': 'no-cache'        \n",
    "}\n",
    "\n",
    "# === UTILITIES ===\n",
    "def sanitize_filename(s: str) -> str:\n",
    "    s = str(s) \n",
    "    s = re.sub(r'[\\\\/*?:\"<>|]', \"\", s) \n",
    "    s = re.sub(r'\\s+', \" \", s).strip() \n",
    "    if not s: s = \"untitled_article\"   \n",
    "    return s.replace(\" \", \"_\")[:150] \n",
    "\n",
    "def _get_ncbi_params(extra=None):\n",
    "    params = {\"tool\": \"PDFFetcherV12\"} \n",
    "    if CROSSREF_EMAIL and CROSSREF_EMAIL != \"your_email@example.com\":\n",
    "        params[\"email\"] = CROSSREF_EMAIL\n",
    "    if NCBI_API_KEY and NCBI_API_KEY != \"YOUR_API_KEY_HERE\":\n",
    "        params[\"api_key\"] = NCBI_API_KEY\n",
    "    if extra:\n",
    "        params.update(extra)\n",
    "    return params\n",
    "\n",
    "# === PROOF-OF-WORK (PoW) SOLVING LOGIC for PMC ===\n",
    "def extract_pow_params_from_html(html_content: str) -> tuple[str, int, str, str, str] | None:\n",
    "    challenge_match = re.search(r'const\\s+POW_CHALLENGE\\s*=\\s*\"(.*?)\"', html_content)\n",
    "    difficulty_match = re.search(r'const\\s+POW_DIFFICULTY\\s*=\\s*\"(.*?)\"', html_content)\n",
    "    cookie_name_match = re.search(r'const\\s+POW_COOKIE_NAME\\s*=\\s*\"(.*?)\"', html_content)\n",
    "    cookie_exp_match = re.search(r'const\\s+POW_COOKIE_EXPIRATION\\s*=\\s*\"(.*?)\"', html_content) \n",
    "    cookie_path_match = re.search(r'const\\s+POW_COOKIE_PATH\\s*=\\s*\"(.*?)\"', html_content)\n",
    "\n",
    "    if challenge_match and difficulty_match and cookie_name_match and cookie_path_match:\n",
    "        challenge_string = challenge_match.group(1)\n",
    "        cookie_name = cookie_name_match.group(1)\n",
    "        cookie_exp_str = cookie_exp_match.group(1) if cookie_exp_match else \"0.208333\" \n",
    "        cookie_path = cookie_path_match.group(1)\n",
    "        try:\n",
    "            difficulty = int(difficulty_match.group(1))\n",
    "            logger.info(f\"Extracted PoW params: Challenge='{challenge_string[:20]}...', Diff={difficulty}, Name='{cookie_name}'\")\n",
    "            return challenge_string, difficulty, cookie_name, cookie_exp_str, cookie_path\n",
    "        except ValueError:\n",
    "            logger.error(f\"Could not parse PoW difficulty as int: '{difficulty_match.group(1)}'\")\n",
    "    else:\n",
    "        logger.warning(\"Could not find all required PoW parameters in HTML content.\")\n",
    "    return None\n",
    "\n",
    "def solve_pmc_pow(challenge_string: str, difficulty: int) -> tuple[int, str] | None:\n",
    "    logger.info(f\"Solving PoW: challenge='{challenge_string[:20]}...', difficulty={difficulty}\")\n",
    "    target_prefix = \"0\" * difficulty\n",
    "    nonce = 0\n",
    "    max_nonce_map = {4: 2_000_000, 5: 35_000_000, 6: 500_000_000} \n",
    "    max_nonce = max_nonce_map.get(difficulty, 100_000_000) \n",
    "\n",
    "    start_time = time.time()\n",
    "    while nonce <= max_nonce:\n",
    "        test_string = challenge_string + str(nonce)\n",
    "        hash_object = hashlib.sha256(test_string.encode('utf-8')) \n",
    "        hex_digest = hash_object.hexdigest()\n",
    "        if hex_digest.startswith(target_prefix):\n",
    "            duration = time.time() - start_time\n",
    "            logger.info(f\"PoW SOLVED! Nonce: {nonce}, Hash: {hex_digest[:10]}..., Time: {duration:.4f}s\")\n",
    "            return nonce, hex_digest\n",
    "        if nonce > 0 and nonce % 1_000_000 == 0: \n",
    "            logger.debug(f\"PoW progress: nonce {nonce}...\")\n",
    "        nonce += 1\n",
    "    duration = time.time() - start_time\n",
    "    logger.error(f\"PoW FAILED to solve (max_nonce {max_nonce} reached for difficulty {difficulty}). Time: {duration:.2f}s\")\n",
    "    return None\n",
    "\n",
    "# === PDF VALIDATION (returns failure reason string or None for success) ===\n",
    "def validate_downloaded_pdf(pdf_path: str, pmid_for_log: str) -> str | None:\n",
    "    \"\"\"Validates a downloaded PDF. Returns None if valid, else a string describing failure reason.\"\"\"\n",
    "    failure_reason = \"\"\n",
    "    try:\n",
    "        file_size_kb = os.path.getsize(pdf_path) / 1024\n",
    "        if file_size_kb < MIN_PDF_SIZE_KB:\n",
    "            failure_reason = f\"File size {file_size_kb:.2f} KB < threshold {MIN_PDF_SIZE_KB} KB\"\n",
    "            logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}'.\")\n",
    "            return failure_reason\n",
    "\n",
    "        reader = None\n",
    "        num_pages = 0\n",
    "        try:\n",
    "            reader = PdfReader(pdf_path)\n",
    "            num_pages = len(reader.pages)\n",
    "            if num_pages < MIN_PDF_PAGES:\n",
    "                failure_reason = f\"Page count {num_pages} < threshold {MIN_PDF_PAGES}\"\n",
    "                logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f} KB).\")\n",
    "                return failure_reason\n",
    "        except PdfReadError as e:\n",
    "            failure_reason = f\"pypdf PdfReadError: {e}\"\n",
    "            logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f} KB).\")\n",
    "            return failure_reason\n",
    "        except Exception as e_open:\n",
    "            failure_reason = f\"pypdf unexpected open error: {e_open}\"\n",
    "            logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f} KB).\")\n",
    "            return failure_reason\n",
    "\n",
    "        extracted_text_len = 0\n",
    "        max_pages_to_check_text = min(3, num_pages)\n",
    "        if reader:\n",
    "            for i in range(max_pages_to_check_text):\n",
    "                try:\n",
    "                    page = reader.pages[i]\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        extracted_text_len += len(page_text)\n",
    "                except Exception as e_text_extract:\n",
    "                    logger.warning(f\"PDF Validation ? {pmid_for_log}: Error extracting text from page {i+1} of '{os.path.basename(pdf_path)}': {e_text_extract}. Continuing.\")\n",
    "        \n",
    "        if extracted_text_len < MIN_TEXT_LENGTH_CHARS:\n",
    "            failure_reason = f\"Insufficient text ({extracted_text_len} chars from first {max_pages_to_check_text} page(s) < threshold {MIN_TEXT_LENGTH_CHARS})\"\n",
    "            logger.warning(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f} KB).\")\n",
    "            return failure_reason\n",
    "\n",
    "        logger.info(f\"PDF Validation ✓ {pmid_for_log}: File '{os.path.basename(pdf_path)}' (Size: {file_size_kb:.2f}KB, Pages: {num_pages}, TextLen: {extracted_text_len} from first {max_pages_to_check_text} page(s)) passed validation.\")\n",
    "        return None # Success\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"PDF Validation ✗ {pmid_for_log}: File not found at {pdf_path} for validation.\")\n",
    "        return \"File not found for validation.\" \n",
    "    except Exception as e:\n",
    "        failure_reason = f\"Unexpected error during validation setup: {e}\"\n",
    "        logger.error(f\"PDF Validation ✗ {pmid_for_log}: {failure_reason} for {pdf_path}\", exc_info=True)\n",
    "        return failure_reason \n",
    "\n",
    "# === ENHANCED LOGGING FOR FAILURES/SUSPICIOUS FILES ===\n",
    "def log_enhanced_failure_details(log_filename: str, pmid: str, metadata: dict, \n",
    "                                 failure_type: str, details: str | None = None, \n",
    "                                 file_path: str | None = None, logged_set: set | None = None):\n",
    "    \"\"\"Logs detailed information about a failed or suspicious PMID to a specified log file.\"\"\"\n",
    "    if logged_set is not None: \n",
    "        if pmid in logged_set:\n",
    "            logger.debug(f\"PMID {pmid} already logged in {log_filename}. Skipping duplicate entry.\")\n",
    "            return \n",
    "    \n",
    "    if metadata is None: \n",
    "        metadata = {} \n",
    "        logger.warning(f\"No metadata provided for PMID {pmid} during detailed logging to {log_filename}.\")\n",
    "\n",
    "    entry_lines = [\n",
    "        \"----------------------------------------\",\n",
    "        f\"PMID: {pmid}\",\n",
    "        f\"Failure Type: {failure_type}\"\n",
    "    ]\n",
    "    if details: \n",
    "        entry_lines.append(f\"Details: {details}\")\n",
    "    if file_path: \n",
    "        entry_lines.append(f\"File Path: {os.path.abspath(file_path)}\") \n",
    "    \n",
    "    entry_lines.extend([\n",
    "        f\"DOI: {metadata.get('doi', 'N/A')}\",\n",
    "        f\"Year: {metadata.get('year', 'N/A')}\",\n",
    "        f\"Author: {metadata.get('author', 'N/A')}\",\n",
    "        f\"Title: {metadata.get('title', 'N/A')}\",\n",
    "        f\"Abstract:\\n{metadata.get('abstract', 'N/A')}\",\n",
    "        f\"MeSH Terms: {metadata.get('mesh_terms', 'N/A')}\",\n",
    "        \"----------------------------------------\\n\" \n",
    "    ])\n",
    "    \n",
    "    log_file_full_path = os.path.join(OUTPUT_PDF_DIR, log_filename)\n",
    "    try:\n",
    "        with open(log_file_full_path, \"a\", encoding=\"utf-8\") as f_log:\n",
    "            f_log.write(\"\\n\".join(entry_lines))\n",
    "        if logged_set is not None:\n",
    "            logged_set.add(pmid)\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Could not write to enhanced log file {log_file_full_path}: {e}\")\n",
    "\n",
    "\n",
    "# === CENTRALIZED PDF DOWNLOADER (handles PoW for PMC) ===\n",
    "def download_and_save_pdf(\n",
    "    session: requests.Session, \n",
    "    pdf_url: str, \n",
    "    output_path: str, \n",
    "    pmid_for_log: str, \n",
    "    source_name: str,\n",
    "    article_metadata: dict, \n",
    "    referer: str | None = None\n",
    "    ) -> bool:\n",
    "    \n",
    "    logger.info(f\"{source_name} → PMID {pmid_for_log}: Attempting download from {pdf_url}\")\n",
    "    \n",
    "    current_headers = session.headers.copy() \n",
    "    current_headers.update(BROWSER_LIKE_HEADERS) \n",
    "    current_headers['Accept'] = 'application/pdf,text/html;q=0.9,application/xhtml+xml,application/xml;q=0.8,*/*;q=0.5'\n",
    "    if referer:\n",
    "        current_headers['Referer'] = referer\n",
    "    \n",
    "    parsed_pdf_url = urlparse(pdf_url)\n",
    "    if 'pmc.ncbi.nlm.nih.gov' in parsed_pdf_url.netloc:\n",
    "        is_same_origin_pmc = referer and 'pmc.ncbi.nlm.nih.gov' in urlparse(referer).netloc\n",
    "        current_headers['Sec-Fetch-Site'] = 'same-origin' if is_same_origin_pmc else 'cross-site'\n",
    "\n",
    "    temp_pdf_path = output_path + \".tmp\" \n",
    "\n",
    "    try:\n",
    "        r = session.get(pdf_url, headers=current_headers, stream=True, timeout=(15, 60), allow_redirects=True) \n",
    "        r.raise_for_status() \n",
    "        final_url_after_redirects = r.url \n",
    "        content_type = r.headers.get('Content-Type', '').lower()\n",
    "        logger.debug(f\"{source_name} → PMID {pmid_for_log}: Initial GET to {pdf_url} (final: {final_url_after_redirects}), Content-Type: {content_type}\")\n",
    "\n",
    "        html_content_bytes = None \n",
    "\n",
    "        if \"pmc.ncbi.nlm.nih.gov\" in urlparse(final_url_after_redirects).netloc and 'text/html' in content_type:\n",
    "            logger.info(f\"{source_name} → PMID {pmid_for_log}: Received HTML from PMC URL, attempting PoW solve.\")\n",
    "            html_content_bytes = r.content \n",
    "            html_content_str = html_content_bytes.decode('utf-8', errors='replace')\n",
    "            \n",
    "            sanitized_source_detail_for_html = sanitize_filename(source_name)\n",
    "            debug_html_path = os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR, f\"{pmid_for_log}_{sanitized_source_detail_for_html}_challenge.html\")\n",
    "            os.makedirs(os.path.dirname(debug_html_path), exist_ok=True)\n",
    "            with open(debug_html_path, \"w\", encoding=\"utf-8\") as f_debug: f_debug.write(html_content_str)\n",
    "            logger.info(f\"Saved PMC challenge HTML to {debug_html_path}\")\n",
    "\n",
    "            pow_params = extract_pow_params_from_html(html_content_str)\n",
    "            if not pow_params:\n",
    "                logger.error(f\"{source_name} → PMID {pmid_for_log}: Failed to extract PoW params from PMC HTML.\")\n",
    "                return False \n",
    "            \n",
    "            challenge_str, difficulty_val, cookie_name, _, cookie_path = pow_params\n",
    "            solution = solve_pmc_pow(challenge_str, difficulty_val)\n",
    "            if not solution:\n",
    "                logger.error(f\"{source_name} → PMID {pmid_for_log}: Failed to solve PMC PoW.\")\n",
    "                return False \n",
    "\n",
    "            nonce_found, _ = solution\n",
    "            pow_cookie_value = f\"{challenge_str},{nonce_found}\" \n",
    "            \n",
    "            parsed_uri = urlparse(final_url_after_redirects)\n",
    "            session.cookies.set(name=cookie_name, value=pow_cookie_value, domain=parsed_uri.hostname, path=cookie_path)\n",
    "            logger.info(f\"Set PoW cookie '{cookie_name}' in session for {parsed_uri.hostname}.\")\n",
    "\n",
    "            logger.info(f\"{source_name} → PMID {pmid_for_log}: Re-attempting GET to {final_url_after_redirects} WITH PoW cookie.\")\n",
    "            current_headers['Accept'] = 'application/pdf,application/octet-stream,*/*;q=0.8' \n",
    "            current_headers['Referer'] = final_url_after_redirects \n",
    "            current_headers['Sec-Fetch-Site'] = 'same-origin' \n",
    "\n",
    "            r = session.get(final_url_after_redirects, headers=current_headers, stream=True, timeout=(15, 60))\n",
    "            r.raise_for_status()\n",
    "            content_type = r.headers.get('Content-Type', '').lower() \n",
    "            logger.debug(f\"{source_name} → PMID {pmid_for_log}: Second GET (post-PoW), Content-Type: {content_type}\")\n",
    "\n",
    "        if 'application/pdf' in content_type or final_url_after_redirects.lower().endswith(\".pdf\"):\n",
    "            with open(temp_pdf_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=81920): \n",
    "                    f.write(chunk)\n",
    "            \n",
    "            validation_failure_reason = validate_downloaded_pdf(temp_pdf_path, pmid_for_log)\n",
    "            if not validation_failure_reason: \n",
    "                os.rename(temp_pdf_path, output_path) \n",
    "                logger.info(f\"{source_name} ✓ PMID {pmid_for_log}: Successfully downloaded and validated PDF to {output_path}\")\n",
    "                return True\n",
    "            else: \n",
    "                logger.warning(f\"{source_name} ✗ PMID {pmid_for_log}: PDF from {final_url_after_redirects} failed validation: {validation_failure_reason}\")\n",
    "                suspicious_dir = os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR)\n",
    "                os.makedirs(suspicious_dir, exist_ok=True)\n",
    "                \n",
    "                base_output_filename = sanitize_filename(os.path.basename(output_path).replace(\".pdf\", \"\")) \n",
    "                sanitized_source_detail = sanitize_filename(source_name) \n",
    "                suspicious_filename = f\"{base_output_filename}.{sanitized_source_detail}.validation_failed.pdf\"\n",
    "                if len(os.path.join(suspicious_dir, suspicious_filename)) > 250: \n",
    "                    suspicious_filename = f\"{base_output_filename[:100]}.{sanitized_source_detail[:50]}.validation_failed.pdf\"\n",
    "\n",
    "                suspicious_path = os.path.join(suspicious_dir, suspicious_filename)\n",
    "                try:\n",
    "                    if os.path.exists(temp_pdf_path): \n",
    "                        os.rename(temp_pdf_path, suspicious_path)\n",
    "                        logger.info(f\"Moved suspicious PDF to {suspicious_path}\")\n",
    "                        log_enhanced_failure_details(\n",
    "                            \"suspicious_articles_details.log\", \n",
    "                            pmid_for_log, \n",
    "                            article_metadata, \n",
    "                            \"Suspicious PDF (Validation Failed)\", \n",
    "                            details=validation_failure_reason, \n",
    "                            file_path=suspicious_path,\n",
    "                            logged_set=LOGGED_SUSPICIOUS_PMIDS\n",
    "                        )\n",
    "                    else: \n",
    "                        logger.warning(f\"Temporary PDF {temp_pdf_path} not found for moving to suspicious.\")\n",
    "                except OSError as e_rename: \n",
    "                    logger.error(f\"OSError moving suspicious PDF {temp_pdf_path} to {suspicious_path}: {e_rename}\")\n",
    "                    if os.path.exists(temp_pdf_path): os.remove(temp_pdf_path) \n",
    "                return False\n",
    "        else: \n",
    "            logger.warning(f\"{source_name} ✗ PMID {pmid_for_log}: Non-PDF content from {final_url_after_redirects}. Content-Type: {content_type}\")\n",
    "            \n",
    "            sanitized_source_detail_for_debug = sanitize_filename(source_name)\n",
    "            debug_content_filename = f\"{pmid_for_log}_{sanitized_source_detail_for_debug}_unexpected_content.dat\"\n",
    "            debug_content_path = os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR, debug_content_filename)\n",
    "            os.makedirs(os.path.dirname(debug_content_path), exist_ok=True)\n",
    "            try:\n",
    "                content_to_save = html_content_bytes if html_content_bytes is not None else r.content\n",
    "                with open(debug_content_path, 'wb') as f_debug:\n",
    "                    f_debug.write(content_to_save)\n",
    "                logger.info(f\"Saved unexpected content ({len(content_to_save)} bytes) to {debug_content_path}\")\n",
    "                log_enhanced_failure_details(\n",
    "                    \"suspicious_articles_details.log\", \n",
    "                    pmid_for_log, \n",
    "                    article_metadata,\n",
    "                    \"Non-PDF Content Received\",\n",
    "                    details=f\"Content-Type: {content_type}, URL: {final_url_after_redirects}\",\n",
    "                    file_path=debug_content_path,\n",
    "                    logged_set=LOGGED_SUSPICIOUS_PMIDS \n",
    "                )\n",
    "            except Exception as e_save_debug:\n",
    "                logger.error(f\"Error saving unexpected content for PMID {pmid_for_log}: {e_save_debug}\")\n",
    "            return False\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.warning(f\"{source_name} ✗ PMID {pmid_for_log}: RequestException for {pdf_url}: {e}\")\n",
    "    except Exception as e_main: \n",
    "        logger.error(f\"{source_name} ✗ PMID {pmid_for_log}: Unexpected error during download from {pdf_url}: {e_main}\", exc_info=True)\n",
    "    \n",
    "    if os.path.exists(temp_pdf_path):\n",
    "        try:\n",
    "            os.remove(temp_pdf_path)\n",
    "        except OSError as e_remove:\n",
    "            logger.warning(f\"Could not remove temp PDF {temp_pdf_path}: {e_remove}\")\n",
    "    return False\n",
    "\n",
    "# === STEP 1: FETCH METADATA FROM PUBMED ===\n",
    "def fetch_metadata(pmids):\n",
    "    meta = {}\n",
    "    batches = [pmids[i:i+EFETCH_BATCH_SIZE] for i in range(0, len(pmids), EFETCH_BATCH_SIZE)]\n",
    "    logger.info(f\"EFetch PubMed metadata for {len(pmids)} PMIDs in {len(batches)} batch(es)...\")\n",
    "    \n",
    "    for i_batch, batch in enumerate(batches):\n",
    "        efetch_payload = _get_ncbi_params({\n",
    "            \"db\": \"pubmed\",\n",
    "            \"retmode\": \"xml\",\n",
    "            \"id\": \",\".join(map(str, batch))\n",
    "        })\n",
    "        logger.info(f\"NCBI EFetch POST (batch {i_batch+1}/{len(batches)}) → IDs={','.join(map(str,batch[:3]))}...\")\n",
    "        try:\n",
    "            resp = session_ncbi.post(\n",
    "                \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\",\n",
    "                data=efetch_payload, \n",
    "                timeout=60 \n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            root = ET.fromstring(resp.content)\n",
    "            \n",
    "            for art_node in root.findall(\".//PubmedArticle\"):\n",
    "                pmid_el = art_node.find(\".//PMID\")\n",
    "                pmid = pmid_el.text.strip() if pmid_el is not None and pmid_el.text else None\n",
    "                if not pmid: continue\n",
    "\n",
    "                doi_el = art_node.find(\".//ArticleId[@IdType='doi']\") or \\\n",
    "                         art_node.find(\".//ELocationID[@EIdType='doi'][@ValidYN='Y']\")\n",
    "                doi = doi_el.text.strip() if doi_el is not None and doi_el.text else None\n",
    "                \n",
    "                year_el = art_node.find(\".//PubDate/Year\") or \\\n",
    "                          art_node.find(\".//Journal/JournalIssue/PubDate/Year\")\n",
    "                year = \"UnknownYear\"\n",
    "                if year_el is not None and year_el.text and year_el.text.strip().isdigit() and len(year_el.text.strip()) == 4:\n",
    "                    year = year_el.text.strip()\n",
    "                else:\n",
    "                    medline_date_el = art_node.find(\".//PubDate/MedlineDate\") or \\\n",
    "                                      art_node.find(\".//Article/Journal/JournalIssue/PubDate/MedlineDate\")\n",
    "                    if medline_date_el is not None and medline_date_el.text:\n",
    "                        year_match = re.match(r\"^\\d{4}\", medline_date_el.text.strip())\n",
    "                        if year_match: year = year_match.group(0)\n",
    "                \n",
    "                author_el = art_node.find(\".//AuthorList/Author[1]/LastName\")\n",
    "                author = author_el.text.strip() if author_el is not None and author_el.text else \"UnknownAuthor\"\n",
    "                \n",
    "                title_el = art_node.find(\".//ArticleTitle\")\n",
    "                title = \"\".join(title_el.itertext()).strip() if title_el is not None else f\"NoTitle_{pmid}\"\n",
    "\n",
    "                abstract_parts = []\n",
    "                for abstract_text_node in art_node.findall(\".//Abstract/AbstractText\"):\n",
    "                    if abstract_text_node.text:\n",
    "                        label = abstract_text_node.get(\"Label\")\n",
    "                        if label:\n",
    "                            abstract_parts.append(f\"[{label.upper()}] {abstract_text_node.text.strip()}\")\n",
    "                        else:\n",
    "                            abstract_parts.append(abstract_text_node.text.strip())\n",
    "                abstract = \"\\n\".join(abstract_parts) if abstract_parts else \"N/A\"\n",
    "\n",
    "                mesh_terms = []\n",
    "                for mesh_heading_node in art_node.findall(\".//MeshHeadingList/MeshHeading\"):\n",
    "                    descriptor_name_node = mesh_heading_node.find(\"./DescriptorName\")\n",
    "                    if descriptor_name_node is not None and descriptor_name_node.text:\n",
    "                        mesh_terms.append(descriptor_name_node.text.strip())\n",
    "                mesh_terms_str = \"; \".join(mesh_terms) if mesh_terms else \"N/A\"\n",
    "                \n",
    "                meta[pmid] = {\n",
    "                    'doi': doi, 'year': year, 'author': author, 'title': title,\n",
    "                    'abstract': abstract, 'mesh_terms': mesh_terms_str\n",
    "                }\n",
    "        except requests.exceptions.RequestException as e_req:\n",
    "            logger.warning(f\"NCBI EFetch batch {i_batch+1} RequestException: {e_req}\")\n",
    "        except ET.ParseError as e_xml:\n",
    "            response_text_snippet = resp.text[:200] if 'resp' in locals() and hasattr(resp, 'text') else \"N/A\"\n",
    "            logger.warning(f\"NCBI EFetch batch {i_batch+1} XML ParseError: {e_xml}. Content: {response_text_snippet}\")\n",
    "        except Exception as e_generic:\n",
    "            logger.error(f\"NCBI EFetch batch {i_batch+1} unexpected error: {e_generic}\", exc_info=True)\n",
    "        \n",
    "        if i_batch < len(batches) - 1: \n",
    "            time.sleep(DELAY_NCBI)\n",
    "\n",
    "    missing_meta_pmids = [p for p in pmids if p not in meta]\n",
    "    if missing_meta_pmids:\n",
    "        logger.warning(f\"Metadata missing for {len(missing_meta_pmids)} PMIDs: {missing_meta_pmids[:10]}...\")\n",
    "    return meta\n",
    "\n",
    "# === STEP 2: OPEN ACCESS (Unpaywall, PMC with PoW) ===\n",
    "def unpaywall_get_pdf_url(doi: str) -> str | None:\n",
    "    if not doi: return None \n",
    "    if not UNPAYWALL_EMAIL or UNPAYWALL_EMAIL == \"your_email@example.com\":\n",
    "        logger.debug(f\"Unpaywall API skipped for DOI {doi}: UNPAYWALL_EMAIL not configured.\")\n",
    "        return None\n",
    "\n",
    "    api_url = f\"https://api.unpaywall.org/v2/{quote_plus(doi)}?email={UNPAYWALL_EMAIL}\"\n",
    "    logger.info(f\"Unpaywall API GET → DOI {doi}\") \n",
    "    try:\n",
    "        r = session_oa.get(api_url, timeout=20) \n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        \n",
    "        if data.get(\"is_oa\"):\n",
    "            pdf_url = None\n",
    "            best_loc = data.get(\"best_oa_location\")\n",
    "            if best_loc and best_loc.get(\"url_for_pdf\"):\n",
    "                pdf_url = best_loc.get(\"url_for_pdf\")\n",
    "            \n",
    "            if not pdf_url:\n",
    "                for loc in data.get(\"oa_locations\", []):\n",
    "                    if loc.get(\"url_for_pdf\"):\n",
    "                        pdf_url = loc.get(\"url_for_pdf\")\n",
    "                        logger.debug(f\"Unpaywall API: Found PDF URL in other oa_locations: {pdf_url}\")\n",
    "                        break \n",
    "            \n",
    "            if pdf_url:\n",
    "                logger.info(f\"Unpaywall API ✓ DOI {doi}: Found PDF URL: {pdf_url.split('?')[0]}...\") \n",
    "                return pdf_url\n",
    "            else:\n",
    "                logger.info(f\"Unpaywall API ? DOI {doi}: Article is OA, but no direct PDF URL in Unpaywall response.\")\n",
    "        else:\n",
    "            logger.info(f\"Unpaywall API ~ DOI {doi}: Not OA according to Unpaywall.\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e_req:\n",
    "        logger.warning(f\"Unpaywall API ✗ DOI {doi}: RequestException: {e_req}\")\n",
    "    except ValueError as e_json: \n",
    "        logger.warning(f\"Unpaywall API ✗ DOI {doi}: JSON Decode Error: {e_json}. Response: {r.text[:200] if 'r' in locals() else 'N/A'}\")\n",
    "    except Exception as e_generic:\n",
    "        logger.error(f\"Unpaywall API ✗ DOI {doi}: Unexpected error: {e_generic}\", exc_info=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "def pmc_id_for_pmid(pmid: str, article_metadata: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    Finds the PMCID for a given PMID using Entrez ELink.\n",
    "    Prioritizes direct 'pubmed_pmc' link.\n",
    "    article_metadata is passed for potential future verification steps.\n",
    "    \"\"\"\n",
    "    linkname_to_try = \"pubmed_pmc\" \n",
    "\n",
    "    params = _get_ncbi_params({\n",
    "        \"dbfrom\": \"pubmed\",\n",
    "        \"db\": \"pmc\",\n",
    "        \"id\": pmid,\n",
    "        \"cmd\": \"neighbor_score\", \n",
    "        \"linkname\": linkname_to_try\n",
    "    })\n",
    "\n",
    "    logger.info(f\"PMC ID ELink → PMID {pmid}: Querying with linkname '{linkname_to_try}'.\")\n",
    "    try:\n",
    "        r = session_ncbi.post(\n",
    "            \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\",\n",
    "            data=params,\n",
    "            timeout=20 \n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        root = ET.fromstring(r.content)\n",
    "\n",
    "        link_set_db_el = root.find(f\".//LinkSetDb[LinkName='{linkname_to_try}']\")\n",
    "\n",
    "        if link_set_db_el is None:\n",
    "            info_el_general = root.find(\".//Info\") \n",
    "            if info_el_general is not None and info_el_general.text:\n",
    "                 logger.info(f\"PMC ID ELink ~ PMID {pmid}: NCBI Info: {info_el_general.text.strip()}\")\n",
    "            else:\n",
    "                 logger.debug(f\"PMC ID ELink ~ PMID {pmid}: No <LinkSetDb> for '{linkname_to_try}'. XML: {r.text[:250]}\")\n",
    "            return None\n",
    "\n",
    "        ids = [el.text.strip() for el in link_set_db_el.findall(\"./Link/Id\") if el.text]\n",
    "\n",
    "        if ids:\n",
    "            pmc_candidate_id_num = ids[0] \n",
    "            returned_pmcid = \"\"\n",
    "            if pmc_candidate_id_num.upper().startswith(\"PMC\"):\n",
    "                returned_pmcid = pmc_candidate_id_num\n",
    "            elif pmc_candidate_id_num.isdigit():\n",
    "                returned_pmcid = \"PMC\" + pmc_candidate_id_num\n",
    "            else:\n",
    "                logger.warning(f\"PMC ID ELink ? PMID {pmid}: Non-standard ID '{pmc_candidate_id_num}' from '{linkname_to_try}'.\")\n",
    "                return None \n",
    "            \n",
    "            logger.info(f\"PMC ID ELink ✓ PMID {pmid}: Found PMCID {returned_pmcid} via '{linkname_to_try}'.\")\n",
    "            return returned_pmcid\n",
    "        else:\n",
    "            info_el = link_set_db_el.find(\"./Info\") \n",
    "            if info_el is not None and info_el.text:\n",
    "                logger.info(f\"PMC ID ELink ~ PMID {pmid}: NCBI Info for '{linkname_to_try}': {info_el.text.strip()}\")\n",
    "            else:\n",
    "                logger.info(f\"PMC ID ELink ~ PMID {pmid}: No PMCID <Id> elements for '{linkname_to_try}'.\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e_req:\n",
    "        logger.warning(f\"PMC ID ELink ✗ PMID {pmid} ('{linkname_to_try}'): RequestException: {e_req}\")\n",
    "    except ET.ParseError as e_xml:\n",
    "        response_text_snippet = r.text[:250] if 'r' in locals() and hasattr(r, 'text') else \"N/A\"\n",
    "        logger.warning(f\"PMC ID ELink ✗ PMID {pmid} ('{linkname_to_try}'): XML ParseError: {e_xml}. Content: {response_text_snippet}\")\n",
    "    except Exception as e_generic:\n",
    "        logger.error(f\"PMC ID ELink ✗ PMID {pmid} ('{linkname_to_try}'): Unexpected error: {e_generic}\", exc_info=True)\n",
    "    \n",
    "    logger.warning(f\"PMC ID ELink ✗ PMID {pmid}: No PMCID from '{linkname_to_try}' after full attempt.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def pmc_attempt_download(pmcid: str, pmid: str, md: dict) -> bool:\n",
    "    \"\"\"Attempts to download a PDF from PMC, handling PoW.\"\"\"\n",
    "    year_val = md.get('year', 'UnknownYear')\n",
    "    author_val = md.get('author', 'UnknownAuthor')\n",
    "    title_val = md.get('title', f'NoTitle_{pmid}')\n",
    "    fname_base = sanitize_filename(f\"{year_val}-{pmid}-{author_val}-{title_val}\")\n",
    "    output_pdf_path  = os.path.join(OUTPUT_PDF_DIR, fname_base + \".pdf\")\n",
    "\n",
    "    if os.path.exists(output_pdf_path): \n",
    "        logger.info(f\"PMC ✓ {pmid} ({pmcid}): PDF already exists at {output_pdf_path} (checked in pmc_attempt_download).\")\n",
    "        return True\n",
    "\n",
    "    pmc_article_pdf_landing_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/\"\n",
    "    final_pdf_url_from_pmc = None \n",
    "    \n",
    "    logger.info(f\"PMC HEAD → PMID {pmid} ({pmcid}): Probing {pmc_article_pdf_landing_url} for final PDF URL.\")\n",
    "    head_req_headers = session_oa.headers.copy()\n",
    "    head_req_headers.update(BROWSER_LIKE_HEADERS) \n",
    "    head_req_headers['Accept'] = 'application/pdf, text/html;q=0.9, */*;q=0.8' \n",
    "    head_req_headers['Sec-Fetch-Site'] = 'cross-site' \n",
    "\n",
    "    try:\n",
    "        head_resp = session_oa.head(\n",
    "            pmc_article_pdf_landing_url,\n",
    "            headers=head_req_headers,\n",
    "            timeout=(10, 25), \n",
    "            allow_redirects=True \n",
    "        )\n",
    "        head_resp.raise_for_status()\n",
    "        final_pdf_url_from_pmc = head_resp.url \n",
    "        \n",
    "        if not (final_pdf_url_from_pmc.lower().endswith(\".pdf\") or \"format=pdf\" in final_pdf_url_from_pmc.lower() or \"/pdf/\" in final_pdf_url_from_pmc.lower()):\n",
    "            logger.warning(f\"PMC HEAD ? PMID {pmid} ({pmcid}): Resolved URL {final_pdf_url_from_pmc} doesn't strongly indicate PDF. Proceeding cautiously.\")\n",
    "        else:\n",
    "            logger.info(f\"PMC HEAD ✓ PMID {pmid} ({pmcid}): Resolved potential PDF URL: {final_pdf_url_from_pmc}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.warning(f\"PMC HEAD ✗ PMID {pmid} ({pmcid}) for {pmc_article_pdf_landing_url}: {e}. Will attempt GET on landing URL.\")\n",
    "        final_pdf_url_from_pmc = pmc_article_pdf_landing_url \n",
    "    except Exception as e_head_generic:\n",
    "        logger.error(f\"PMC HEAD ✗ PMID {pmid} ({pmcid}): Unexpected error during HEAD request: {e_head_generic}\", exc_info=True)\n",
    "        return False \n",
    "\n",
    "    if final_pdf_url_from_pmc:\n",
    "        # Pass md (article_metadata) to download_and_save_pdf\n",
    "        return download_and_save_pdf(\n",
    "            session_oa, \n",
    "            final_pdf_url_from_pmc, \n",
    "            output_pdf_path, \n",
    "            pmid, \n",
    "            source_name=f\"PMC({pmcid})\", \n",
    "            article_metadata=md, # Passing md here\n",
    "            referer=pmc_article_pdf_landing_url \n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"PMC ✗ PMID {pmid} ({pmcid}): No URL determined for download attempt after HEAD request.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def oa_worker(pmid: str, md: dict) -> tuple[str, bool]:\n",
    "    \"\"\"Worker for Open Access PDF fetching (Unpaywall then PMC).\"\"\"\n",
    "    try: \n",
    "        doi = md.get('doi')\n",
    "        year_val = md.get('year', 'UnknownYear')\n",
    "        author_val = md.get('author', 'UnknownAuthor')\n",
    "        title_val = md.get('title', f'NoTitle_{pmid}') \n",
    "\n",
    "        fname_base = sanitize_filename(f\"{year_val}-{pmid}-{author_val}-{title_val}\")\n",
    "        output_pdf_path  = os.path.join(OUTPUT_PDF_DIR, fname_base + \".pdf\")\n",
    "\n",
    "        if os.path.exists(output_pdf_path): \n",
    "            logger.info(f\"OA ✓ {pmid}: PDF already exists at {output_pdf_path}\")\n",
    "            return pmid, True\n",
    "\n",
    "        if doi:\n",
    "            unpaywall_url = unpaywall_get_pdf_url(doi) \n",
    "            if unpaywall_url:\n",
    "                referer_unpaywall = f\"https://doi.org/{quote_plus(doi)}\"\n",
    "                # Pass md (article_metadata) to download_and_save_pdf\n",
    "                if download_and_save_pdf(session_oa, unpaywall_url, output_pdf_path, pmid, source_name=f\"Unpaywall(DOI:{doi})\", article_metadata=md, referer=referer_unpaywall):\n",
    "                    return pmid, True\n",
    "                else:\n",
    "                    logger.info(f\"OA: Unpaywall attempt for PMID {pmid} (DOI {doi}) failed download/validation. Trying PMC.\")\n",
    "            else:\n",
    "                logger.info(f\"OA: No PDF URL from Unpaywall for PMID {pmid} (DOI {doi}). Trying PMC.\")\n",
    "        else:\n",
    "            logger.info(f\"OA: No DOI for PMID {pmid}. Skipping Unpaywall, trying PMC.\")\n",
    "        \n",
    "        pmcid = pmc_id_for_pmid(pmid, md) \n",
    "        if pmcid:\n",
    "            if pmc_attempt_download(pmcid, pmid, md):\n",
    "                return pmid, True\n",
    "        \n",
    "        logger.warning(f\"OA ✗ {pmid}: No PDF found via Unpaywall or PMC.\")\n",
    "        return pmid, False\n",
    "    \n",
    "    except Exception as e_oa_worker: \n",
    "        logger.error(f\"OA Worker UNHANDLED EXCEPTION for PMID {pmid}: {e_oa_worker}\", exc_info=True)\n",
    "        return pmid, False \n",
    "\n",
    "# === STEP 3: SCI-HUB ===\n",
    "def test_scihub_domain(domain: str) -> bool:\n",
    "    \"\"\"Tests if a Sci-Hub domain is responsive.\"\"\"\n",
    "    test_doi = \"10.1000/182\" \n",
    "    url = f\"{domain.rstrip('/')}/{test_doi}\"\n",
    "    logger.debug(f\"Sci-Hub TEST GET → {url}\")\n",
    "    try:\n",
    "        r = session_scihub.get(url, timeout=10, headers=BROWSER_LIKE_HEADERS) \n",
    "        if r.status_code == 200 and ('html' in r.headers.get('Content-Type','').lower() or \\\n",
    "                                     any(kw in r.text.lower() for kw in ['sci-hub', 'save', 'download', '<button id=\"download\">'])):\n",
    "            logger.info(f\"Sci-Hub TEST ✓ {domain} is responsive (status {r.status_code}).\")\n",
    "            return True\n",
    "        elif r.status_code == 404: \n",
    "             logger.info(f\"Sci-Hub TEST ✓ {domain} is responsive (status 404, expected for non-existent test DOI).\")\n",
    "             return True\n",
    "        else:\n",
    "            logger.warning(f\"Sci-Hub TEST ? {domain} responded status {r.status_code}, CT: {r.headers.get('Content-Type','')}. Text snippet: {r.text[:100]}\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.warning(f\"Sci-Hub TEST ✗ {domain} timed out.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.warning(f\"Sci-Hub TEST ✗ {domain} error: {e}\")\n",
    "    except Exception as e_generic_test: \n",
    "        logger.error(f\"Sci-Hub TEST ✗ {domain} unexpected error: {e_generic_test}\", exc_info=True)\n",
    "    return False\n",
    "\n",
    "def init_scihub_domains() -> list[str]:\n",
    "    \"\"\"Probes Sci-Hub domains for availability and returns a list of working ones.\"\"\"\n",
    "    logger.info(\"Probing Sci-Hub mirrors for availability...\")\n",
    "    working_domains = []\n",
    "    with ThreadPoolExecutor(max_workers=min(len(SCI_HUB_DOMAINS), 3), thread_name_prefix=\"SciHub_Domain_Test\") as executor:\n",
    "        future_to_domain = {executor.submit(test_scihub_domain, d): d for d in SCI_HUB_DOMAINS}\n",
    "        for future in as_completed(future_to_domain):\n",
    "            domain = future_to_domain[future]\n",
    "            try:\n",
    "                if future.result(): \n",
    "                    working_domains.append(domain)\n",
    "            except Exception as exc: \n",
    "                logger.error(f\"Sci-Hub domain test for {domain} generated an exception during result retrieval: {exc}\")\n",
    "    \n",
    "    if not working_domains:\n",
    "        logger.error(\"CRITICAL: No working Sci-Hub domains found after testing!\")\n",
    "    else:\n",
    "        if \"https://sci-hub.se\" in working_domains:\n",
    "            working_domains.insert(0, working_domains.pop(working_domains.index(\"https://sci-hub.se\")))\n",
    "        logger.info(f\"Using Sci-Hub domains: {working_domains}\")\n",
    "    return working_domains\n",
    "\n",
    "\n",
    "def find_scihub_pdf_in_html(html_content: bytes, base_page_url: str) -> str | None:\n",
    "    \"\"\"Parses Sci-Hub HTML content to find the direct PDF link.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    parsed_base_url = urlparse(base_page_url)\n",
    "    absolute_base = f\"{parsed_base_url.scheme}://{parsed_base_url.netloc}\"\n",
    "\n",
    "    selectors_and_attrs = [\n",
    "        ('iframe#pdf', 'src'),             \n",
    "        ('iframe#article', 'src'),         \n",
    "        ('embed[type=\"application/pdf\"]', 'src'), \n",
    "        ('iframe[src*=\".pdf\"]', 'src'),    \n",
    "        ('a#download', 'href'),            \n",
    "        ('div.buttons > a[href*=\".pdf\"]', 'href'), \n",
    "        ('div#buttons > a[href*=\".pdf\"]', 'href'), \n",
    "        ('a[href*=\".pdf\"]', 'href')        \n",
    "    ]\n",
    "\n",
    "    for selector, attr in selectors_and_attrs:\n",
    "        element = soup.select_one(selector)\n",
    "        if element and element.get(attr):\n",
    "            src_val = element.get(attr)\n",
    "            if src_val.startswith(\"//\"): \n",
    "                src_val = f\"{parsed_base_url.scheme}:{src_val}\"\n",
    "            \n",
    "            if not src_val.lower().startswith(('data:', 'javascript:')):\n",
    "                resolved_url = urljoin(absolute_base, src_val) \n",
    "                if \".pdf\" in resolved_url.lower() or any(sh_domain_base in resolved_url for sh_domain_base in [urlparse(d).netloc for d in SCI_HUB_DOMAINS]):\n",
    "                    logger.debug(f\"Sci-Hub HTML Parse: Found PDF link '{resolved_url}' using selector '{selector}'\")\n",
    "                    return resolved_url\n",
    "\n",
    "    onclick_buttons = soup.select('button[onclick*=\"location.href\"], a[onclick*=\"location.href\"]')\n",
    "    for button in onclick_buttons:\n",
    "        onclick_val = button.get('onclick', '')\n",
    "        match = re.search(r\"location\\.href\\s*=\\s*['\\\"]([^'\\\"]+\\.pdf[^'\\\"]*)['\\\"]\", onclick_val, re.IGNORECASE)\n",
    "        if match:\n",
    "            href = match.group(1).strip()\n",
    "            if href.startswith(\"//\"): href = f\"{parsed_base_url.scheme}:{href}\"\n",
    "            resolved_url = urljoin(absolute_base, href)\n",
    "            logger.debug(f\"Sci-Hub HTML Parse: Found PDF link '{resolved_url}' from onclick attribute.\")\n",
    "            return resolved_url\n",
    "            \n",
    "    logger.debug(f\"Sci-Hub HTML Parse: No obvious PDF link found in HTML from {base_page_url}\")\n",
    "    return None\n",
    "\n",
    "def scihub_worker(identifier: str, pmid: str, md: dict, active_domains: list) -> tuple[str, bool]:\n",
    "    \"\"\"Worker for Sci-Hub PDF fetching.\"\"\"\n",
    "    year_val = md.get('year', 'UnknownYear')\n",
    "    author_val = md.get('author', 'UnknownAuthor')\n",
    "    title_val = md.get('title', f'NoTitle_{pmid}')\n",
    "    fname_base = sanitize_filename(f\"{year_val}-{pmid}-{author_val}-{title_val}\")\n",
    "    output_pdf_path = os.path.join(OUTPUT_PDF_DIR, fname_base + \".pdf\")\n",
    "\n",
    "    if os.path.exists(output_pdf_path): \n",
    "        logger.info(f\"Sci-Hub ✓ {pmid}: PDF already exists at {output_pdf_path} (checked in scihub_worker).\")\n",
    "        return pmid, True\n",
    "    \n",
    "    if not active_domains: \n",
    "        logger.error(f\"Sci-Hub ✗ {pmid}: No active Sci-Hub domains to try for identifier '{identifier}'.\")\n",
    "        return pmid, False\n",
    "\n",
    "    for i, domain_url in enumerate(active_domains): \n",
    "        scihub_page_url = f\"{domain_url.rstrip('/')}/{quote_plus(identifier)}\"\n",
    "        logger.info(f\"Sci-Hub HTML GET → PMID {pmid} from {scihub_page_url} (Attempt {i+1}/{len(active_domains)})\")\n",
    "        \n",
    "        sh_headers = session_scihub.headers.copy() \n",
    "        sh_headers.update(BROWSER_LIKE_HEADERS)    \n",
    "        sh_headers['Sec-Fetch-Site'] = 'none'      \n",
    "\n",
    "        try:\n",
    "            r_page = session_scihub.get(scihub_page_url, headers=sh_headers, timeout=30) \n",
    "            r_page.raise_for_status() \n",
    "            page_content_type = r_page.headers.get('Content-Type','').lower()\n",
    "\n",
    "            if 'application/pdf' in page_content_type:\n",
    "                logger.info(f\"Sci-Hub ? PMID {pmid}: URL {scihub_page_url} served PDF directly. Attempting download...\")\n",
    "                # Pass md (article_metadata) to download_and_save_pdf\n",
    "                if download_and_save_pdf(session_scihub, r_page.url, output_pdf_path, pmid, source_name=f\"SciHub_Direct({domain_url})\", article_metadata=md, referer=domain_url):\n",
    "                    return pmid, True\n",
    "                else:\n",
    "                    logger.warning(f\"Sci-Hub ✗ PMID {pmid}: Direct PDF from {scihub_page_url} failed validation or download.\")\n",
    "                    continue \n",
    "\n",
    "            elif 'html' in page_content_type or r_page.content[:100].strip().lower().startswith((b'<!doctype html', b'<html')):\n",
    "                pdf_url_from_html = find_scihub_pdf_in_html(r_page.content, r_page.url) \n",
    "                \n",
    "                if pdf_url_from_html:\n",
    "                    logger.info(f\"Sci-Hub HTML ✓ PMID {pmid}: Found potential PDF link: {pdf_url_from_html.split('?')[0]}...\")\n",
    "                    # Pass md (article_metadata) to download_and_save_pdf\n",
    "                    if download_and_save_pdf(session_scihub, pdf_url_from_html, output_pdf_path, pmid, source_name=f\"SciHub_Extracted({domain_url})\", article_metadata=md, referer=scihub_page_url):\n",
    "                        return pmid, True\n",
    "                else:\n",
    "                    logger.warning(f\"Sci-Hub HTML ✗ {pmid} via {domain_url}: No PDF link found within HTML from {scihub_page_url}\")\n",
    "            else: \n",
    "                logger.warning(f\"Sci-Hub ✗ {pmid} via {domain_url}: Unexpected Content-Type '{page_content_type}' from {scihub_page_url}. Snippet: {r_page.text[:100]}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e_req:\n",
    "            logger.warning(f\"Sci-Hub ✗ {pmid} via {domain_url} ({scihub_page_url}): RequestException: {e_req}\")\n",
    "        except Exception as e_sh_domain: \n",
    "            logger.error(f\"Sci-Hub ✗ {pmid} via {domain_url} ({scihub_page_url}): General error {e_sh_domain.__class__.__name__}: {e_sh_domain}\", exc_info=True)\n",
    "        \n",
    "        if len(active_domains) > 1 and i < len(active_domains) - 1: \n",
    "            time.sleep(DELAY_SCIHUB) \n",
    "\n",
    "    logger.error(f\"Sci-Hub ✗ {pmid}: Failed for identifier '{identifier}' after trying all active domains.\")\n",
    "    return pmid, False\n",
    "\n",
    "\n",
    "# === MAIN SCRIPT EXECUTION ===\n",
    "def main():\n",
    "    t_start = time.time()\n",
    "    logger.info(f\"=== PDF Fetcher v12-pow started at {time.strftime('%Y-%m-%d %H:%M:%S')} ===\")\n",
    "\n",
    "    # Optional File Logging Setup\n",
    "    log_file_handler_path = os.path.join(OUTPUT_PDF_DIR, \"pdf_fetcher_v12_run.log\")\n",
    "    try:\n",
    "        os.makedirs(OUTPUT_PDF_DIR, exist_ok=True) \n",
    "        # fh = logging.FileHandler(log_file_handler_path, mode='a') \n",
    "        # fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - [%(threadName)s] - %(funcName)s - %(message)s'))\n",
    "        # logger.addHandler(fh) # Uncomment to enable file logging\n",
    "        # logger.info(f\"Detailed run logging to file: {os.path.abspath(log_file_handler_path)}\")\n",
    "    except Exception as e_log_file:\n",
    "        logger.error(f\"Could not set up file logging to {log_file_handler_path}: {e_log_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(EXCEL_FILE_PATH)\n",
    "        if 'PMID' not in df.columns:\n",
    "            logger.error(f\"Excel file {EXCEL_FILE_PATH} must contain a 'PMID' column.\")\n",
    "            return\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Excel file not found: {EXCEL_FILE_PATH}\")\n",
    "        return\n",
    "    except Exception as e_excel: \n",
    "        logger.error(f\"Cannot read Excel file {EXCEL_FILE_PATH}: {e_excel}\", exc_info=True)\n",
    "        return\n",
    "    \n",
    "    pmids_raw = df['PMID'].dropna().unique()\n",
    "    pmids = []\n",
    "    for p_raw in pmids_raw:\n",
    "        try:\n",
    "            pmids.append(str(int(float(str(p_raw))))) \n",
    "        except ValueError:\n",
    "            logger.warning(f\"Skipping invalid PMID format in Excel: '{p_raw}'\")\n",
    "    \n",
    "    if not pmids:\n",
    "        logger.error(\"No valid PMIDs found in the Excel file.\")\n",
    "        return\n",
    "    logger.info(f\"Loaded {len(pmids)} unique, valid PMIDs from {EXCEL_FILE_PATH}\")\n",
    "\n",
    "    metadata_dict = fetch_metadata(pmids)\n",
    "    valid_pmids_with_meta = [p for p in pmids if p in metadata_dict and metadata_dict[p]]\n",
    "    \n",
    "    logger.info(f\"Successfully fetched metadata for {len(valid_pmids_with_meta)} PMIDs.\")\n",
    "    if not valid_pmids_with_meta:\n",
    "        logger.error(\"No metadata could be fetched for any valid PMIDs. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        os.makedirs(OUTPUT_PDF_DIR, exist_ok=True)\n",
    "        os.makedirs(os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR), exist_ok=True)\n",
    "        logger.info(f\"PDFs will be saved to: {os.path.abspath(OUTPUT_PDF_DIR)}\")\n",
    "        logger.info(f\"Suspicious/failed validation files will be in: {os.path.abspath(os.path.join(OUTPUT_PDF_DIR, SUSPICIOUS_PDF_SUBDIR))}\")\n",
    "    except OSError as e_mkdir:\n",
    "        logger.error(f\"Could not create output directories: {e_mkdir}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"--- Starting Open Access Download Phase ---\")\n",
    "    oa_succeeded_pmids, oa_failed_pmids = [], []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREADS, thread_name_prefix=\"OA_Worker\") as executor:\n",
    "        future_to_pmid_oa = {\n",
    "            executor.submit(oa_worker, pmid, metadata_dict[pmid]): pmid\n",
    "            for pmid in valid_pmids_with_meta \n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_pmid_oa):\n",
    "            pmid_processed = future_to_pmid_oa[future]\n",
    "            try:\n",
    "                worker_result = future.result() \n",
    "                \n",
    "                if worker_result is None: \n",
    "                    logger.error(f\"OA Thread ✗ For PMID {pmid_processed}, future.result() was None. Treating as failure.\")\n",
    "                    oa_failed_pmids.append(pmid_processed)\n",
    "                elif isinstance(worker_result, tuple) and len(worker_result) == 2:\n",
    "                    returned_pmid, success_status = worker_result\n",
    "                    if returned_pmid != pmid_processed:\n",
    "                         logger.warning(f\"OA Thread ? Mismatch in returned PMID: expected {pmid_processed}, got {returned_pmid}. Processing with {pmid_processed}.\")\n",
    "                    \n",
    "                    if success_status:\n",
    "                        oa_succeeded_pmids.append(pmid_processed)\n",
    "                    else:\n",
    "                        oa_failed_pmids.append(pmid_processed)\n",
    "                else: \n",
    "                    logger.error(f\"OA Thread ✗ For PMID {pmid_processed}, worker returned unexpected result: {worker_result}. Treating as failure.\")\n",
    "                    oa_failed_pmids.append(pmid_processed)\n",
    "\n",
    "            except Exception as e_thread: \n",
    "                logger.error(f\"OA Thread ✗ Exception processing PMID {pmid_processed}: {e_thread}\", exc_info=True)\n",
    "                oa_failed_pmids.append(pmid_processed) \n",
    "    \n",
    "    logger.info(f\"Open Access Phase Summary: {len(oa_succeeded_pmids)} PDFs successfully downloaded and validated.\")\n",
    "    if oa_failed_pmids:\n",
    "        logger.info(f\"{len(oa_failed_pmids)} PMIDs not fetched via OA or failed validation: {sorted(oa_failed_pmids)[:10]}...\")\n",
    "\n",
    "    sci_hub_succeeded_pmids = []\n",
    "    # Initialize final_still_failed_pmids with PMIDs that failed OA phase.\n",
    "    # This list will be further reduced if Sci-Hub succeeds for any of them.\n",
    "    pmids_for_scihub_attempt = list(oa_failed_pmids) \n",
    "    final_still_failed_pmids = [] # PMIDs that fail both OA and Sci-Hub attempts\n",
    "\n",
    "    if pmids_for_scihub_attempt: \n",
    "        logger.info(\"--- Starting Sci-Hub Download Phase for Remaining PMIDs ---\")\n",
    "        active_scihub_domains = init_scihub_domains()\n",
    "        \n",
    "        if not active_scihub_domains:\n",
    "            logger.error(\"Sci-Hub phase skipped: No active Sci-Hub domains found.\")\n",
    "            final_still_failed_pmids = list(pmids_for_scihub_attempt) # All OA failures are now final failures\n",
    "        else:\n",
    "            scihub_max_workers = max(1, MAX_THREADS // 2 if MAX_THREADS > 1 else 1)\n",
    "            with ThreadPoolExecutor(max_workers=scihub_max_workers, thread_name_prefix=\"SciHub_Worker\") as executor_sh:\n",
    "                future_to_pmid_scihub = {}\n",
    "                for pmid_to_try_scihub in pmids_for_scihub_attempt:\n",
    "                    if pmid_to_try_scihub not in metadata_dict: \n",
    "                        logger.warning(f\"Sci-Hub: Metadata missing for PMID {pmid_to_try_scihub}, skipping.\")\n",
    "                        final_still_failed_pmids.append(pmid_to_try_scihub)\n",
    "                        continue\n",
    "                        \n",
    "                    meta_for_pmid = metadata_dict[pmid_to_try_scihub]\n",
    "                    identifier_for_scihub = meta_for_pmid.get('doi') if meta_for_pmid.get('doi') else pmid_to_try_scihub\n",
    "                    \n",
    "                    future_to_pmid_scihub[executor_sh.submit(\n",
    "                        scihub_worker, \n",
    "                        identifier_for_scihub, \n",
    "                        pmid_to_try_scihub, \n",
    "                        meta_for_pmid, \n",
    "                        active_scihub_domains\n",
    "                    )] = pmid_to_try_scihub\n",
    "                \n",
    "                for future_sh in as_completed(future_to_pmid_scihub):\n",
    "                    pmid_processed_scihub = future_to_pmid_scihub[future_sh]\n",
    "                    try:\n",
    "                        sh_worker_result = future_sh.result()\n",
    "                        if sh_worker_result is None:\n",
    "                            logger.error(f\"Sci-Hub Thread ✗ For PMID {pmid_processed_scihub}, future.result() was None. Treating as failure.\")\n",
    "                            final_still_failed_pmids.append(pmid_processed_scihub)\n",
    "                        elif isinstance(sh_worker_result, tuple) and len(sh_worker_result) == 2:\n",
    "                            _, success_status_scihub = sh_worker_result\n",
    "                            if success_status_scihub:\n",
    "                                sci_hub_succeeded_pmids.append(pmid_processed_scihub)\n",
    "                            else:\n",
    "                                final_still_failed_pmids.append(pmid_processed_scihub)\n",
    "                        else:\n",
    "                            logger.error(f\"Sci-Hub Thread ✗ For PMID {pmid_processed_scihub}, worker returned unexpected result: {sh_worker_result}. Treating as failure.\")\n",
    "                            final_still_failed_pmids.append(pmid_processed_scihub)\n",
    "                    except Exception as e_sh_thread:\n",
    "                        logger.error(f\"Sci-Hub Thread ✗ Exception processing PMID {pmid_processed_scihub}: {e_sh_thread}\", exc_info=True)\n",
    "                        final_still_failed_pmids.append(pmid_processed_scihub)\n",
    "\n",
    "            logger.info(f\"Sci-Hub Phase Summary: {len(sci_hub_succeeded_pmids)} PDFs successfully downloaded and validated.\")\n",
    "            # final_still_failed_pmids now contains those that failed both OA and SciHub\n",
    "            if final_still_failed_pmids:\n",
    "                 logger.info(f\"{len(final_still_failed_pmids)} PMIDs still missing after Sci-Hub attempts or validation: {sorted(final_still_failed_pmids)[:10]}...\")\n",
    "    else: \n",
    "        logger.info(\"--- Sci-Hub Download Phase Skipped: No PMIDs failed the Open Access phase. ---\")\n",
    "        final_still_failed_pmids = [] # No OA failures means no final failures if SciHub isn't run\n",
    "\n",
    "    total_succeeded = len(oa_succeeded_pmids) + len(sci_hub_succeeded_pmids)\n",
    "    total_time_taken = time.time() - t_start\n",
    "    logger.info(\"--- Overall Summary ---\")\n",
    "    logger.info(f\"Processed {len(pmids)} unique input PMIDs.\")\n",
    "    logger.info(f\"Attempted downloads for {len(valid_pmids_with_meta)} PMIDs (those with metadata).\")\n",
    "    logger.info(f\"Total PDFs successfully downloaded & validated: {total_succeeded} / {len(valid_pmids_with_meta)}.\")\n",
    "    logger.info(f\"  - Via Open Access (Unpaywall/PMC): {len(oa_succeeded_pmids)}\")\n",
    "    logger.info(f\"  - Via Sci-Hub: {len(sci_hub_succeeded_pmids)}\")\n",
    "    \n",
    "    all_attempted_pmids = set(valid_pmids_with_meta)\n",
    "    all_succeeded_pmids = set(oa_succeeded_pmids) | set(sci_hub_succeeded_pmids)\n",
    "    # PMIDs that were attempted but are not in any succeeded list are the true failures\n",
    "    final_truly_failed_pmids_for_log = sorted(list(all_attempted_pmids - all_succeeded_pmids))\n",
    "\n",
    "    if final_truly_failed_pmids_for_log:\n",
    "        logger.info(f\"Total PMIDs ultimately NOT downloaded or failed validation: {len(final_truly_failed_pmids_for_log)}\")\n",
    "        \n",
    "        failed_log_filename = \"failed_articles_details.log\" \n",
    "        logger.info(f\"Logging details of {len(final_truly_failed_pmids_for_log)} failed PMIDs to {os.path.join(OUTPUT_PDF_DIR, failed_log_filename)}\")\n",
    "        for pmid_fail in final_truly_failed_pmids_for_log:\n",
    "            # Ensure we use the global set for this log to avoid re-logging if main is somehow run multiple times on same set.\n",
    "            log_enhanced_failure_details(\n",
    "                failed_log_filename,\n",
    "                pmid_fail,\n",
    "                metadata_dict.get(pmid_fail, {}), # Use .get for safety\n",
    "                \"Full-text Retrieval Failed\",\n",
    "                details=\"Failed all download attempts (Open Access and Sci-Hub, or PDF validation failed for all sources).\",\n",
    "                logged_set=LOGGED_FAILED_PMIDS \n",
    "            )\n",
    "    else: # No truly failed PMIDs\n",
    "        if total_succeeded == len(valid_pmids_with_meta) and len(valid_pmids_with_meta) > 0:\n",
    "             logger.info(\"All requested PDFs (with metadata) successfully downloaded and validated!\")\n",
    "        elif len(valid_pmids_with_meta) == 0 : \n",
    "             logger.info(\"No PMIDs with metadata were available to attempt download.\")\n",
    "        # else case: total_succeeded < valid_pmids_with_meta but final_truly_failed_pmids_for_log is empty.\n",
    "        # This would be an anomaly in logic, but the calculation for final_truly_failed_pmids_for_log should cover it.\n",
    "        # If it occurs, it means some PMIDs were neither succeeded nor marked as failed - needs investigation.\n",
    "        elif total_succeeded < len(valid_pmids_with_meta):\n",
    "            logger.warning(\"Processing complete, but count of succeeded PDFs is less than attempted PMIDs with metadata, yet no specific PMIDs were flagged as finally failed. Please review logs for anomalies.\")\n",
    "\n",
    "\n",
    "    logger.info(f\"Total execution time: {total_time_taken:.2f} seconds.\")\n",
    "    logger.info(f\"=== PDF Fetcher v12-pow completed at {time.strftime('%Y-%m-%d %H:%M:%S')} ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize global sets at the start of a run if script is run multiple times in same session (e.g. Jupyter)\n",
    "    LOGGED_SUSPICIOUS_PMIDS.clear()\n",
    "    LOGGED_FAILED_PMIDS.clear()\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (litscape)",
   "language": "python",
   "name": "litscape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
