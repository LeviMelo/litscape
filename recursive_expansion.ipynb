{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44be61c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed PMIDs: ['39489669']\n"
     ]
    },
    {
     "ename": "NetworkXError",
     "evalue": "The edge 28713211-20626510 not in graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litscape\\Lib\\site-packages\\networkx\\classes\\digraph.py:848\u001b[39m, in \u001b[36mDiGraph.remove_edge\u001b[39m\u001b[34m(self, u, v)\u001b[39m\n\u001b[32m    847\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_succ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pred[v][u]\n",
      "\u001b[31mKeyError\u001b[39m: '20626510'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNetworkXError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 220\u001b[39m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSet SEED_DOIS or SEED_QUERY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    219\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSeed PMIDs:\u001b[39m\u001b[33m\"\u001b[39m, seed_pmids)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m G = \u001b[43mrecursive_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_pmids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Plot with matplotlib\u001b[39;00m\n\u001b[32m    223\u001b[39m pos = nx.spring_layout(G, k=\u001b[32m0.5\u001b[39m, iterations=\u001b[32m50\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 204\u001b[39m, in \u001b[36mrecursive_expand\u001b[39m\u001b[34m(pmids)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# chronology & cycle enforcement\u001b[39;00m\n\u001b[32m    203\u001b[39m     removed = remove_time_invalid_edges(G)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[43menforce_acyclic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     current = next_gen\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36menforce_acyclic\u001b[39m\u001b[34m(G)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nx.is_directed_acyclic_graph(G):\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m cycle \u001b[38;5;129;01min\u001b[39;00m nx.simple_cycles(G):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# remove one edge per cycle\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[43mG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litscape\\Lib\\site-packages\\networkx\\classes\\digraph.py:851\u001b[39m, in \u001b[36mDiGraph.remove_edge\u001b[39m\u001b[34m(self, u, v)\u001b[39m\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pred[v][u]\n\u001b[32m    850\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NetworkXError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe edge \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mu\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in graph.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    852\u001b[39m nx._clear_cache(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mNetworkXError\u001b[39m: The edge 28713211-20626510 not in graph."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# =============================================================================\n",
    "# SCRIPT: recursive_dag_expansion.py\n",
    "# =============================================================================\n",
    "# Purpose:\n",
    "#   - Start from a PubMed query or list of DOIs.\n",
    "#   - Recursively expand references (“parents”) and citations (“children”) up to N generations.\n",
    "#   - Use the V14 pipeline for:\n",
    "#       • ESearch → PMIDs (with WebEnv/QueryKey)\n",
    "#       • EFetch → dates, DOI, references\n",
    "#       • CrossRef augmentation for low‐ref articles\n",
    "#       • ELink (POST batching) → citation edges\n",
    "#       • Chronological filtering & DAG enforcement\n",
    "#   - Benchmark per‐generation node/edge growth and total runtime.\n",
    "#   - Produce an interactive network plot with nodes colored by generation.\n",
    "# =============================================================================\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "SEED_QUERY    = None\n",
    "SEED_DOIS     = [\"10.1053/j.jvca.2024.10.005\"]\n",
    "MAX_DEPTH     = 5\n",
    "SEARCH_MAX    = 100\n",
    "ELINK_BATCH   = 100\n",
    "AUGMENT_REFS        = True\n",
    "AUGMENT_THRESHOLD   = 5\n",
    "CROSSREF_FILTER_BATCH_SIZE = 50\n",
    "DOI_ES_BATCH        = 100\n",
    "SLEEP_SECONDS       = 0.1\n",
    "\n",
    "NCBI_TOOL     = \"recursive_dag_expansion\"\n",
    "NCBI_EMAIL    = \"levi4328@gmail.com\"\n",
    "NCBI_API_KEY  = \"44d5c1b49a9ed02ae1fc52fa9d01e148e009\"\n",
    "\n",
    "# --- Session Setup ---\n",
    "session = requests.Session()\n",
    "session_cr = requests.Session()\n",
    "\n",
    "def _ncbi_params(extra=None):\n",
    "    p = {\"tool\":NCBI_TOOL}\n",
    "    if NCBI_EMAIL:    p[\"email\"]   = NCBI_EMAIL\n",
    "    if NCBI_API_KEY:  p[\"api_key\"] = NCBI_API_KEY\n",
    "    if extra: p.update(extra)\n",
    "    return p\n",
    "\n",
    "# --- 1) ESearch for PMIDs or DOI→PMID via OR-query ---\n",
    "def esearch_pmids(term, retmax):\n",
    "    r = session.get(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\",\n",
    "                    params=_ncbi_params({\"db\":\"pubmed\",\"term\":term,\"retmax\":retmax,\"usehistory\":\"y\"}))\n",
    "    r.raise_for_status(); root = ET.fromstring(r.content)\n",
    "    ids = [e.text for e in root.findall(\".//IdList/Id\")]\n",
    "    we, qk = root.findtext(\".//WebEnv\"), root.findtext(\".//QueryKey\")\n",
    "    return ids, we, qk\n",
    "\n",
    "def dois_to_pmids(dois, batch_size=DOI_ES_BATCH):\n",
    "    doi2pm = {}\n",
    "    for i in range(0,len(dois),batch_size):\n",
    "        batch = dois[i:i+batch_size]\n",
    "        term  = \" OR \".join(f\"{d}[doi]\" for d in batch)\n",
    "        r = session.post(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\",\n",
    "                         data=_ncbi_params({\"db\":\"pubmed\",\"term\":term,\"retmode\":\"json\",\"retmax\":str(len(batch))}))\n",
    "        r.raise_for_status()\n",
    "        ids = r.json()[\"esearchresult\"].get(\"idlist\",[])\n",
    "        for d,p in zip(batch,ids):\n",
    "            doi2pm[d]=p\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "    return doi2pm\n",
    "\n",
    "# --- 2) EFetch: dates, refs, DOI ---\n",
    "def fetch_details(pmids, we=None, qk=None):\n",
    "    params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"retmax\":len(pmids)}\n",
    "    if we and qk:\n",
    "        params.update({\"WebEnv\":we,\"query_key\":qk})\n",
    "        r = session.post(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\",\n",
    "                         data=_ncbi_params(params))\n",
    "    else:\n",
    "        params[\"id\"] = \",\".join(pmids)\n",
    "        r = session.post(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\",\n",
    "                         data=_ncbi_params(params))\n",
    "    r.raise_for_status(); root = ET.fromstring(r.content)\n",
    "    MONTH = {m:i for i,m in enumerate([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],1)}\n",
    "    out = {}\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pm = art.findtext(\".//PMID\")\n",
    "        # date\n",
    "        ad = art.find(\".//ArticleDate\") or art.find(\".//JournalIssue/PubDate\")\n",
    "        dt = None\n",
    "        if ad is not None:\n",
    "            y,mo,da = ad.findtext(\"Year\"),ad.findtext(\"Month\"),ad.findtext(\"Day\")\n",
    "            if y and (mo or da):\n",
    "                mnum = int(mo) if mo and mo.isdigit() else MONTH.get(mo[:3],1)\n",
    "                dnum = int(da) if da and da.isdigit() else 1\n",
    "                try: dt = datetime.date(int(y),mnum,dnum)\n",
    "                except: pass\n",
    "        # refs\n",
    "        refs = [r.findtext(\".//ArticleId[@IdType='pubmed']\")\n",
    "                for r in art.findall(\".//ReferenceList/Reference\")\n",
    "                if r.findtext(\".//ArticleId[@IdType='pubmed']\")]\n",
    "        # doi\n",
    "        doi = art.findtext(\".//ArticleIdList/ArticleId[@IdType='doi']\")\n",
    "        out[pm] = {\"date\":dt, \"refs\":refs, \"doi\":doi}\n",
    "    return out\n",
    "\n",
    "# --- 3) CrossRef augment ---\n",
    "def crossref_refs(dois):\n",
    "    out = {}\n",
    "    for i in range(0,len(dois),CROSSREF_FILTER_BATCH_SIZE):\n",
    "        batch = dois[i:i+CROSSREF_FILTER_BATCH_SIZE]\n",
    "        fv    = \",\".join(f\"doi:{d}\" for d in batch)\n",
    "        r = session_cr.get(\"https://api.crossref.org/works\",\n",
    "                           params={\"filter\":fv,\"rows\":len(batch)},\n",
    "                           headers={\"User-Agent\":f\"{NCBI_TOOL}({NCBI_EMAIL})\"})\n",
    "        items = r.json().get(\"message\",{}).get(\"items\",[])\n",
    "        for itm in items:\n",
    "            doi0 = itm.get(\"DOI\")\n",
    "            refs = [ref.get(\"DOI\") for ref in itm.get(\"reference\",[]) if ref.get(\"DOI\")]\n",
    "            out[doi0] = refs\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "    return out\n",
    "\n",
    "# --- 4) ELink citations ---\n",
    "def fetch_citations(pmids):\n",
    "    out = {}\n",
    "    def batch_post(batch):\n",
    "        data = [(\"dbfrom\",\"pubmed\"),(\"linkname\",\"pubmed_pubmed_citedin\"),(\"cmd\",\"neighbor\")]\n",
    "        data += [(\"id\",p) for p in batch] + list(_ncbi_params().items())\n",
    "        r = session.post(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\", data=data)\n",
    "        r.raise_for_status(); return r.content\n",
    "    for i in range(0,len(pmids),ELINK_BATCH):\n",
    "        batch = pmids[i:i+ELINK_BATCH]\n",
    "        raw   = batch_post(batch)\n",
    "        root  = ET.fromstring(raw)\n",
    "        for ls in root.findall(\".//LinkSet\"):\n",
    "            src = ls.findtext(\".//IdList/Id\")\n",
    "            cits= [c.text for c in ls.findall(\".//LinkSetDb/Link/Id\") if c.text]\n",
    "            out[src] = cits\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "    return out\n",
    "\n",
    "# --- 5) Chronological & DAG checks ---\n",
    "def remove_time_invalid_edges(G):\n",
    "    to_remove=[]\n",
    "    for u,v in G.edges():\n",
    "        du, dv = G.nodes[u].get(\"date\"), G.nodes[v].get(\"date\")\n",
    "        if du and dv and du > dv:\n",
    "            to_remove.append((u,v))\n",
    "    G.remove_edges_from(to_remove)\n",
    "    return to_remove\n",
    "\n",
    "def enforce_acyclic(G):\n",
    "    if not nx.is_directed_acyclic_graph(G):\n",
    "        for cycle in nx.simple_cycles(G):\n",
    "            # remove one edge per cycle\n",
    "            G.remove_edge(cycle[-1], cycle[0])\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# --- Recursive Expansion ---\n",
    "def recursive_expand(pmids):\n",
    "    G = nx.DiGraph()\n",
    "    gen_map = {pm:0 for pm in pmids}\n",
    "    for pm in pmids: G.add_node(pm, date=None, generation=0)\n",
    "\n",
    "    current = pmids\n",
    "    for gen in range(1, MAX_DEPTH+1):\n",
    "        # fetch details + citing\n",
    "        details = fetch_details(current)\n",
    "        # CrossRef augment\n",
    "        if AUGMENT_REFS:\n",
    "            cands = [pm for pm,d in details.items() if len(d[\"refs\"])<AUGMENT_THRESHOLD and d[\"doi\"]]\n",
    "            cr   = crossref_refs([details[pm][\"doi\"] for pm in cands])\n",
    "            doi2pm = dois_to_pmids(sum(cr.values(), []))\n",
    "            for pm in cands:\n",
    "                mapped = [doi2pm.get(d) for d in cr.get(details[pm][\"doi\"],[])]\n",
    "                details[pm][\"refs\"].extend([m for m in mapped if m])\n",
    "\n",
    "        # parents\n",
    "        parents = set(sum((d[\"refs\"] for d in details.values()), []))\n",
    "        # citations\n",
    "        cits    = set(sum((fetch_citations(current).values()), []))\n",
    "\n",
    "        next_gen = list((parents|cits) - set(G.nodes()))\n",
    "        # add nodes + edges\n",
    "        for pm in next_gen:\n",
    "            G.add_node(pm, generation=gen, date=None)\n",
    "        # add edges\n",
    "        for src,info in details.items():\n",
    "            for r in info[\"refs\"]:\n",
    "                if G.has_node(src) and G.has_node(r):\n",
    "                    G.add_edge(r, src, provenance=\"reference\")\n",
    "        for src,clist in fetch_citations(current).items():\n",
    "            for c in clist:\n",
    "                if G.has_node(src) and G.has_node(c):\n",
    "                    G.add_edge(src, c, provenance=\"citation\")\n",
    "\n",
    "        # chronology & cycle enforcement\n",
    "        removed = remove_time_invalid_edges(G)\n",
    "        enforce_acyclic(G)\n",
    "\n",
    "        current = next_gen\n",
    "\n",
    "    return G\n",
    "\n",
    "# --- Main ---\n",
    "if __name__==\"__main__\":\n",
    "    if SEED_DOIS:\n",
    "        seed_pmids = list(dois_to_pmids(SEED_DOIS).values())\n",
    "    elif SEED_QUERY:\n",
    "        seed_pmids, we, qk = esearch_pmids(SEED_QUERY, SEARCH_MAX)\n",
    "    else:\n",
    "        raise ValueError(\"Set SEED_DOIS or SEED_QUERY\")\n",
    "\n",
    "    print(\"Seed PMIDs:\", seed_pmids)\n",
    "    G = recursive_expand(seed_pmids)\n",
    "\n",
    "    # Plot with matplotlib\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "    colors = [G.nodes[n][\"generation\"] for n in G.nodes()]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=colors, cmap=\"viridis\", node_size=50)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, arrows=False)\n",
    "    plt.title(\"Recursive Citation DAG (nodes by generation)\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ad80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛔ ESearch DOI→PMID batch failed: 400 Bad Request\n",
      "Raw response:\n",
      " {\"error\":\"API key invalid\",\"api-key\":\"YOUR_NCBI_KEY\",\"type\":\"invalid\",\n",
      "\"status\":\"unknown\"}\n",
      "⚠️ Batch #1 DOI→PMID error: 400 Client Error: Bad Request for url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\n",
      "   Mapped 0/1 DOIs → PMIDs\n",
      "Seed PMIDs: []\n",
      "\n",
      "=== DEPTH 1 (linear) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EFetch: 0it [00:00, ?it/s]\n",
      "ELink: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEPTH 2 (linear) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EFetch: 0it [00:00, ?it/s]\n",
      "ELink: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEPTH 3 (linear) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EFetch: 0it [00:00, ?it/s]\n",
      "ELink: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEPTH 4 (linear) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EFetch: 0it [00:00, ?it/s]\n",
      "ELink: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metrics ===\n",
      "   depth  new_nodes  cum_nodes  edges\n",
      "0      1          0          0      0\n",
      "1      2          0          0      0\n",
      "2      3          0          0      0\n",
      "3      4          0          0      0\n",
      "\n",
      "DAG check: ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 296\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[32m    295\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDAG check:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m✅\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nx.is_directed_acyclic_graph(G) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m❌ (cycles!)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[43mplot_generations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mEXPANSION_MODE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcapitalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m expansion (depth ≤ \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMAX_DEPTH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 271\u001b[39m, in \u001b[36mplot_generations\u001b[39m\u001b[34m(G, title)\u001b[39m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i,n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes):\n\u001b[32m    270\u001b[39m         pos[n]=(g, -i)          \u001b[38;5;66;03m# x=gen, y=stack\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m cmap=plt.cm.get_cmap(\u001b[33m\"\u001b[39m\u001b[33mviridis\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m)\u001b[49m+\u001b[32m1\u001b[39m)\n\u001b[32m    272\u001b[39m colors=[cmap[G.nodes[n][\u001b[33m'\u001b[39m\u001b[33mgen\u001b[39m\u001b[33m'\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m G.nodes()]\n\u001b[32m    273\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m,\u001b[32m6\u001b[39m))\n",
      "\u001b[31mValueError\u001b[39m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL: Recursive Citation Expansion DAG Builder V15\n",
    "# ============================================================\n",
    "# Purpose\n",
    "# -------\n",
    "# 1.  Bootstrap from either a PubMed query or a list of DOIs.\n",
    "# 2.  Build a *directed*, time‑consistent citation graph.\n",
    "# 3.  Recursively expand   P₁…Pₙ  (references / “parents”)\n",
    "#                        + C₁…Cₙ  (citations / “children”)\n",
    "#     in two modes:\n",
    "#       • linear  – chain:  Pₙ→⋯→P₁→Seed→C₁→⋯→Cₙ\n",
    "#       • global  – snow‑ball: expand parents **and** children of the *current*\n",
    "#                                 frontier at every depth.\n",
    "# 4.  Threshold filter: a new node is accepted only if it has ≥ T\n",
    "#     (absolute ‑or‑ relative %) links *into* the frontier that spawned it.\n",
    "# 5.  CrossRef‑based augmentation for low‑reference articles\n",
    "#     (same batching & OR‑ESearch DOI→PMID mapping as V14).\n",
    "# 6.  Strict DAG enforcement:\n",
    "#       • drop time‑inverted edges   (src_date > tgt_date)\n",
    "#       • after each layer, verify graph is acyclic; remove offending edges.\n",
    "# 7.  Benchmark & log per generation:\n",
    "#       nodes added, edges added, cumulative nodes, run‑time.\n",
    "# 8.  Visualise final graph:\n",
    "#       nodes coloured by generation, layers laid out left→right.\n",
    "#\n",
    "# Requirements\n",
    "# ------------\n",
    "#   pip install requests networkx matplotlib pandas tqdm lxml\n",
    "# ============================================================\n",
    "\n",
    "import requests, time, datetime, xml.etree.ElementTree as ET, networkx as nx\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import ChunkedEncodingError, RequestException\n",
    "from urllib3.exceptions import ProtocolError\n",
    "\n",
    "# ── User Config ──────────────────────────────────────────────\n",
    "SEED_QUERY      = None\n",
    "SEED_DOIS       = [\"10.1053/j.jvca.2024.10.005\"]\n",
    "MAX_DEPTH       = 4\n",
    "\n",
    "EXPANSION_MODE  = \"linear\"     # \"linear\" or \"global\"\n",
    "THRESHOLD_TYPE  = \"relative\"   # \"absolute\" or \"relative\"\n",
    "THRESHOLD_VAL   = 0.05         # ≥ count  (abs)   or ≥ p% (rel)\n",
    "\n",
    "AUGMENT_REFS          = True\n",
    "AUGMENT_THRESHOLD     = 3      # if < this many refs, augment via CrossRef\n",
    "CROSSREF_BATCH        = 50\n",
    "\n",
    "BATCH_SIZE            = 200    # for EFetch / ELink\n",
    "SLEEP_SECONDS         = 0.1\n",
    "NCBI_TOOL             = \"dag_checker_v15\"\n",
    "NCBI_EMAIL            = \"levi4328@gmail.com\"\n",
    "NCBI_API_KEY          = \"YOUR_NCBI_KEY\"  # optional\n",
    "\n",
    "# ── NCBI helpers ─────────────────────────────────────────────\n",
    "session = requests.Session()\n",
    "def ncbi(extra=None):\n",
    "    p = {\"tool\": NCBI_TOOL, \"email\": NCBI_EMAIL}\n",
    "    if NCBI_API_KEY: p[\"api_key\"] = NCBI_API_KEY\n",
    "    if extra: p.update(extra)\n",
    "    return p\n",
    "\n",
    "# ── Bootstrap functions ─────────────────────────────────────\n",
    "def esearch_to_pmids(query, retmax=100):\n",
    "    r = session.get(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\",\n",
    "                    params=ncbi({\"db\":\"pubmed\",\"term\":query,\n",
    "                                 \"retmax\":str(retmax),\"usehistory\":\"n\"}), timeout=30)\n",
    "    r.raise_for_status(); root = ET.fromstring(r.content)\n",
    "    return [e.text for e in root.findall(\".//IdList/Id\")]\n",
    "\n",
    "def doi_batch_esearch(dois, bsz=100):\n",
    "    \"\"\"\n",
    "    Batch-convert DOIs → PMIDs via OR-joined ESearch POST.\n",
    "    Wraps each DOI in quotes:  \"10.xxxx/yyy\"[doi].\n",
    "    \"\"\"\n",
    "    doi2pmid = {}\n",
    "    for i in range(0, len(dois), bsz):\n",
    "        batch = dois[i:i+bsz]\n",
    "        # wrap each DOI in quotes to avoid Bad Request\n",
    "        term = \" OR \".join(f'\"{d}\"[doi]' for d in batch)\n",
    "        params = ncbi({\n",
    "            \"db\":      \"pubmed\",\n",
    "            \"term\":    term,\n",
    "            \"retmode\": \"json\",\n",
    "            \"retmax\":  str(len(batch))\n",
    "        })\n",
    "        try:\n",
    "            resp = session.post(\n",
    "                \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\",\n",
    "                data=params,\n",
    "                timeout=60\n",
    "            )\n",
    "            # if PubMed still complains, dump the body for debugging\n",
    "            if not resp.ok:\n",
    "                print(\"⛔ ESearch DOI→PMID batch failed:\",\n",
    "                      resp.status_code, resp.reason)\n",
    "                print(\"Raw response:\\n\", resp.text)\n",
    "            resp.raise_for_status()\n",
    "            idlist = resp.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "            # map 1:1 for as many returned IDs as we got\n",
    "            for doi, pm in zip(batch, idlist):\n",
    "                doi2pmid[doi] = pm\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Batch #{i//bsz+1} DOI→PMID error:\", e)\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "    print(f\"   Mapped {len(doi2pmid)}/{len(dois)} DOIs → PMIDs\")\n",
    "    return doi2pmid\n",
    "\n",
    "\n",
    "# ── EFetch details + DOI extraction ─────────────────────────\n",
    "MONTH = {m:i for i,m in enumerate(\n",
    "    [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],1)}\n",
    "def fetch_details(pmids):\n",
    "    out={}\n",
    "    for chunk in tqdm(range(0,len(pmids),BATCH_SIZE), desc=\"EFetch\"):\n",
    "        batch=pmids[chunk:chunk+BATCH_SIZE]\n",
    "        r=session.post(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\",\n",
    "                       data=ncbi({\"db\":\"pubmed\",\"retmode\":\"xml\",\"id\":\",\".join(batch)}),\n",
    "                       timeout=120)\n",
    "        r.raise_for_status(); root=ET.fromstring(r.content)\n",
    "        for art in root.findall(\".//PubmedArticle\"):\n",
    "            pm=art.findtext(\".//PMID\")\n",
    "            ad = art.find(\".//ArticleDate\") or art.find(\".//PubDate\")\n",
    "            date=None\n",
    "            if ad is not None:\n",
    "                y,mo,da = ad.findtext(\"Year\"),ad.findtext(\"Month\"),ad.findtext(\"Day\")\n",
    "                if y and (mo or da):\n",
    "                    m=int(mo) if mo and mo.isdigit() else MONTH.get(mo[:3],1)\n",
    "                    d=int(da) if da and da.isdigit() else 1\n",
    "                    try: date=datetime.date(int(y),m,d)\n",
    "                    except: pass\n",
    "            refs=[ r.findtext(\"./ArticleId[@IdType='pubmed']\")\n",
    "                   for r in art.findall(\".//ReferenceList/Reference\")\n",
    "                   if r.findtext(\"./ArticleId[@IdType='pubmed']\") ]\n",
    "            doi = art.findtext(\".//ArticleIdList/ArticleId[@IdType='doi']\")\n",
    "            out[pm]={\"date\":date,\"refs\":refs,\"doi\":doi}\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "    return out\n",
    "\n",
    "# ── ELink citations ─────────────────────────────────────────\n",
    "def fetch_citations(pmids):\n",
    "    out={}\n",
    "    def _call(batch):\n",
    "        data=[(\"dbfrom\",\"pubmed\"),(\"linkname\",\"pubmed_pubmed_citedin\"),(\"cmd\",\"neighbor\")]\n",
    "        data+=[(\"id\",p) for p in batch]+list(ncbi().items())\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                r=session.post(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\",\n",
    "                               data=data,timeout=120)\n",
    "                r.raise_for_status(); return r.content\n",
    "            except (ChunkedEncodingError,ProtocolError,RequestException):\n",
    "                time.sleep(SLEEP_SECONDS*(attempt+1))\n",
    "        # split fallback\n",
    "        if len(batch)>1:\n",
    "            mid=len(batch)//2\n",
    "            return (_call(batch[:mid]) or b\"\")+(_call(batch[mid:]) or b\"\")\n",
    "        return b\"\"\n",
    "    for chunk in tqdm(range(0,len(pmids),BATCH_SIZE), desc=\"ELink\"):\n",
    "        raw=_call(pmids[chunk:chunk+BATCH_SIZE])\n",
    "        if not raw: continue\n",
    "        root=ET.fromstring(raw)\n",
    "        for ls in root.findall(\".//LinkSet\"):\n",
    "            src=ls.findtext(\".//IdList/Id\")\n",
    "            outs=[c.text for c in ls.findall(\".//LinkSetDb/Link/Id\") if c.text]\n",
    "            out[src]=outs\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "    return out\n",
    "\n",
    "# ── CrossRef augmentation ──────────────────────────────────\n",
    "def crossref_refs(dois):\n",
    "    out={}\n",
    "    for i in range(0,len(dois),CROSSREF_BATCH):\n",
    "        batch=dois[i:i+CROSSREF_BATCH]\n",
    "        filt=\",\".join(f\"doi:{d}\" for d in batch)\n",
    "        r=requests.get(\"https://api.crossref.org/works\",\n",
    "                       params={\"filter\":filt,\"rows\":len(batch),\"select\":\"DOI,reference\"},\n",
    "                       headers={\"User-Agent\":f\"{NCBI_TOOL}({NCBI_EMAIL})\"},timeout=60)\n",
    "        items=r.json().get(\"message\",{}).get(\"items\",[])\n",
    "        for it in items:\n",
    "            doi=it.get(\"DOI\"); refs=[ref.get(\"DOI\") for ref in it.get(\"reference\",[]) if ref.get(\"DOI\")]\n",
    "            out[doi]=refs\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "    return out\n",
    "\n",
    "# ── DAG / chronology helpers ───────────────────────────────\n",
    "def add_edge_safe(G,u,v,prov=\"ref\"):\n",
    "    du,dv=G.nodes[u].get(\"date\"),G.nodes[v].get(\"date\")\n",
    "    if du and dv and du>dv:         # time‑inverted ⇒ skip\n",
    "        return False\n",
    "    G.add_edge(u,v,prov=prov); return True\n",
    "\n",
    "# ── Core expansion ─────────────────────────────────────────\n",
    "def meets(cnt,total):\n",
    "    if THRESHOLD_TYPE==\"absolute\":  return cnt>=THRESHOLD_VAL\n",
    "    return (cnt/total)>=THRESHOLD_VAL if total else False\n",
    "\n",
    "def expand_graph(seed_pmids):\n",
    "    G=nx.DiGraph()\n",
    "    for pm in seed_pmids: G.add_node(pm,gen=0,src=\"seed\")\n",
    "    frontier=seed_pmids[:]; visited=set(seed_pmids)\n",
    "    metrics=[]\n",
    "    for depth in range(1,MAX_DEPTH+1):\n",
    "        print(f\"\\n=== DEPTH {depth} ({EXPANSION_MODE}) ===\")\n",
    "        # ---------- Parents ----------\n",
    "        det=fetch_details(frontier)\n",
    "        # CrossRef augmentation\n",
    "        if AUGMENT_REFS:\n",
    "            low=[p for p,d in det.items() if len(d[\"refs\"])<AUGMENT_THRESHOLD and d[\"doi\"]]\n",
    "            if low:\n",
    "                cr = crossref_refs([det[p][\"doi\"] for p in low])\n",
    "                all_dois=set(r for lst in cr.values() for r in lst)\n",
    "                m = doi_batch_esearch(list(all_dois))\n",
    "                for p in low:\n",
    "                    add=[m[d] for d in cr.get(det[p][\"doi\"],[]) if d in m]\n",
    "                    det[p][\"refs\"].extend(add)\n",
    "        # candidate counts\n",
    "        par_count={}\n",
    "        for p,info in det.items():\n",
    "            for r in info[\"refs\"]:\n",
    "                par_count.setdefault(r,0); par_count[r]+=1\n",
    "        total=len(frontier)\n",
    "        parents=[r for r,c in par_count.items() if meets(c,total)]\n",
    "        # ---------- Children ----------\n",
    "        cit_map=fetch_citations(frontier)\n",
    "        chi_count={}\n",
    "        for p,cl in cit_map.items():\n",
    "            for c in cl:\n",
    "                chi_count.setdefault(c,0); chi_count[c]+=1\n",
    "        children=[c for c,cnt in chi_count.items() if meets(cnt,total)]\n",
    "\n",
    "        # linear vs global frontier update\n",
    "        if EXPANSION_MODE==\"linear\":\n",
    "            next_frontier=children\n",
    "        else:                      # global\n",
    "            next_frontier=list(set(parents+children))\n",
    "\n",
    "        # Add nodes / edges\n",
    "        new_nodes=[n for n in next_frontier if n not in visited]\n",
    "        det_new=fetch_details(new_nodes) if new_nodes else {}\n",
    "        for n in new_nodes:\n",
    "            G.add_node(n,gen=depth,src=\"parent\" if n in parents else \"child\",\n",
    "                       date=det_new.get(n,{}).get(\"date\"))\n",
    "        # edges parent→frontier\n",
    "        e_added=0\n",
    "        for p in frontier:\n",
    "            for r in det[p][\"refs\"]:\n",
    "                if r in parents and r in G:\n",
    "                    if add_edge_safe(G,r,p,\"ref\"): e_added+=1\n",
    "        # edges frontier→child\n",
    "        for p,cl in cit_map.items():\n",
    "            for c in cl:\n",
    "                if c in children and c in G:\n",
    "                    if add_edge_safe(G,p,c,\"cit\"): e_added+=1\n",
    "        visited.update(new_nodes)\n",
    "        metrics.append({\"depth\":depth,\"new_nodes\":len(new_nodes),\n",
    "                        \"cum_nodes\":G.number_of_nodes(),\"edges\":e_added})\n",
    "        frontier=next_frontier\n",
    "    return G,pd.DataFrame(metrics)\n",
    "\n",
    "# ── PLOT helper ─────────────────────────────────────────────\n",
    "def plot_generations(G,title=\"Citation DAG\"):\n",
    "    # arrange layers horizontally\n",
    "    layers={}\n",
    "    for n,data in G.nodes(data=True):\n",
    "        g=data.get(\"gen\",0); layers.setdefault(g,[]).append(n)\n",
    "    pos={}\n",
    "    for g,nodes in layers.items():\n",
    "        for i,n in enumerate(nodes):\n",
    "            pos[n]=(g, -i)          # x=gen, y=stack\n",
    "    cmap=plt.cm.get_cmap(\"viridis\", max(layers)+1)\n",
    "    colors=[cmap[G.nodes[n]['gen']] for n in G.nodes()]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    nx.draw(G,pos,with_labels=False,node_size=30,edge_color=\"#999\",\n",
    "            node_color=colors)\n",
    "    sm=plt.cm.ScalarMappable(cmap=cmap,norm=plt.Normalize(0,max(layers)))\n",
    "    sm.set_array([]); cbar=plt.colorbar(sm,ticks=range(max(layers)+1))\n",
    "    cbar.set_label(\"Generation\")\n",
    "    plt.title(title); plt.axis(\"off\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ── RUN ─────────────────────────────────────────────────────\n",
    "if __name__==\"__main__\":\n",
    "    if SEED_DOIS:\n",
    "        seeds=list(doi_batch_esearch(SEED_DOIS).values())\n",
    "    elif SEED_QUERY:\n",
    "        seeds=esearch_to_pmids(SEED_QUERY,retmax=100)\n",
    "    else:\n",
    "        raise ValueError(\"Provide SEED_DOIS or SEED_QUERY\")\n",
    "\n",
    "    print(\"Seed PMIDs:\",seeds)\n",
    "    G,df=expand_graph(seeds)\n",
    "    print(\"\\n=== Metrics ===\")\n",
    "    print(df)\n",
    "\n",
    "    print(\"\\nDAG check:\", \"✅\" if nx.is_directed_acyclic_graph(G) else \"❌ (cycles!)\")\n",
    "    plot_generations(G,f\"{EXPANSION_MODE.capitalize()} expansion (depth ≤ {MAX_DEPTH})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (litscape)",
   "language": "python",
   "name": "litscape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
